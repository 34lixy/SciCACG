{"pre": "[CLS] the semantic role labeling system we used is the", "cit": "[CLS] there has also been significant work on #otherefr; #refr,"}
{"pre": "[CLS] the weights of the log-linear interpolation were tuned using", "cit": "[CLS] more recently, another encoder?decoder model using an rnn was"}
{"pre": "[CLS] the second is the third order algorithm of #refr", "cit": "[CLS] the idea is similar to finkler and neumann #otherefr,"}
{"pre": "[CLS] the second baseline is the state-of-theart tagging system is", "cit": "[CLS] in this section, we test our joint model on"}
{"pre": "[CLS] we use the same non-parallel spanish/english corpus used in", "cit": "[CLS] in the context of machine translation, the performance of"}
{"pre": "[CLS] the semantic parsing problem can be performed by using", "cit": "[CLS] we perform experiments in two benchmark semantic parsing datasets:"}
{"pre": "[CLS] the translation quality is evaluated by case-insensitive bleu-4 metric", "cit": "[CLS] we used the japanese parser knp #otherefr and the"}
{"pre": "[CLS] we compare our model to the best results of", "cit": "[CLS] different work exploits distributional information in different forms, including"}
{"pre": "[CLS] the most widely used metrics are based on word", "cit": "[CLS] there exists some work to remove noise from sms"}
{"pre": "[CLS] the first is the pyramid method #refr which is", "cit": "[CLS] research in summarization #otherefr; #refr has shown that, in"}
{"pre": "[CLS] the most widely used approach to build bilingual word", "cit": "[CLS] #refr used monolingual recursive autoencoders for sentiment prediction, with"}
{"pre": "[CLS] the first is a novel stochastic search strategy that", "cit": "[CLS] the smr technique works with statistical word classes #refr"}
{"pre": "[CLS] we use the same dataset as in #refr. [SEP]", "cit": "[CLS] other metrics were also calculated (e.g. the v-measure #refr,"}
{"pre": "[CLS] the model scaling factors ?m1 are trained with respect", "cit": "[CLS] we compare our model against a state-of-the-art statistical machine"}
{"pre": "[CLS] the work presented in this paper connects and extends", "cit": "[CLS] as shown in [#refr], a referring expression must communicate"}
{"pre": "[CLS] we use the stanford corenlp suite to lemmatize and", "cit": "[CLS] we evaluate our proposed model with the most commonly"}
{"pre": "[CLS] the most common approach to deriving translation lexicons from", "cit": "[CLS] researchers at new mexico state university have built an"}
{"pre": "[CLS] the first is the cornell word by a pair", "cit": "[CLS] such methods can achieve better performance, reaching a tagging"}
{"pre": "[CLS] the second type of collection that has been widely", "cit": "[CLS] experiments on two chinese treebanks showed that our approach"}
{"pre": "[CLS] #refr use a similar approach to extract the features", "cit": "[CLS] most existing works on sentiment summarization focus on predicting"}
{"pre": "[CLS] the first is the movie review dataset of #refr,", "cit": "[CLS] #refr propose an integrated statistical parsing technique that augments"}
{"pre": "[CLS] we use the same set of features as in", "cit": "[CLS] some success in this area has been demonstrated via"}
{"pre": "[CLS] the second baseline is the stanford parser #refr. [SEP]", "cit": "[CLS] much attention has recently been devoted to integer linear"}
{"pre": "[CLS] the translation quality is evaluated by case-insensitive bleu-4 metric", "cit": "[CLS] our framework proceeds by using the standard procedure of"}
{"pre": "[CLS] the most popular approach to mt system combination involves", "cit": "[CLS] recent years have witnessed burgeoning development of statistical machine"}
{"pre": "[CLS] the most common approach to handling the unbounded nature", "cit": "[CLS] our analysis more fully explains the positive results achieved"}
{"pre": "[CLS] the parser used in this paper is based on", "cit": "[CLS] we tried to incorporate the binned prosodic information described"}
{"pre": "[CLS] the first is the lbxas algorithm #refr to perform", "cit": "[CLS] measures of cross-language relatedness are useful for a large"}
{"pre": "[CLS] the most common approach to deriving translation lexicons from", "cit": "[CLS] we distinguish social acts from ?social events? as described"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr examines semantic", "cit": "[CLS] we used 600 million japanese web pages #otherefr parsed"}
{"pre": "[CLS] the most popular approach to learning semantic parsers #otherefr;", "cit": "[CLS] while syntactical constraints have been proven to helpful in"}
{"pre": "[CLS] the first is the english-french hansards data set to", "cit": "[CLS] our approach can be considered a generalization of syntactic"}
{"pre": "[CLS] the work presented in this paper connects and extends", "cit": "[CLS] a general abstract model of incremental processing based on"}
{"pre": "[CLS] the most popular seed expansion methods discussed in the", "cit": "[CLS] we also compare our method against wang and #refr?s"}
{"pre": "[CLS] the method we use for identifying word polarity is", "cit": "[CLS] the proposal in this paper is grounded on the"}
{"pre": "[CLS] the first is the method of #refr, which we", "cit": "[CLS] the first task is to answer the closest-opposite questions"}
{"pre": "[CLS] the grammar we used is the implementation of the", "cit": "[CLS] however, there are state-of-the-art unlexicalized parsers #refr, to which"}
{"pre": "[CLS] the work presented in this paper connects and extends", "cit": "[CLS] examples of such systems include tutoring systems, intelligent assistants,"}
{"pre": "[CLS] the work of #refr is closest in spirit to", "cit": "[CLS] there has been a substantial body of work on"}
{"pre": "[CLS] the most widely used metrics are based on word", "cit": "[CLS] for a fair comparison, all aligners are trained on"}
{"pre": "[CLS] the feature weights were tuned using minimum error rate", "cit": "[CLS] it is worth noting that the german parse trees"}
{"pre": "[CLS] the translation model is trained using a variant of", "cit": "[CLS] we identify and eliminate unimportant words, somewhat similar to"}
{"pre": "[CLS] the first is to relax or update the independence", "cit": "[CLS] bbn has developed a software package, the learner, as"}
{"pre": "[CLS] the feature weights were tuned on the development set", "cit": "[CLS] we proposed a variant of an online learning algorithm"}
{"pre": "[CLS] the most widely used approach to build bilingual dictionaries", "cit": "[CLS] #refr derive paraphrases from monolingual data using distributional similarity"}
{"pre": "[CLS] the most common approach to temporal relation extraction is", "cit": "[CLS] recently, there have been many advances in srl #otherefr;"}
{"pre": "[CLS] the model is trained using a variant of the", "cit": "[CLS] however, some recent approaches have explored ways of using"}
{"pre": "[CLS] the model scaling factors ?m1 are trained with respect", "cit": "[CLS] statistical machine translation #otherefr and prediction of twitter responses"}
{"pre": "[CLS] the most common approach to handling the unbounded nature", "cit": "[CLS] researchers have paid a lot of attention to datadriven"}
{"pre": "[CLS] the feature weights were tuned using minimum error rate", "cit": "[CLS] the unsupervised data that we integrate has been created"}
{"pre": "[CLS] the first is the lbxas algorithm #refr which uses", "cit": "[CLS] although this approach can give inaccurate estimates, the counts"}
{"pre": "[CLS] the first is the pyramid method #refr which is", "cit": "[CLS] our projection procedure resembles unsupervised text categorization #refr, with"}
{"pre": "[CLS] the first is the projective dependency model of #refr,", "cit": "[CLS] a similar idea is proposed by #refr for producing"}
{"pre": "[CLS] we use the stanford parser #refr to parse the", "cit": "[CLS] at each step of the distant supervision process, we"}
{"pre": "[CLS] we use the same set of features as in", "cit": "[CLS] discourse parsing tries to identify how the units are"}
{"pre": "[CLS] the grammar we used is the implementation of the", "cit": "[CLS] the existence of the atis database #refr is no"}
{"pre": "[CLS] the graphbased method views the problem as finding an", "cit": "[CLS] the graphbased method views the problem as finding an"}
{"pre": "[CLS] the parser used in this paper is a set", "cit": "[CLS] we use a common graph-based parsing technique #refr; the"}
{"pre": "[CLS] the unsupervised approaches for semi-supervised learning of hidden markov", "cit": "[CLS] this is a variant of the greedy one-to-one mapping"}
{"pre": "[CLS] the second is the third order parser, which is", "cit": "[CLS] first, we investigate the impact of using different flavours"}
{"pre": "[CLS] the first is a task-independent system described in #refr", "cit": "[CLS] #refr presents a detailed overview that also mentions [PAD]"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr examines semantic", "cit": "[CLS] #refr chose an approach motivated by the assumption that"}
{"pre": "[CLS] the first is to relax or update the independence", "cit": "[CLS] finally, another soft-constraint approach that can also be viewed"}
{"pre": "[CLS] the topic segmentation is a nontrivial and a topic", "cit": "[CLS] the artificial data set of #refr is widely used"}
{"pre": "[CLS] the second baseline is a state-of-the-art phrase-based translation system", "cit": "[CLS] another wsd approach incorporating context-dependent phrasal translation lexicons is"}
{"pre": "[CLS] the most widely used metrics are based on word", "cit": "[CLS] analogous techniques for tree-structured translation models involve either allowing"}
{"pre": "[CLS] the second one is the third order algorithm of", "cit": "[CLS] in the second international chinese word segmentation bakeoff (the"}
{"pre": "[CLS] the first is the process of annotating the dependency", "cit": "[CLS] the most representative methods for relation classification use supervised"}
{"pre": "[CLS] we use the same feature representation ?(x, y) as", "cit": "[CLS] we avoid the construction of [PAD] 1this problem is"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm", "cit": "[CLS] the maltparser system for english described in #refr was"}
{"pre": "[CLS] we use the same feature representation ?(x, y) as", "cit": "[CLS] there are two dominant approaches to dependency parsing: graph-based"}
{"pre": "[CLS] the grammar we use is the implementation of the", "cit": "[CLS] whereas we have concentrated on determinism in this paper,"}
{"pre": "[CLS] the dependency structure of a sentence is projective, if", "cit": "[CLS] we carried out a comprehensive evaluation of the automatically"}
{"pre": "[CLS] the second baseline is the stanford parser #refr. [SEP]", "cit": "[CLS] figure 2: a simple lexicalized parse tree. criminative models"}
{"pre": "[CLS] the most common approach to handling the unbounded nature", "cit": "[CLS] more recently, we show that our scfg-based parser can"}
{"pre": "[CLS] the first is the lbxas algorithm #refr which uses", "cit": "[CLS] abstracting from results for concrete test sets, #refr try"}
{"pre": "[CLS] the work presented in this paper connects and extends", "cit": "[CLS] we will extend the logical engine used to infer"}
{"pre": "[CLS] the second dataset is the moonstone dataset of #refr.", "cit": "[CLS] some of these techniques have been successfully applied for"}
{"pre": "[CLS] the work presented in this paper connects and extends", "cit": "[CLS] to expand our lexicon of these nouns, we started"}
{"pre": "[CLS] the most popular seed expansion methods discussed in the", "cit": "[CLS] #refr use a thesaurus to aid in the construction"}
{"pre": "[CLS] the second one is the third approach described in", "cit": "[CLS] analyses have shown that this augmented data can lead"}
{"pre": "[CLS] the first is the lbxas algorithm #refr to perform", "cit": "[CLS] in several studies (e.g., #refr it has been shown"}
{"pre": "[CLS] the learning algorithm used is a variation of the", "cit": "[CLS] in this study we use propbanked versions of the"}
{"pre": "[CLS] the first is the pyramid method #refr which is", "cit": "[CLS] more recently, various researchers have used topic models for"}
{"pre": "[CLS] we use the same data set as in #refr", "cit": "[CLS] #refr implement lexicalized content models that represent global text"}
{"pre": "[CLS] the parser is based on the first-order maximum entropy", "cit": "[CLS] the merging procedure seeks to address overfitting at the"}
{"pre": "[CLS] the system used for the experiment reported below uses", "cit": "[CLS] recent experiments performed by two groups of researchers at"}
{"pre": "[CLS] the first is the proposal and use of the", "cit": "[CLS] a source of statistics widely used in prior work"}
{"pre": "[CLS] the first is the english-french hansards data set to", "cit": "[CLS] details about the speech recognition system we refer to"}
{"pre": "[CLS] the first is the (relatively straightforward) evolution of the", "cit": "[CLS] as using complementary information has been useful in, e.g.,"}
{"pre": "[CLS] the first is the english-french hansards data set to", "cit": "[CLS] the linguistic information within this track is encoded in"}
{"pre": "[CLS] the most popular seed expansion methods discussed in the", "cit": "[CLS] #refr adopted a supervised learning technique to search for"}
{"pre": "[CLS] the most popular algorithm for this weight optimisation is", "cit": "[CLS] for more expressive, linguistically-motivated syntactic mt models #refr, the"}
{"pre": "[CLS] the task of the system is described in detail", "cit": "[CLS] not surprisingly, some degree of disambiguation is needed to"}
{"pre": "[CLS] the most popular approach to mt system combination involves", "cit": "[CLS] a good example of work in this space is"}
{"pre": "[CLS] the feature weights were tuned on the development set", "cit": "[CLS] factored models #refr factor the phrase translation probabilities over"}
{"pre": "[CLS] we use the same set of features as in", "cit": "[CLS] therefore, we were less concerned with improving efficiency, and"}
{"pre": "[CLS] the first is the internet slang dictionary from #refr.", "cit": "[CLS] it has been shown that accurate discourse segmentation within"}
{"pre": "[CLS] the second baseline is the stanford segmenter #refr. [SEP]", "cit": "[CLS] many systems use binary features i.e. the word-internal features,"}
{"pre": "[CLS] the first is the process of syntactic structure of", "cit": "[CLS] this 5all statistical significance tests in these experiments use"}
{"pre": "[CLS] the first is the internet slang dictionary from #refr.", "cit": "[CLS] segmentation is a useful intermediate step in such applications"}
{"pre": "[CLS] the model is trained using a variant of the", "cit": "[CLS] #refr dissected parsing difficulty metrics (including surprisal and entropy)"}
{"pre": "[CLS] the semantic role labeling framework has proven useful in", "cit": "[CLS] sentence splitting and tokenization was carried out using the"}
{"pre": "[CLS] the most popular seed expansion methods discussed in the", "cit": "[CLS] al has been successfully applied to a number of"}
{"pre": "[CLS] we use the same set of features as in", "cit": "[CLS] #refr showed a slight improvement in parsing accuracy when"}
{"pre": "[CLS] the system uses the illinois system described in #refr", "cit": "[CLS] there is currently support for word- net #otherefr, and"}
{"pre": "[CLS] the model of #refr is based on quasi-synchronous grammar", "cit": "[CLS] we follow the approach of #refr and predict the"}
{"pre": "[CLS] the feature weights were tuned on the development set", "cit": "[CLS] handcrafted rules #otherefr; #refr for preordering training data and"}
{"pre": "[CLS] the first is the cornell sentence extraction tool #refr", "cit": "[CLS] #refr found several parsing-based features and part-of-speech based features"}
{"pre": "[CLS] the pos accuracy improves slightly by 0.12 parser tlas", "cit": "[CLS] among recent op performing methods are hidden markov models"}
{"pre": "[CLS] the first is to relax or update the independence", "cit": "[CLS] the reordering metrics require alignments which were created using"}
{"pre": "[CLS] #refr proposed a method for inducing propbank-style pos tagging", "cit": "[CLS] [PAD] is known to not improve upon a single"}
{"pre": "[CLS] the parser is based on the lexicalized pcfg implementation", "cit": "[CLS] many authors (among them #refr incorporate ra into their"}
{"pre": "[CLS] the first is the cornell sentence extraction algorithm by", "cit": "[CLS] nevertheless, we conclude that among currently available approaches, i.e.,"}
{"pre": "[CLS] the most popular approach to mt system combination involves", "cit": "[CLS] using higher order lms to improve the accuracy of"}
{"pre": "[CLS] the first is the cornell sentence boundary detection method", "cit": "[CLS] this is perhaps [PAD] as the finer-grained distinctions in"}
{"pre": "[CLS] the model is trained using a variant of the", "cit": "[CLS] the system translates entences in the atis domain #refr"}
{"pre": "[CLS] the first is to relax or update the independence", "cit": "[CLS] we took the model of #refr as the baseline,"}
{"pre": "[CLS] the first is the task of identifying positive and", "cit": "[CLS] in order to improve qa systems? performance many research"}
{"pre": "[CLS] the first is the (relatively straightforward) evolution of the", "cit": "[CLS] for language acquisition, a natural question is whether prosody"}
{"pre": "[CLS] the first is the cornell sentence extraction tool #refr", "cit": "[CLS] while many works #otherefr view the properties of positivity"}
{"pre": "[CLS] the second dataset is the moonstone dataset of #refr.", "cit": "[CLS] because our algorithm does not consider the context given"}
{"pre": "[CLS] the model is trained using a variant of the", "cit": "[CLS] one source of constraint which our model still does"}
{"pre": "[CLS] the first is the method of #refr, which we", "cit": "[CLS] analysis in order to distinguish antonyms from synonyms, the"}
{"pre": "[CLS] the english side of the parallel corpus was parsed", "cit": "[CLS] given a bilingual corpus, we use giza++ #refr as"}
{"pre": "[CLS] we use the stanford dependency parser #refr to parse", "cit": "[CLS] several researchers explored joint ds and ps information to"}
{"pre": "[CLS] we use the stanford parser #refr to parse the", "cit": "[CLS] we compare against the original miml-re model using the"}
{"pre": "[CLS] the grammar we used is the implementation of the", "cit": "[CLS] 4 here i propose what appears to me to"}
{"pre": "[CLS] the word alignment is generated using the giza++ toolkit", "cit": "[CLS] it is a fundamental and often a necessary step"}
{"pre": "[CLS] the work of #refr is closest in spirit to", "cit": "[CLS] #refr took the approach that large number of entities"}
{"pre": "[CLS] we use the same set of features as in", "cit": "[CLS] the field of syntactic parsing has received a great"}
{"pre": "[CLS] the most common approach to handling the unbounded nature", "cit": "[CLS] greedy local search #otherefr; #refr has typically been used"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr examines semantic", "cit": "[CLS] this representation departs from the vector space metaphor #otherefr;"}
{"pre": "[CLS] the first is the task of identifying positive and", "cit": "[CLS] since japanese does not delimit words by white-space, the"}
{"pre": "[CLS] the edr has close ties to the named entity", "cit": "[CLS] #refr first identified the importance of syntactic [PAD] parsing"}
{"pre": "[CLS] the model is trained using a variant of the", "cit": "[CLS] one focusses on filtering the extracted hierarchical rules either"}
{"pre": "[CLS] the model is trained using a variant of the", "cit": "[CLS] rather than using sentence length as a proxy, measures"}
{"pre": "[CLS] #refr use a similar approach to extract a sentence", "cit": "[CLS] sentence compression is typically formulated as the problem of"}
{"pre": "[CLS] the first is the projective dependency model of #refr,", "cit": "[CLS] features we have designed rather simple features based on"}
{"pre": "[CLS] the most common approach to deriving translation lexicons from", "cit": "[CLS] each list contained the n-best translations produced by the"}
{"pre": "[CLS] the first is the cornell sentence extraction tool #refr", "cit": "[CLS] in the following model, summarized here from the full"}
{"pre": "[CLS] the most popular approach to mt system combination involves", "cit": "[CLS] however, although it is standard practice in mt evaluation"}
{"pre": "[CLS] we use the dataset from #refr which consists of", "cit": "[CLS] for example, it has been shown that grammatical error"}
{"pre": "[CLS] we use the stanford pos tagger #refr to tokenize", "cit": "[CLS] the correctness of fields extracted via a conditional random"}
{"pre": "[CLS] the first is the lbxas algorithm #refr which uses", "cit": "[CLS] previous work has shown that within a given discourse"}
{"pre": "[CLS] the first is the proposal by #refr and only", "cit": "[CLS] instead researchers condition parsing decisions on many other features,"}
{"pre": "[CLS] the semantic format is dependencyminimal recursion semantics (dmrs) which", "cit": "[CLS] in particular, each [PAD] 1988/, and/rau and jitcobs 1988/"}
{"pre": "[CLS] the lingo grammar matrix #refr is a toolkit for", "cit": "[CLS] unlike the typical content specification modules #otherefr; #refr, our"}
{"pre": "[CLS] we use the same set of features as in", "cit": "[CLS] approaches to this problem began by taking all fragments"}
{"pre": "[CLS] the first is the method of #refr, which we", "cit": "[CLS] the smoothing methods proposed in the literature #otherefr, and"}
{"pre": "[CLS] the model of #refr uses a similar structure analysis", "cit": "[CLS] such annotated resources are scarce, expensive to create and"}
{"pre": "[CLS] the first is the cornell sentence boundary detection method", "cit": "[CLS] it is highly effective for learners to receive feedback"}
{"pre": "[CLS] the goal of this paper is to address the", "cit": "[CLS] a review of methods for word sense disambiguation is"}
{"pre": "[CLS] we use the same non-parallel spanish/english corpus used in", "cit": "[CLS] many previous works show promising results with an assumption"}
{"pre": "[CLS] the most common approach to deriving translation lexicons from", "cit": "[CLS] the pyramid method #refr, was inspired in part by"}
{"pre": "[CLS] the first is the (relatively straightforward) evolution of the", "cit": "[CLS] correcting pos annotation errors can be done by applying"}
{"pre": "[CLS] #refr proposed a method for identifying word polarity of", "cit": "[CLS] the opinosis dataset #refr consists of short user reviews"}
{"pre": "[CLS] in this paper, we focus on the following five", "cit": "[CLS] future research will investigate more sophisticated methods of text-to-text"}
{"pre": "[CLS] the first is the movie review data of pang", "cit": "[CLS] we pre-process the dataset with the following tools: the"}
{"pre": "[CLS] the most popular seed expansion methods discussed in the", "cit": "[CLS] false opinion [PAD] in another case, the phrase [PAD]"}
{"pre": "[CLS] #refr proposed a method for summarizing scientific articles by", "cit": "[CLS] #refr investigate questions of dialectal differences and variation in"}
{"pre": "[CLS] the parser used in this paper is a set", "cit": "[CLS] in addition, we have thrown further doubt on the"}
{"pre": "[CLS] the first is the cornell sentence extraction tool #refr", "cit": "[CLS] the use of prosodic and coarticulatory context has improved"}
{"pre": "[CLS] the translation quality is evaluated by case-insensitive bleu-4 metric", "cit": "[CLS] #refr investigated system combination using automatic generation of diverse"}
{"pre": "[CLS] the first is the (relatively straightforward) evolution of the", "cit": "[CLS] therefore, the feature model focuses on each kind of"}
{"pre": "[CLS] #refr and li et al. #otherefr both present models", "cit": "[CLS] nearly all previous work on unsupervised grammar induction has"}
{"pre": "[CLS] the first is to relax or update the independence", "cit": "[CLS] another common type of projection methods map labels from"}
{"pre": "[CLS] the model is trained using a variant of the", "cit": "[CLS] our interpretation is more useful than past interpretations involving"}
{"pre": "[CLS] the first is the (relatively straightforward) evolution of the", "cit": "[CLS] parc 700 dependency bank following #refr the results are"}
{"pre": "[CLS] the most common approach to deriving translation lexicons from", "cit": "[CLS] although statistical approaches to automatic term recognition, e.g. #otherefr;"}
{"pre": "[CLS] the most common approach to deriving semantic relatedness is", "cit": "[CLS] it has received significant attention in recent years due"}
{"pre": "[CLS] the first is the english-french word alignment, where we", "cit": "[CLS] results of this evaluation and comparisons with our theoretical"}
{"pre": "[CLS] we use the same set of features as in", "cit": "[CLS] alignments were learned using two iterations of model 1"}
{"pre": "[CLS] the most popular seed expansion methods discussed in the", "cit": "[CLS] #refr use machine translation for multilingual sentiment analysis. [SEP]"}
{"pre": "[CLS] the first is the proposal by #refr and #otherefr,", "cit": "[CLS] here we report first quantitative results on the [PAD]"}
{"pre": "[CLS] the first is the pyramid method #refr which is", "cit": "[CLS] for example, citation-based summarization systems #refr and survey generation"}
{"pre": "[CLS] the best results were obtained by the unsupervised part-of-speech", "cit": "[CLS] we use mxpost tagger #otherefr for pos tagging, charniak"}
{"pre": "[CLS] the first is usually focus on exploiting automatic generated", "cit": "[CLS] thanks to this sort of activation flow fig tends"}
{"pre": "[CLS] the parsing algorithm is based on the algorithm of", "cit": "[CLS] the deterministic shift/reduce classifier-based dependency parsing approach #refr has"}
{"pre": "[CLS] we use the same non-parallel spanish/english corpus used in", "cit": "[CLS] this method is more similar to the one used"}
{"pre": "[CLS] the most widely used metrics are based on the", "cit": "[CLS] #refr used a roget-like thesaurus, co-occurrence statistics, and a"}
{"pre": "[CLS] in the context of machine translation, #refr use a", "cit": "[CLS] the english corpus contains [PAD] sentences, that were pos"}
{"pre": "[CLS] the idea of using reinforcement learning to automate the", "cit": "[CLS] #refr introduced a corpus-based approach for generating a k"}
{"pre": "[CLS] the first is the cornell sentence extraction tool #refr", "cit": "[CLS] gi#refr reports a relation discovery algorithm based on [PAD]"}
{"pre": "[CLS] the second is the third order algorithm of #refr", "cit": "[CLS] our error analysis above also highlights our task?s difference"}
{"pre": "[CLS] we use the dataset from #refr which consists of", "cit": "[CLS] to address this problem, there have been several recent"}
{"pre": "[CLS] the second dataset is the moonstone dataset of #refr.", "cit": "[CLS] several approaches have modified the lesk algorithm to reduce"}
{"pre": "[CLS] the model is trained using a variant of the", "cit": "[CLS] in recent years a variety of statistical models for"}
{"pre": "[CLS] the most common approach to deriving translation lexicons from", "cit": "[CLS] we have been building the [PAD] mt system for"}
{"pre": "[CLS] the morphological analysis used in the experiments is based", "cit": "[CLS] there are two kinds of methods for morphological disambiguation:"}
{"pre": "[CLS] the model is similar to that of #refr who", "cit": "[CLS] methodologies such as lexicalisation #otherefr; #refr and tree transformations"}
{"pre": "[CLS] the most widely used metrics are based on word", "cit": "[CLS] #refr suggested a division of countability into five major"}
{"pre": "[CLS] we use the same set of features as in", "cit": "[CLS] in this section we introduce basic definitions related to"}
{"pre": "[CLS] the most popular seed expansion methods discussed in the", "cit": "[CLS] in principle, the comparison can be done via [PAD]"}
{"pre": "[CLS] the second is the third order algorithm of #refr", "cit": "[CLS] an investigation propose that boundary strength according to the"}
{"pre": "[CLS] #refr use a probabilistic model to predict the orientation", "cit": "[CLS] several methods have been proposed with regard to aligning"}
{"pre": "[CLS] the first is the proposal by #refr and used", "cit": "[CLS] #refr\\] propose a large scale method, but results are"}
{"pre": "[CLS] the first is the pyramid method #refr which is", "cit": "[CLS] there are also automatic measures which do not require"}
{"pre": "[CLS] the most popular seed expansion methods discussed in the", "cit": "[CLS] at first, additional linguistic resources, such as specialized dictionaries"}
{"pre": "[CLS] we use the same set of features as in", "cit": "[CLS] we choose to use an earlier neural network based"}
{"pre": "[CLS] the most common approach to handling the unbounded nature", "cit": "[CLS] in #refr we proposed a general protocol for handling"}
{"pre": "[CLS] #refr proposed a method for extracting social networks from", "cit": "[CLS] for instance, measures that compute the association strength between"}
{"pre": "[CLS] the first is the movie review dataset of #refr,", "cit": "[CLS] in #refr, it is proposed a matrix-vector recursive neural"}
{"pre": "[CLS] the first is the cornell sentence extraction tool #refr", "cit": "[CLS] this problem is a counterpart to the image description"}
{"pre": "[CLS] the most common approach to handling the unbounded nature", "cit": "[CLS] unlabeled data has been shown to improve the accuracy"}
{"pre": "[CLS] the most common approach to deriving semantic relations is", "cit": "[CLS] the method has also been popular in the related"}
{"pre": "[CLS] the first is the proposal by #refr and the", "cit": "[CLS] care was taken to ensure not just that the"}
{"pre": "[CLS] the first is usually focus on exploiting automatic generated", "cit": "[CLS] previous work on transfer-based mt systems #refr and alignment-based"}
{"pre": "[CLS] the parsing algorithm is based on the algorithm of", "cit": "[CLS] to speed up viterbi parsing, sophisticated search strategies have"}
{"pre": "[CLS] we use the stanford corenlp suite to lemmatize and", "cit": "[CLS] historically mt researchers have focused their attention on the"}
{"pre": "[CLS] the first is the cornell sentence extraction tool #refr", "cit": "[CLS] researchers have explored error detection for manually tagged corpora"}
{"pre": "[CLS] the first is the cornell sentence extraction tool #refr", "cit": "[CLS] although the overall parsing style of our system integrates"}
{"pre": "[CLS] the model is trained using a variant of the", "cit": "[CLS] several unsupervised pos induction systems make use of morphological"}
{"pre": "[CLS] the grammar we use is the pcfg implementation of", "cit": "[CLS] our data sources are the german negra #refr and"}
{"pre": "[CLS] the first is the method of co-training #otherefr, which", "cit": "[CLS] we show that by augmenting the delexicalized direct transfer"}
{"pre": "[CLS] the model is trained using a variant of the", "cit": "[CLS] subsequent work explored ways of exploiting linguistically annotated data"}
{"pre": "[CLS] the first is the cornell sentence extraction tool #refr", "cit": "[CLS] the speed and cost benefits for annotation are certainly"}
{"pre": "[CLS] we use the same set of features as in", "cit": "[CLS] related sentences are represented by a word graph so"}
{"pre": "[CLS] we use the stanford corenlp suite to lemmatize and", "cit": "[CLS] most of the research as focused on bilingual terminology"}
{"pre": "[CLS] the system analyzes natural language annotations to produce ternary", "cit": "[CLS] the domaingeneral dm was mostly abstracted from the talk"}
{"pre": "[CLS] in the last decade, semantic role labeling #refr, and", "cit": "[CLS] this is partly due to its relevance for applications"}
{"pre": "[CLS] the feature weights were tuned on a development set", "cit": "[CLS] bilingual word alignments are trained and combined from two"}
{"pre": "[CLS] the feature weights were tuned using minimum error rate", "cit": "[CLS] thanks to specific reordering modeling components, phrase-based smt #otherefr;"}
{"pre": "[CLS] the grammar we use is the pcfg implementation of", "cit": "[CLS] these rules can be handcrafted grammar rules, such as"}
{"pre": "[CLS] #refr proposed a method to align multiple translations of", "cit": "[CLS] over the years, several approaches for mining translations from"}
{"pre": "[CLS] #refr use a probabilistic model to predict positive/negative language", "cit": "[CLS] many computational models of compositionality focus on learning vector"}
{"pre": "[CLS] the most common approach to handling the unbounded nature", "cit": "[CLS] this is the approach taken by #refr, where they"}
{"pre": "[CLS] we use the same set of features as in", "cit": "[CLS] transliteration methods typically fall into two categories: generative approaches"}
{"pre": "[CLS] #refr proposed a method for word segmentation and pos", "cit": "[CLS] these results are [PAD] considering that #refr ignored candidates"}
{"pre": "[CLS] the work of #refr is closest in spirit to", "cit": "[CLS] amazon mechanical turk is a crowdsourcing platform that has"}
{"pre": "[CLS] the work of #refr is closest in spirit to", "cit": "[CLS] recently, #refr have reported experiments on learning information structure"}
{"pre": "[CLS] the first is the proposal by #refr and the", "cit": "[CLS] #refr employ a semi-automatic method to improve a large-scale"}
{"pre": "[CLS] the most common approach is to use beam search", "cit": "[CLS] there have been a number of recent studies on"}
{"pre": "[CLS] the most widely used metrics are based on bleu", "cit": "[CLS] finally, we have also considered ulc, which is a"}
{"pre": "[CLS] the most common approach to handling the unbounded nature", "cit": "[CLS] this weight vector is learned using a simple perceptron"}
{"pre": "[CLS] the model is trained using a variant of the", "cit": "[CLS] they are characterized by the following properties #refrb): [SEP]"}
{"pre": "[CLS] we use the stanford corenlp suite to lemmatize and", "cit": "[CLS] work on preposition errors has used a similar classification"}
{"pre": "[CLS] we use the standard mert #refr to tune the", "cit": "[CLS] parallel sentences were first word-aligned using a maxent aligner"}
{"pre": "[CLS] the first is usually focus on exploiting automatic generated", "cit": "[CLS] identification of semantically similar situations can be improved by"}
{"pre": "[CLS] we use the same set of features as in", "cit": "[CLS] others have exploited the automatic transfer of some already"}
{"pre": "[CLS] the most popular approach to paraphrase acquisition algorithms are", "cit": "[CLS] recent work has shown how paraphrases can improve question"}
{"pre": "[CLS] the first is the method of co-training #otherefr, which", "cit": "[CLS] currently, the performance of even the most simple direct"}
{"pre": "[CLS] the second baseline is a state-ofthe-art system #refr. [SEP]", "cit": "[CLS] for our chinese to english translation experiments, we generated"}
{"pre": "[CLS] the model is trained using a variant of the", "cit": "[CLS] orig with fixed hyperparameters performs best, with the highest"}
{"pre": "[CLS] we use the same set of features as in", "cit": "[CLS] conditional random fields #otherefr are quite effective at sequence"}
{"pre": "[CLS] the semantic role labeling framework has proven useful in", "cit": "[CLS] state-of-art approaches to frame-based srl are based on support"}
{"pre": "[CLS] the most common approach to deriving translation lexicons from", "cit": "[CLS] this would also include more semantic information, e.g., in"}
{"pre": "[CLS] the best results were obtained by the organizers using", "cit": "[CLS] following #refr, we use the default parameters (? ="}
{"pre": "[CLS] the first is the process of annotating the annotation", "cit": "[CLS] some of these needs can be addressed by emerging"}
{"pre": "[CLS] we use the same non-parallel spanish/english corpus used in", "cit": "[CLS] our work joins others in using continuous representations to"}
{"pre": "[CLS] the lingo grammar matrix #refr is a toolkit for", "cit": "[CLS] 5the meaning of the word \"synchronized\" here is exactly"}
{"pre": "[CLS] the grammar we used is the implementation of the", "cit": "[CLS] a data-oriented parsing model #otherefr; #refr, 1993a) is characterized"}
{"pre": "[CLS] the second baseline is the state-of-theart joint model #refr.", "cit": "[CLS] transition-based dependency parsing can be modeled under this framework,"}
{"pre": "[CLS] the model is trained using a variant of the", "cit": "[CLS] phrase-based smt models, such as the alignment template model"}
{"pre": "[CLS] the most widely used approach to build a number", "cit": "[CLS] similar work has also been performed in the area"}
{"pre": "[CLS] the most common approach to deriving knowledge acquisition approaches", "cit": "[CLS] many approaches have been devised, including the identification of"}
{"pre": "[CLS] the first is the lbxas algorithm #refr which uses", "cit": "[CLS] #refr described a wsd method and an implementation based"}
{"pre": "[CLS] we use the same data set as in #refr", "cit": "[CLS] deep and accurate text analysis based on discriminative models"}
{"pre": "[CLS] the most common approach to handling the unbounded nature", "cit": "[CLS] we have extended non-projective unlabeled dependency parsing #refr to"}
{"pre": "[CLS] the parser of #refr is a two-stage ctf [SEP]", "cit": "[CLS] there has been much recent work in attempting to"}
{"pre": "[CLS] we use the stanford corenlp suite to lemmatize and", "cit": "[CLS] recently, #refr has reported a word accuracy of 92.6%"}
{"pre": "[CLS] the tempeval community focused on the classification of the", "cit": "[CLS] previous research has shown that rst trees can play"}
{"pre": "[CLS] the second baseline is the stanford parser #refr. [SEP]", "cit": "[CLS] these same features also proved crucial to subsequent approaches,"}
{"pre": "[CLS] we use the stanford corenlp suite to lemmatize and", "cit": "[CLS] for this reason, we compute an unweighted entity-constrained mention"}
{"pre": "[CLS] the first is the proposal of #refr and use", "cit": "[CLS] after the error miner identifies afwater as a problematic"}
{"pre": "[CLS] the features used in this work are based on", "cit": "[CLS] to further assure the quality of the annotation, a"}
{"pre": "[CLS] the work presented in this paper connects and extends", "cit": "[CLS] these user simulations are now commonly used in statistical"}
{"pre": "[CLS] #refr proposed a method for identifying the polarity of", "cit": "[CLS] other work has shown that co-occurrence of words #otherefr"}
{"pre": "[CLS] the semantic role labeling framework has proven useful in", "cit": "[CLS] it is a well-documented fact #refr that subcategorisation frames"}
{"pre": "[CLS] the model scaling factors are optimized with respect to", "cit": "[CLS] in this paper we focus on recurrent neural network"}
{"pre": "[CLS] we use the same data set as in #refr", "cit": "[CLS] as our approach for incorporating unlabeled data, we basically"}
{"pre": "[CLS] we use the same dataset as in #refr which", "cit": "[CLS] we extended the unsupervised corpus-extracted phrase approximation method of"}
{"pre": "[CLS] the first is the pyramid method #refr which is", "cit": "[CLS] the paradigm of translation for [PAD] introduced by kay"}
{"pre": "[CLS] the grammar we use is the pcfg version of", "cit": "[CLS] the projected trees can be used to answer linguistic"}
{"pre": "[CLS] the most common approach to handling the unbounded nature", "cit": "[CLS] we evaluated our system using the standard evaluation script"}
{"pre": "[CLS] the most common approach to deriving translation lexicons from", "cit": "[CLS] second, we prune the phrase-table using a statistical significance"}
{"pre": "[CLS] we use the stanford corenlp suite to lemmatize and", "cit": "[CLS] to this end, patternbased approaches have long been used"}
{"pre": "[CLS] the first is the proposal by #refr and the", "cit": "[CLS] they include such topics as: work with various corpus"}
{"pre": "[CLS] the most popular seed expansion methods discussed in the", "cit": "[CLS] first, even when sentiment is the desired focus, researchers"}
{"pre": "[CLS] we use the same feature set as in #refr.", "cit": "[CLS] compared to graph-based dependency parsing, it typically offers linear"}
{"pre": "[CLS] the work of #refr is most similar to ours", "cit": "[CLS] unbalanced corpora are common in a number of different"}
{"pre": "[CLS] the most widely used metrics are based on bleu", "cit": "[CLS] metrics in the rouge family allow for skip n-grams"}
{"pre": "[CLS] the feature weights were tuned on the development set", "cit": "[CLS] therefore, recent efforts #refr have concentrated on feature design"}
{"pre": "[CLS] the first is the partition of the scoring parts", "cit": "[CLS] #refr report better results than ours on portuguese, slovene,"}
{"pre": "[CLS] the word alignment is generated using the giza++ toolkit", "cit": "[CLS] we used three aligners in this work: giza++ #otherefr,"}
{"pre": "[CLS] we use the same data set as in #refr", "cit": "[CLS] #refr propose a solution to this problem: for each"}
{"pre": "[CLS] the first is the lbxas algorithm #refr which uses", "cit": "[CLS] among the current similar works, table 4 shows that"}
{"pre": "[CLS] the most common approach to handling the unbounded nature", "cit": "[CLS] a similar modification was used by #refr for the"}
{"pre": "[CLS] the semantic parser is trained using the propbank corpus,", "cit": "[CLS] in both cases, transfer is driven by the transfer"}
{"pre": "[CLS] we use the dataset from #refr which consists of", "cit": "[CLS] such text data tend to be long and generate"}
{"pre": "[CLS] the first is the pyramid method #refr which is", "cit": "[CLS] in a related recent approach, #refr presented preliminary results"}
{"pre": "[CLS] the most popular approach to mt system combination involves", "cit": "[CLS] although it has been shown that increasing the amount"}
{"pre": "[CLS] the first is the movie review dataset of #refr,", "cit": "[CLS] we first extracted named entities using a [PAD] ner"}
{"pre": "[CLS] the word alignment is generated using the giza++ toolkit", "cit": "[CLS] past studies of combining alternative alignments focused on minimizing"}
{"pre": "[CLS] the most popular approach to semantic parsers #otherefr; #refr", "cit": "[CLS] #refr presented a statistical system that automatically produces an"}
{"pre": "[CLS] the first is the movie review dataset of bunescu", "cit": "[CLS] we employ the features of noun terms, and sentiment"}
{"pre": "[CLS] the feature weights were tuned on the development set", "cit": "[CLS] a translation model consists of two distinct elements: an"}
{"pre": "[CLS] the first is the pyramid method #refr which is", "cit": "[CLS] we are inspired by multiple sequence alignment methods in"}
{"pre": "[CLS] the second is the third order algorithm of #refr", "cit": "[CLS] unlike other languages #refr, chinese unk translation cannot use"}
{"pre": "[CLS] the most common approach to handling the unbounded nature", "cit": "[CLS] for this paper, we consider tree-shaped hierarchies so that"}
{"pre": "[CLS] we use the subjectivity lexicon of #refr,2 and polarity", "cit": "[CLS] we follow the same approach as in #refr to"}
{"pre": "[CLS] we use the same set of features as in", "cit": "[CLS] the dataset is the same as in leading works"}
{"pre": "[CLS] the second is the third order algorithm of #refr", "cit": "[CLS] part of the work using this tool was described"}
{"pre": "[CLS] the first is usually focus on exploiting automatic generated", "cit": "[CLS] however, the performance of context vectors drastically decreases for"}
{"pre": "[CLS] the first is the english-french word alignment, where we", "cit": "[CLS] #refr explored strategies for selecting better random [PAD] [PAD]"}
{"pre": "[CLS] the semantic ontribution of a lexical anchor includes both", "cit": "[CLS] the grammar matrix customization system #refr presents the [PAD]"}
{"pre": "[CLS] the first is to relax or update the independence", "cit": "[CLS] the model is called transfer- driven machine translation (tdmt)"}
{"pre": "[CLS] the parsing algorithm is based on the algorithm of", "cit": "[CLS] these modifications, however, give a system which suffers 2see,"}
{"pre": "[CLS] the work of #refr is closest in spirit to", "cit": "[CLS] on the other hand, #refr proposed a resolving algorithm"}
{"pre": "[CLS] the second is the third order algorithm of #refr", "cit": "[CLS] cutting introduced grouping of words into equiva.lence classes based"}
{"pre": "[CLS] the most popular seed expansion methods discussed in the", "cit": "[CLS] #refr proposed several approaches for cross lingual subjectivity analysis"}
{"pre": "[CLS] the first is the method of #refr, which we", "cit": "[CLS] #refr evaluate various similarity measures based on 1000 frequent"}
{"pre": "[CLS] the first is the method of #refr, which we", "cit": "[CLS] this was the setting obtaining the best results in"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr examines semantic", "cit": "[CLS] in the last 4-5 years, researchers have begun to"}
{"pre": "[CLS] the translation quality is evaluated by case-insensitive bleu-4 #refr.", "cit": "[CLS] mert is the standard technique for obtaining a machine"}
{"pre": "[CLS] the first is the lbxas algorithm #refr which uses", "cit": "[CLS] pseudo-word evaluations are currently used to evaluate a variety"}
{"pre": "[CLS] we use the same set of features as in", "cit": "[CLS] statistical machine learning methods such as hidden markov models"}
{"pre": "[CLS] the second is the third order algorithm of #refr", "cit": "[CLS] the advantage of using pos tags rather than words"}
{"pre": "[CLS] in the field of natural language processing, various statistical", "cit": "[CLS] the second line of research uses comparable or bilingual"}
{"pre": "[CLS] we use the same set of features as in", "cit": "[CLS] these rules can be designed manually #otherefr; #refr. [SEP]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature", "cit": "[CLS] #refr adapt the technique of supertagging #otherefr to ccg,"}
{"pre": "[CLS] the most relevant work is #refr, which uses cross-lingual", "cit": "[CLS] it also exploits the classification given by the comlex"}
{"pre": "[CLS] #refr proposed a method for extracting social networks from", "cit": "[CLS] recently, studies have explored dialog act tagging in written"}
{"pre": "[CLS] we use the same dataset as in #refr which", "cit": "[CLS] a number of relation extraction kernels have been proposed,"}
{"pre": "[CLS] the grammar we used is the implementation of the", "cit": "[CLS] for one, it allows a tight correspondence between syntax"}
{"pre": "[CLS] we use the stanford parser #refr to parse the", "cit": "[CLS] recently, there have been two promising research directions for"}
{"pre": "[CLS] the system used for the experiment reported below uses", "cit": "[CLS] the k?means algorithm is used for clustering the contexts,"}
{"pre": "[CLS] the work presented in this paper connects and extends", "cit": "[CLS] one type of approach uses information extraction techniques such"}
{"pre": "[CLS] we use the same non-parallel spanish/english corpus used in", "cit": "[CLS] the mapping from a word alignment to the set"}
{"pre": "[CLS] the most popular algorithm for this weight optimisation is", "cit": "[CLS] word alignment and tokenization can also be optimized by"}
{"pre": "[CLS] the parsing model is a reimplementation of the averaged", "cit": "[CLS] it is also interesting to note that the best"}
{"pre": "[CLS] the feature weights were tuned on the development set", "cit": "[CLS] the binarization method used by #refr can cover many"}
{"pre": "[CLS] the most common approach to deriving translation lexicons from", "cit": "[CLS] in contrast, approaches to wsd attempt o take advantage"}
{"pre": "[CLS] we use the same set of features as in", "cit": "[CLS] one option would be to leverage unannotated text #otherefr;"}
{"pre": "[CLS] the task of paraphrasing tweets is also related to", "cit": "[CLS] initially all sentences are pre-processed by the corenlp #otherefr;"}
{"pre": "[CLS] the parsing algorithm is based on the algorithm of", "cit": "[CLS] our data source is the german negra treebank #refr."}
{"pre": "[CLS] the first is the lbxas algorithm #refr which uses", "cit": "[CLS] automatic detection of such argument alternations is important to"}
{"pre": "[CLS] the first is the internet slang dictionary from #refr.", "cit": "[CLS] only for the machine translation task, #refr report several"}
{"pre": "[CLS] the most common approach to handling the unbounded nature", "cit": "[CLS] next, other research efforts utilized the three categories into"}
{"pre": "[CLS] #refr proposed a method for summarizing scientific articles by", "cit": "[CLS] this is because in citations, the discussion of the"}
{"pre": "[CLS] #refr proposed a method for identifying the polarity of", "cit": "[CLS] the work of #otherefr; #refr focuses on manually constructing"}
{"pre": "[CLS] we use the same feature representation ?(x, y) as", "cit": "[CLS] an appealing alternative to a similar approximation to the"}
{"pre": "[CLS] the first is to relax or update the independence", "cit": "[CLS] finally, a few efforts #otherefr; #refr have tried to"}
{"pre": "[CLS] the id task #refra) concerns the extraction of events", "cit": "[CLS] therefore, we prepare single bilingual signs for expressing their"}
{"pre": "[CLS] we use the same feature representation ?(x, y) as", "cit": "[CLS] #refr use shift-reduce parsing to impose itg #otherefr constraints"}
{"pre": "[CLS] the first is the observation of #refr, who use", "cit": "[CLS] similar tagsets are used by other studies on grammar"}
{"pre": "[CLS] we use the same dataset as in #refr which", "cit": "[CLS] a variety of features have been explored for erd"}
{"pre": "[CLS] the semeval-2007 task 04 #otherefr task 08 #refr aimed", "cit": "[CLS] however, it is fair to compare our work against"}
{"pre": "[CLS] the first is the internet slang dictionary from #refr.", "cit": "[CLS] this feature set is very close to that used"}
{"pre": "[CLS] the grammar we used is the implementation of the", "cit": "[CLS] the inside weight w i and the outside weight"}
{"pre": "[CLS] we use the same feature set as in #refr.", "cit": "[CLS] various recent attempts have been made to include non-local"}
{"pre": "[CLS] the model is trained using a variant of the", "cit": "[CLS] this feature set is reminiscent of the featurized representation"}
{"pre": "[CLS] the work of #refr is closest in spirit to", "cit": "[CLS] for example, a popular approach to reduce annotation effort"}
{"pre": "[CLS] the model of #refr has recently been applied to", "cit": "[CLS] these efforts have been met with some success in"}
{"pre": "[CLS] the feature sets we used are similar to other", "cit": "[CLS] our use of an lsvm to assign credit during"}
{"pre": "[CLS] the most common approach to handling the unbounded nature", "cit": "[CLS] the two most popular algorithms are mcdonald?s mst- parser"}
{"pre": "[CLS] the most popular seed expansion methods discussed in the", "cit": "[CLS] #refr developed a method in which they first identify"}
{"pre": "[CLS] we use the stanford corenlp suite to lemmatize and", "cit": "[CLS] we follow the candidate-ranking model proposed by #refr. [SEP]"}
{"pre": "[CLS] we use the same feature representation ?(x, y) as", "cit": "[CLS] recent randomized language models #refr make use of bloom"}
{"pre": "[CLS] the feature weights were tuned on the development set", "cit": "[CLS] #refr introduced the concept of lexical weighting to check"}
{"pre": "[CLS] the morphological disambiguation of arabic morphology was then performed", "cit": "[CLS] much work has been done on automatic arabic diacritization"}
{"pre": "[CLS] we use the dataset from #refr which consists of", "cit": "[CLS] this kind of supervision is similar to the seeding"}
{"pre": "[CLS] the character-based ?iob? tagging approach has been widely used", "cit": "[CLS] in our system, the well-known forward maximum matching algorithm"}
{"pre": "[CLS] the most popular seed expansion methods discussed in the", "cit": "[CLS] the wordlevel sentiment annotation is to utilize the polarity"}
{"pre": "[CLS] the system was tuned on the development set devtest2006", "cit": "[CLS] this alignment matrix contains scores for all word correspondences"}
{"pre": "[CLS] we use the stanford parser #refr to parse the", "cit": "[CLS] it has already been proposed for phrase-based #otherefr, hierarchical"}
{"pre": "[CLS] the first is the lbxas algorithm #refr which uses", "cit": "[CLS] class-based approaches #otherefr and #refr are more promising: the"}
{"pre": "[CLS] the most common approach is to use beam search", "cit": "[CLS] in contrast, #refr show that parser agreement is a"}
{"pre": "[CLS] the first is the method of #refr, which we", "cit": "[CLS] for comparison, we also include results for a setting"}
{"pre": "[CLS] we use the stanford parser #refr to parse the", "cit": "[CLS] in recent years discriminative probabilistic models have been successfully"}
{"pre": "[CLS] we use the same non-parallel spanish/english corpus used in", "cit": "[CLS] these records are also known as field books and"}
{"pre": "[CLS] the model of #refr is based on the concept", "cit": "[CLS] that parsing model has since been extended to make"}
{"pre": "[CLS] the system uses the illinois model described in #refr", "cit": "[CLS] this is also the task addressed by other wsd"}
{"pre": "[CLS] the first is the method described in #refr, which", "cit": "[CLS] statistical data about these various cooccurrence r lations is"}
{"pre": "[CLS] the first is a task-independent system #refr which uses", "cit": "[CLS] he has achieved state-of-the art results by applying m.e."}
{"pre": "[CLS] the first is the cornell sentence extraction tool #refr", "cit": "[CLS] among the machine learning algorithms studied, rule based systems"}
{"pre": "[CLS] the most widely used metrics are based on bleu", "cit": "[CLS] rather than relying on volunteers or [PAD] nlp research"}
{"pre": "[CLS] the model weights are optimized to maximize bleu #refr", "cit": "[CLS] other researchers #otherefr; #refr have reported performance gains in"}
{"pre": "[CLS] the first is the task of identifying the semantic", "cit": "[CLS] in addition, timeline summarization techniques #otherefr and some event-event"}
{"pre": "[CLS] we use the stanford corenlp suite to lemmatize and", "cit": "[CLS] for example, a memorization feature is a word pair"}
{"pre": "[CLS] the system developed for the shared task on semantic", "cit": "[CLS] experiments have been made on the automatic acquisition of"}
{"pre": "[CLS] the most common approach to deriving translation lexicons from", "cit": "[CLS] fortunately, there is a growing body of work on"}
{"pre": "[CLS] we use the same dataset as in #refr, which", "cit": "[CLS] this assumption, or its minor relaxations, is relatively standard"}
{"pre": "[CLS] the first is the pyramid method #refr which is", "cit": "[CLS] other research has introduced the notion of identifying concepts"}
{"pre": "[CLS] #refr proposed a method for extracting semantic similarity between", "cit": "[CLS] early work on automatically inducing semantic relations between words,"}
{"pre": "[CLS] we use the stanford corenlp suite to lemmatize and", "cit": "[CLS] experiments in both #otherefr and #refr find no conclusive"}
{"pre": "[CLS] the work of #refr has approved its effectiveness in", "cit": "[CLS] first, as in #refr, we stipulate that some attributes"}
{"pre": "[CLS] #refr used a random walk model to predict word", "cit": "[CLS] active learning has been shown, for a number of"}
{"pre": "[CLS] the first is the method described in #refr which", "cit": "[CLS] therefore, svms have shown good performance for text categorization"}
{"pre": "[CLS] we use the stanford corenlp suite to lemmatize and", "cit": "[CLS] first, we briefly introduce our method for constructing ncfs"}
{"pre": "[CLS] the first is the proposal and use of semantic", "cit": "[CLS] traditionally, broad coverage has always been considered to be"}
{"pre": "[CLS] the model is trained using a variant of the", "cit": "[CLS] common combination methods include the union or intersection of"}
{"pre": "[CLS] we use the stanford parser #refr to parse the", "cit": "[CLS] following #refr, we use an n-best list of 10000"}
{"pre": "[CLS] we use the same dataset as in #refr. [SEP]", "cit": "[CLS] wasp #refr is a model motivated by statistical synchronous"}
{"pre": "[CLS] the first is the movie review dataset of #refr,", "cit": "[CLS] the resource presented in #refr uses a similar binomial"}
{"pre": "[CLS] the model is trained using a variant of the", "cit": "[CLS] previous approaches #otherefr; #refr have performed this task by"}
{"pre": "[CLS] the most common approach to deriving translation lexicons from", "cit": "[CLS] the method #otherefr aims to detect omission-type and replacement-type"}
{"pre": "[CLS] the model of #refr is based on the same", "cit": "[CLS] in recent years, conditional random fields #otherefr and information"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr examines semantic", "cit": "[CLS] this is a significant improvement with respect to previous"}
{"pre": "[CLS] we use the same feature representation ?(x, y) as", "cit": "[CLS] as a consequence, finding the highest scoring parse tree"}
