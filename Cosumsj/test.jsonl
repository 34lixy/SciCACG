{"pre": "[CLS] the first is the case for the semantic role labeling of the predicate-argument structure of the input text with", "cit": "[CLS] there is a substantial body of literature on the semantics of english complement constructions starting with #otherefr; #refr. [SEP]"}
{"pre": "[CLS] the translation quality is measured by bleu #refr. [SEP] [SEP] = 4. [SEP] = 4. [SEP] = ? i", "cit": "[CLS] the generated paraphrases along with their source head- 1this list of aligned pairs is available at http://ilk.uvt.nl/?swubben/resources.html lines are"}
{"pre": "[CLS] the system is based on the inprotk toolkit for dialogue systems developed by #refr. [SEP] system, which uses a", "cit": "[CLS] this methodology was first used in nlg in the mid-1990s by #refr and lester and porter #otherefr, and continues"}
{"pre": "[CLS] the system uses a log-linear combination of #refr. [SEP] model to predict the best derivation of the best derivation", "cit": "[CLS] word representations, especially brown clustering, have been demonstrated to improve the performance of ner system when added as a"}
{"pre": "[CLS] the first is the automatic evaluation of discourse structure of discourse structure of discourse structure of discourse structure of", "cit": "[CLS] as it currently stands, the vital/okay dichotomy is troublesome because there is no way to operationalize such a classification"}
{"pre": "[CLS] #refr use a similar approach to identify the polarity of adjectives. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] we use the brown coherence toolkit7 #refr to construct the grids. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to translate the phrases as the", "cit": "[CLS] table 2: impact of truecasing on case-sensitive bleu in a more integrated approach, factored translation models #refr allow us"}
{"pre": "[CLS] #refr use a supervised sentiment classifier to detect the polarity of words in a target words and their polarity", "cit": "[CLS] if a sentiment lexicon is available for one domain, domain adaptation can be used, provided the domains are sufficiently"}
{"pre": "[CLS] the generation system is based on the inprotk project #refr that we will use the assumption that the assumption", "cit": "[CLS] for example, the direction assistant system #refr mapped a hand-crafted route grammar to a discourse structure for generated irections."}
{"pre": "[CLS] the work of #refr is similar to ours in that it is possible to use a more complex model", "cit": "[CLS] there are many similarity functions proposed in the literature, see e.g., #refr; we use the cosine between the distributional"}
{"pre": "[CLS] the most common approach to paraphrase extraction is to use a paraphrase database into a translation model #otherefr; #refr.", "cit": "[CLS] figure 2: definition and instructions for annotation to provide a one-sentence description of the main action or event in"}
{"pre": "[CLS] the first is the task of identifying the polarity of a text that is a text that a text", "cit": "[CLS] projecting information available in one language onto another has been explored in areas such as part-of-speech tagging #otherefr; #refr,"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] initially work focused on word-based alignment, but more and more work is also addressing alignment at the higher levels"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] propbank contains about 53,700 sentences and a fixed split between training and testing which has been used in other"}
{"pre": "[CLS] the second approach is to use a small number of seed sets of words and their similarity and their", "cit": "[CLS] among the existing sense-tagged corpora, the semcor corpus #refr is one of the most widely used. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to paraphrase generation is to paraphrase generation from multiple translation #otherefr; #refr. [SEP] tables by", "cit": "[CLS] paraphrases, i.e. differing textual realizations of the same meaning, are a crucial components of text-to-text generation systems, and have"}
{"pre": "[CLS] in recent years, there has been a growing interest in the task of generating referring expressions, reflected in the", "cit": "[CLS] more recently, #refr attempt a similar task, but this time in an interactive navigational domain; as well as determining"}
{"pre": "[CLS] the most common approach to paraphrase generation is to paraphrase generation from corpora #otherefr; #refr. [SEP] words to a", "cit": "[CLS] it might be possible to achieve similar effects using images or panels of images as the stimulus #otherefr; #refr,"}
{"pre": "[CLS] the first is the automatic evaluation of discourse relations #refr. [SEP] of the discourse markers in the discourse treebank", "cit": "[CLS] tempeval #refr, in 2007, and more recently tempeval-2 #otherefr, were concerned with this problem. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the process of annotating the word segmentation and then the word segmentation model of a word", "cit": "[CLS] we evaluate the performance of aps on three tasks: finding topical boundaries in transcripts of course lectures #otherefr, identifying"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] a variety of computational approaches to metaphorical language have been developed, e.g., #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the weights of the log-linear model are optimized with minimum error rate training (mert) #refr. [SEP] bleu score #refr", "cit": "[CLS] as always, trade-offs exist between time, space, and accuracy, with many recent papers considering smallbut-approximate noisy lms #otherefr or"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within the meaning representation of the meaning of the", "cit": "[CLS] much more attention has been devoted to discourse-level constraints on adjacent sentences indicative of coherence and good text flow"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] opinion mining spans a variety of subtasks including: creating opinion word lexicons #otherefr, identifying opinion expressions #refr, identifying polarities"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a task of identifying the degree of semantic relations between words", "cit": "[CLS] we can find a similarity also to the pagerank algorithm #otherefr, which has been applied also to natural language"}
{"pre": "[CLS] the first is the task of identifying the event descriptions of the bionlp?09 shared task on event extraction #refr.", "cit": "[CLS] related work on interpreting temporal expressions has focused on constructing hand-crafted interpretation rules #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second approach is to use a semantic parser to generate the parse tree and the parse tree in", "cit": "[CLS] 2.1.1 features and search the feature function?syn is a factored representation, meaning that we compute the score of the"}
{"pre": "[CLS] the second approach is to use a semantic similarity measure as defined by #refr and is to measure the", "cit": "[CLS] we conducted experiments on the task of word sense disambiguation of s3ls data, this time not just on the"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the entire sequence of the entire", "cit": "[CLS] this approach strongly improved performance on cognate identification, while variations of it have also proven successful in transliteration discovery"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for dependency parsing #refr. [SEP] grammars", "cit": "[CLS] graph-based dependency parsers parameterize models directly over substructures of the tree, including single arcs #refr, sibling or grandparent arcs"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to learn a translation models from a", "cit": "[CLS] possibly the most remarkable evolution of recent years in statistical machine translation is the step from word-based models to"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for each sentence with dependency trees for", "cit": "[CLS] the differences between the performance of ind[s+p] and mono are statistically significant in the bootstrap method #refr, with a"}
{"pre": "[CLS] in this paper, we present a state-of-the-art dependency parsing algorithm for dependency parsing #refr. [SEP] parsing is the one", "cit": "[CLS] we also present a reranking of the n -best parses produced by bitpar #refr, a state of the art"}
{"pre": "[CLS] the system used in this paper is based on the stanford parser #refr and the stanford parser #otherefr. [SEP]", "cit": "[CLS] state-of-the-art approaches include both learningbased #otherefr; #refr and deterministic models #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the unsupervised pos induction task has been extensively studied in the nlp community, and has been studied #otherefr; #refr.", "cit": "[CLS] naseem et al #otherefr and #refr study related but different multilingual grammar and tagger induction tasks, where it is"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to learn a translation probabilities from", "cit": "[CLS] #refr explored multiple decomposition structures for generating mtus in the task of lexical selection, and to rerank the n-best"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] in a slightly more general formulation, it was first published by #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to translation lexicons from the target language model is the target side of the target", "cit": "[CLS] rtms rank 1st in all of the tasks and subtasks of qet14 #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the system is based on the inprotk toolkit for the paradise system described in #refr. [SEP] system, pharaoh #otherefr", "cit": "[CLS] prominent examples include nasa?s clarissa procedure navigator #refr, geneva university?s multi-modal mobile-platform calendar application #otherefr internal technology fair, and"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] #refr introduced minimum error rate training #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to compute the polarity of the polarity of the polarity of the polarity of", "cit": "[CLS] the most commonly used datasets include: the mpqa corpus of news documents #otherefr, amazon review data #refr, the jdpa"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with the one of", "cit": "[CLS] other related work has been focused on semisupervised parsing methods which utilize labeled data to annotate unlabeled data, then"}
{"pre": "[CLS] the first is the task of coreference resolution #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] ? the typed-similarity [SEP]", "cit": "[CLS] however, as suggested by #refr, there is no need to generate the model following that order. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to paraphrase acquisition is the source sentences into a target language #otherefr; #refr. [SEP] input", "cit": "[CLS] several researchers #otherefr, #refr, van der plas and bouma #otherefr) have used large monolingual corpora to extract distributionally similar"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of meaning that are not", "cit": "[CLS] the term selector comes from #refr, and refers to a word which can take the place of another given"}
{"pre": "[CLS] we use the stanford parser #refr to extract dependency relations for each sentence and extract the text from the", "cit": "[CLS] shorter words #otherefr, less complex syntax #refr and high cohesion between sentences #otherefr typically indicate easier and more ?readable?"}
{"pre": "[CLS] the task of identifying the semantic relations between nominals of a document has been shown to be a text", "cit": "[CLS] although traditional research on taxonomy construction focuses on extracting local relations between concept pairs #otherefr; gi#refr, more recent efforts"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] we have modified kriya #otherefr, an in-house implementation of hierarchical phrase-based translation system #refr, to implement ensemble decoding using"}
{"pre": "[CLS] the most common approach to this problem is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] caraballo #refr also exploited these syntactic structures and applied a cosine vector model to produce semantic groupings. [SEP] [PAD]"}
{"pre": "[CLS] #refr used a supervised sentiment classification approach to classify reviews as positive or negative or negative or negative or", "cit": "[CLS] another thing worth noting is that, in regression, gps are outperformed by both versions of 6in semeval 2010, only"}
{"pre": "[CLS] the most popular approach to translation models #otherefr; #refr rely on the target language model and the target language", "cit": "[CLS] we report case sensitive bleu #refr scorebleuc for all experiments. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the automatic acquisition of subcategorization frames from corpora #otherefr; #refr. [SEP] the original text with the", "cit": "[CLS] regarding the function l , wordnet is constructed such that always picking the first sense of a given nominal"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be np-hard #refr. [SEP] s", "cit": "[CLS] handcrafted rules #otherefr; #refr for preordering training data and system input have been explored in numerous publications. [SEP] [PAD]"}
{"pre": "[CLS] the translation quality is measured by bleu #refr. [SEP] [SEP] [SEP] = 4. [SEP] = 4. [SEP] = ?", "cit": "[CLS] previous work has focused on automatically learning and integrating translations of very specific mwe categories, such as, for instance,"}
{"pre": "[CLS] the first is the task of coreference resolution #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] ? ? ? ? ?", "cit": "[CLS] many systems form the chains by joining each positively-classified pair (i.e. singlelink) or with simple improvements such as linking"}
{"pre": "[CLS] the most popular approach to translation models #otherefr; #refr rely on parallel data #otherefr. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] there has been a large body of work showing the efficacy of preordering source sentences using a source parser"}
{"pre": "[CLS] we use the stanford named entity recognizer #refr to extract named entities from the named entities from the stanford", "cit": "[CLS] there are many ways to represent events, ranging from role-based representations such as frames #otherefr and narrative schemas #refr."}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] if the system could explore semantic information beyond the scope of its local knowledge and use external knowledge sources"}
{"pre": "[CLS] we use the berkeley parser #refr to train the feature weights for each model with a log-linear model with", "cit": "[CLS] again, the equivalence relation will in general not be compatible with the parsing algorithm, so the k-best lists can"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] all non-projective dependencies in the training and gold sets were projectivized prior to training and parsing using the algorithm"}
{"pre": "[CLS] the most common approach to paraphrase generation is to either by using syntactic information #otherefr; #refr. [SEP] words to", "cit": "[CLS] #refr, zhou et al. #otherefr). [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to compute similarities between words in a sentence. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] for treebanks with non-projective trees we use the pseudo-projective parsing technique to transform the treebank into projective structures #refr."}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] some examples of the idealistic approach are the direct ibm word model #refr, the phrase-based approach of marcu and"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the input is to be", "cit": "[CLS] to incorporate the supervision on words and documents at same time into the active learning scheme, recently an active"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with a probability distribution", "cit": "[CLS] prior work in this area has considered the effect of splitting and merging these states #otherefr; #refr, as well"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] of a text in", "cit": "[CLS] #refr showed that personal and demonstrative pronouns are used in contrasting situations: personal pronouns are preferred when both the"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to learn a translation models from a", "cit": "[CLS] some of them developed syntax-based models on complete syntactic trees with treebank annotations #refr, and others used source-language syntax"}
{"pre": "[CLS] the first is the task of identifying the event extraction of event descriptions of event descriptions of event descriptions", "cit": "[CLS] event-event relations have been mostly studied from the perspective of temporal ordering; e.g., #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the overall sentence pairs from the input text with a text that", "cit": "[CLS] authors hid rules behind euphemisms such as ?dependency restrictions? #otherefr, ?entity type constraints? #refr, or ?seed dictionaries? #otherefr. [SEP]"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] shared", "cit": "[CLS] while uturku (bjo?#refr and vibghent #otherefr pursue approaches building on dictionaryand rule-based extraction. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking approaches #otherefr; #refr to learning for parsing", "cit": "[CLS] unambiguity regularization is also related to the minimum entropy regularization framework for semi-supervised learning #otherefr; #refr, which tries to"}
{"pre": "[CLS] the most common approach to paraphrase extraction is to use a bilingual lexicon from comparable corpora #otherefr; #refr. [SEP]", "cit": "[CLS] in this work, we extend an existing machine translation metric, terp #refra), by adding support for more detailed feature"}
{"pre": "[CLS] in this paper, we propose a discriminative model for training using a discriminative model based on the averaged perceptron", "cit": "[CLS] for example, consider the dependency grammar induction results shown in table 1 when training the 1it is not strictly"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to learn a translation models from a", "cit": "[CLS] in the original work #otherefr the posterior probability p(ei1|fj1 ) is decomposed following a noisy-channel approach, but current stateof-the-art"}
{"pre": "[CLS] the work of #refr is similar to ours in that it is possible to use discourse relations that the", "cit": "[CLS] #refr show a difference across level 1 senses (comparison, contingency, tem- poral and expansion) in the pdtb in terms"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within the source sentence into a target sentence into", "cit": "[CLS] 3.2.3 precision grammar features following #refr and wagner et al. #otherefr, we use features extracted from precision grammar parsers."}
{"pre": "[CLS] the feature weights were tuned using minimum error rate training (mert) #refr on the development set using minimum error", "cit": "[CLS] parameter settings we tune our system toward approximate sentence-level bleu #refr,3 and the decoder is configured to use cube"}
{"pre": "[CLS] the first is the task of identifying the sentence planning problem that the central role of determining the user", "cit": "[CLS] since the full brevity algorithm #otherefr, the locative algorithm #refr, and graph-based approaches #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of identifying positive and negative sentiments from a passage, #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] most works dealing with contradiction recognition up till now #otherefr; #refr focus on recognizing contradictions between full sentences or"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the word alignments for each sentence and then extract the word", "cit": "[CLS] in work that is closely related to ours, #refr formulated word reordering as a linear ordering problem (lop), an"}
{"pre": "[CLS] the task of relation extraction has received much attention in recent years #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] while one goal of preemptive ie is to avoid relation-specificity, preemptive ie does not emphasize web scalability, which is"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] however, while discriminative models promise much, they have not been shown to deliver significant gains 1we class approaches using"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of possible antecedents for the", "cit": "[CLS] these approaches are not mutually exclusive and there are, of course, some hybrid cases, for example #refr uses information"}
{"pre": "[CLS] the approach of #refr is based on the assumption that the assumption of the assumption that the assumption of", "cit": "[CLS] statistical significance is tested on the bleu metric using paired bootstrap resampling #refr with n = 1000 and p"}
{"pre": "[CLS] the feature weights were tuned using mert #refr on the development set. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] this is similar to the pcfg feature proposed by #refr and is intended to encourage the production of syntactically"}
{"pre": "[CLS] the system used in this paper is based on the c&c parser #otherefr and the c&c parser #refr. [SEP]", "cit": "[CLS] a recent approach to this problem #refr #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a morphological analyzer and morphological analyzer and morphological analysis to train a pos tagging method for arabic", "cit": "[CLS] the only work on arabic tagging that uses a corpus for training and evaluation (that we are aware of),"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to learn a translation models from a", "cit": "[CLS] as an alternative option to our verb-modifier experiments, structured language models #refr might be considered to improve clause coherence,"}
{"pre": "[CLS] the first is the case for the generation of referring expressions from the dialogue manager sends dialogue acts (ddas)", "cit": "[CLS] to achieve incrementality, most dialogue systems employ an incremental chart parser #otherefr; #refr etc.). [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of converting natural language processing #otherefr; #refr. [SEP] [SEP] [SEP] to be a given", "cit": "[CLS] moreover, the automatic feature weighting in the similarity metric of a memory-based learner makes the approach well-suited for domains"}
{"pre": "[CLS] the system is based on the inprotk toolkit #otherefr and the standard implementation of the bleu score #refr for", "cit": "[CLS] bleu #refr, nist #otherefrmetric require reference translation to compare it with mt output in fully automatic mode, which resulted"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] the integrated model is based on a fullylexicalized probabilistic model for japanese syntactic and case structure analysis #refrb). [SEP]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] however, researchers have shown that this performance gap diminishes when using a larger distortion limit #refr and may disappear"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] in other cases, they can be constructed automatically or semi-automatically using any of several methods #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a similar to ours of identifying the degree of semantic relations", "cit": "[CLS] #refr use statistical measures of syntactic behaviour to gauge whether a verb and noun combination is likely to be"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] the", "cit": "[CLS] tempeval-2 #refr extended tempeval-1, growing into a multilingual task, and consisting of six subtasks rather than three. [SEP] [PAD]"}
{"pre": "[CLS] the translation system is evaluated using bleu #refr, which is calculated by the quality of the case-insensitive bleu-4 metric", "cit": "[CLS] of these, only feature weights can be trained, for which we used minimum error rate training with version 1.04"}
{"pre": "[CLS] the feature weights were tuned using minimum error rate training (mert) #refr on the development set using the development", "cit": "[CLS] since its introduction, the minimum error rate training (mert) #refr method has been the most popular method used for"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] #refr create ?distributional profiles? for frames. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the translation quality is measured by bleu #refr. [SEP] [SEP] = 4. [SEP] = 4. [SEP] = ? i", "cit": "[CLS] if in-domain and out-of-domain lms are usually mixed with the well-studied interpolation techniques, training translation models from data of"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] we segmented the chinese data using the stanford chinese segmenter #refr in ?ctb? mode, giving us 7.9m chinese tokens"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] sentence pair of a", "cit": "[CLS] an expectation-maximization approach to ne disambiguation problem was reported by #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] in statistical parsing literature, it is common to see parsers trained and tested on the same textual domain #otherefr;"}
{"pre": "[CLS] the system is based on the moses toolkit #otherefr and the moses toolkit #refr. [SEP] system, pharaoh decoder with", "cit": "[CLS] in addition, a bilingual language model #refr is used as well as a discriminative word lexicon #otherefr using source"}
{"pre": "[CLS] the most common approach to this problem is to use a dictionary to model the decision to generate a", "cit": "[CLS] the majority of previous papers in this area have presented machine learning methods with models being trained on well-formed"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with the one of", "cit": "[CLS] #refr used transformed constituency treebank from prague dependency treebank for constituent parsing on czech. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained on the penn treebank corpus and tested on the penn treebank wall street journal, parser", "cit": "[CLS] very bright research has been carried out in this field, resulting in several top performing completely automatic parsers #otherefr;"}
{"pre": "[CLS] the most common approach to dependency parsing is to use a dependency parser #otherefr; #refr. [SEP] parser to extract", "cit": "[CLS] the use of this algorithm was inspired by #refr, who used the top branches of the cluster hierarchy as"}
{"pre": "[CLS] the second approach is to use a small number of seed examples for training data #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] na??ve bayes models #otherefr) as well as maximum entropy models (e.g., #refr, klein and manning #otherefr) in particular have"}
{"pre": "[CLS] the feature weights were tuned using minimum error rate training (mert) #refr on the development set using the development", "cit": "[CLS] the basic model of the our system is a log-linear model #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to identify the discourse relations in which a discourse structure of discourse structure of", "cit": "[CLS] #refr proposed a document compression method that directly models the probability of a summary given an rst-dt by using"}
{"pre": "[CLS] the first is the same as #refr, which is the most popular treebank as the most popular treebank of", "cit": "[CLS] over the past decade, there has been tremendous progress on learning parsing models from treebank data #refr. [SEP] [PAD]"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] to set up our baseline results, we used an extensively optimized standard back-off 4-grams language model using kneser-ney smoothing"}
{"pre": "[CLS] the grammar is a grammar that is a parsing model that of #refr and a probability distribution over the", "cit": "[CLS] several parsing algorithms for lcfrs or equivalent formalisms are found in the literature; see for instance #otherefr; #refr. [SEP]"}
{"pre": "[CLS] the first is the process of annotating the training data with the same as described in #refr. [SEP] test", "cit": "[CLS] lately, there has been a lot of work on acquiring paradigms as part of the word-segmentation problem #otherefr; #refr."}
{"pre": "[CLS] the most common approach to joint inference is to use beam search #otherefr; #refr. [SEP] model the entire sequence", "cit": "[CLS] we used the first-stage pcfg parser of charniak and johnson #otherefr for english and bitpar #refr for german. [SEP]"}
{"pre": "[CLS] we use the stanford parser #refr to extract dependency relations from the dependency trees for each sentence with dependency", "cit": "[CLS] supervised methods include hidden markov model (hmm), maximum entropy, conditional random fields (crf), and support vector machines (svm) #refr."}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] since language transfer occurs when grammatical structures from a first language determine the grammatical structures of a second language,"}
{"pre": "[CLS] the first is the process of annotating the correct grammatical relations between the words in a sentence and then", "cit": "[CLS] note that there may also be syntactic constraints on what may fit into one sentence (e.g., #refr, describe how"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal (wsj) corpus #refr. [SEP] trees and the penn", "cit": "[CLS] few hand-crafted, deep linguistic grammars achieve the coverage and robustness needed to parse large corpora #otherefr; #refr for approaches"}
{"pre": "[CLS] the first is the case for the parsing of the english language model of the penn treebank #otherefr, which", "cit": "[CLS] the morphological clues we use for english are taken directly from the berkeley parser #refr and those for french"}
{"pre": "[CLS] the task of identifying the event descriptions of event extraction and event descriptions has been addressed in the extraction", "cit": "[CLS] in future work we will adapt our machine-learning-based relation extraction component #refr to recognise relations between spatial and temporal"}
{"pre": "[CLS] the parser is trained using the c&c parser #refr. [SEP] parser is trained on the wall street journal (wsj)", "cit": "[CLS] furthermore, to test the grammar precision and accuracy, we use two treebanks: redwoods #refr for english and hinoki #otherefr"}
{"pre": "[CLS] the grammar is a grammar that is a grammar that is a grammar that is a grammar that is", "cit": "[CLS] following #refr, each candidate (x, z) is also represented as a string y. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the mentions of a single sentence and a sentence #refr. [SEP] sentence", "cit": "[CLS] the majority of the features presented here have already been used in the cr system for czech #refr; we"}
{"pre": "[CLS] #refr use a bootstrapping method to identify opinion holders and extract opinion holders and extract opinion expressions from documents", "cit": "[CLS] for example, sentiment analysis/opinion mining from unstructured user generated content such as online reviews and blogs often relies on"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the training data is to", "cit": "[CLS] hence, state-of-the-art parsers either supplement the part-of-speech #otherefra), or learn finer grained tag distinctions using a heuristic learning procedure"}
{"pre": "[CLS] in this paper, we propose a new model to use a discriminative model to generate multiple translation of word", "cit": "[CLS] researchers also introduce topic model for crosslingual language model adaptation #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the semantic textual similarity of a text of a text of a", "cit": "[CLS] we chose to use the distributional similarity score described by lin #otherefr because it is an unparameterized measure which"}
{"pre": "[CLS] in the last decade, there has been a lot of work on joint inference in nlp #otherefr; #refr. [SEP]", "cit": "[CLS] previously, this problem has been addressed using, for example, k-best beam search #otherefr; #refr and parallelization #otherefr. [SEP] [PAD]"}
{"pre": "[CLS] the most popular approach to word alignment is the hmm alignment model #refr. [SEP] model is based on the", "cit": "[CLS] one of the major problems with the ibm models #otherefr and the hmm models #refr is that they are"}
{"pre": "[CLS] the task of identifying the semantic textual similarity of a text is a text of a text #refr. [SEP]", "cit": "[CLS] in addition, it includes a search algorithm which finds an optimal sequence of transformations for any given t/h pair"}
{"pre": "[CLS] the most common approach to parsing is to use a linear model trained on a maximum spanning tree (mst)", "cit": "[CLS] such methods stand in sharp contrast to partially supervised techniques that have recently been proposed to induce hidden grammatical"}
{"pre": "[CLS] the system is based on the stanford parser #refr and the stanford parser #otherefr. [SEP] system #otherefr. [SEP] system", "cit": "[CLS] for each spurt thus processed, pos tags were obtained automatically with the stanford tagger #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for parsing #otherefr; #refr for parsing", "cit": "[CLS] datadriven dependency parsers such as those by nivre et al#otherefr, #refr or huang and sagae #otherefr are accurate and"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] talbanken05 is a swedish treebank converted to dependency format, containing both written and spoken language #refra).1 for each token,"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal (wsj) corpus #refr. [SEP] trees and the penn", "cit": "[CLS] figure 1: dependency forest for a czech sentence from the prague dependency treebank some authors extend dependency forests by"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] #refr overcome the restrictiveness of the syntax-only model by starting with a complete set of phrases as produced by"}
{"pre": "[CLS] the system is based on the standard approach of #refr and the best system for machine translation (mt) systems", "cit": "[CLS] long sentences are removed, and the remaining sentences are pos-tagged and dependency parsed using the pre-trained stanford parser #refr."}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within the source sentence into a target sentence into", "cit": "[CLS] the sentences were parsed with charniak?s parser #refr in order to calculate these features. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be np-hard #refr. [SEP] s", "cit": "[CLS] the value of human annotated syntactic structures for statistical machine translation has been clearly demonstrated in string-to-tree #otherefr; #refr,"}
{"pre": "[CLS] the most common approach to paraphrase generation is to use a lexicon to automatically extracted from corpora #otherefr; #refr.", "cit": "[CLS] cross-lingual semantic similarity between words can be measured using standard vector space similarity #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] the pipeline also supports ghkm grammar extraction #refr using the extractors available from michel galley2 or moses. [SEP] [PAD]"}
{"pre": "[CLS] the most common approach to paraphrase generation is to use a lexicon to paraphrase database into a single sentence", "cit": "[CLS] for example, one may want a text to be shorter #otherefr, tailored to some reader profile #refr, compliant with"}
{"pre": "[CLS] the first is the case for the automatic acquisition of subcategorization frames from the web #otherefr; #refr. [SEP] test", "cit": "[CLS] #refr found that 54% of word tokens are used with their first (or default) sense. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to joint unsupervised learning of dependency parsers is to use syntactic structures in nlp tasks", "cit": "[CLS] we describe an extension of semisupervised structured conditional models (ss-scms) to the dependency parsing problem, whose framework is originally"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a similar to ours of identifying the degree of semantic relations", "cit": "[CLS] #refr identify multi-word paraphrases from a sentence-aligned corpus of monolingual parallel texts. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the semantic textual similarity of a text of a text similarity of", "cit": "[CLS] they developed an iterative approach to construct a parallel corpus, using the interlingual links, info boxes, category pages, and"}
{"pre": "[CLS] the most common approach to semantic role labeling is to use syntactic information #otherefr; #refr. [SEP] [SEP] [SEP] is", "cit": "[CLS] we run all questions through the stanford corenlp pipeline #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to handle the reordering of the", "cit": "[CLS] once this is built, we can apply a target language model to l(s, 1, j) to obtain the final"}
{"pre": "[CLS] the first is the task of identifying the overall sentence pairs from the english text using the stanford parser", "cit": "[CLS] temporal information processing is a topic of natural language processing boosted by recent evaluation campaigns like tern2004,1 tempeval-1 #otherefr"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a text that are semantically", "cit": "[CLS] in this paper, we concentrate on adjectives, which have received less attention (see though #refr and lapata #otherefr). [SEP]"}
{"pre": "[CLS] the second approach is to use a word segmentation and pos tagger to tag dictionary to tag each word", "cit": "[CLS] brent and cartwright #otherefr and #refr). [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the averaged perceptron algorithm #refr to train the parameters of the averaged perceptron algorithm #otherefr. [SEP] [SEP]", "cit": "[CLS] all we need to employ the structured perceptron algorithm #refr or the structured svm algorithm #otherefr is a blackbox"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with a probability distribution", "cit": "[CLS] the work of hall and novak #refr is the closest to ours. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for parsing #refr. [SEP] grammars with", "cit": "[CLS] however, greedy transition-based parsers still fall behind search-based parsers #otherefr; #refr with respect to accuracy. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the system is based on the inprotk toolkit for instance in the standard metrics such as bleu #refr and", "cit": "[CLS] iiowever, #refr have shown that knowledge of target-text length is not crucial to the model is i)ertbrmanee. [SEP] [PAD]"}
{"pre": "[CLS] the pos tagging task is to be able to efficiently perform well on a small number of tags and", "cit": "[CLS] many approaches for pos tagging have been developed in the past, including rule-based tagging #otherefr, cyclic dependency networks #refr,"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] #refr discusses methods for learning synchronized elementary tree pairs from a parallel corpus of parsed sentences. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] parsing technologies have improved considerably in the past few years, and high-performance syntactic parsers are no longer limited to"}
{"pre": "[CLS] in this paper, we propose a method for learning the model of #refr. [SEP] model parameters ? ? ?", "cit": "[CLS] recent investigations along this line includes using word-disambiguation schemes #otherefr; #refr with particular translation models, which showed various degrees"}
{"pre": "[CLS] the first is the automatic evaluation of discourse structure of discourse structure of discourse relations #refr. [SEP] [SEP] [SEP]", "cit": "[CLS] research on discourse cue placement has framed our thinking on asking when and where to generate cmcs (dieugenio, moore"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] however, last year?s top scoring system for chinese #refr, which did not participate this year, had a score that"}
{"pre": "[CLS] in this paper, we propose a method for extracting parallel bilingual news articles from monolingual corpora #refr. [SEP] [SEP]", "cit": "[CLS] extractive methods are generally simpler and have dominated the sentence compression literature #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to extract dependency relations for each word and parse trees from the sentence", "cit": "[CLS] still, it generally extracts single-word terms rather than wellformed and compound concepts. #refr proposed similar lexico-syntactic patterns to extract"}
{"pre": "[CLS] in the context of sentiment analysis and opinion mining has been used to extract opinion expressions #otherefr; #refr. [SEP]", "cit": "[CLS] specifically, we construct features using polarity lexicons (used by #refr), da tags (used by (somasundaran local is supervised, as"}
{"pre": "[CLS] the most common approach to paraphrase generation is to use a lexicon to automatically extracted from corpora #otherefr; #refr.", "cit": "[CLS] we used the ?langid.py? tool #refr at the segment level, and report the percentage of sentence pairs where both"}
{"pre": "[CLS] the most popular approach to translation lexicons from parallel corpora #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ?", "cit": "[CLS] #refr use comparable corpora to score an existing spanish-english phrase table extracted from the europarl corpus. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the translation system is evaluated using bleu #refr, which is computed using the best system of the best system", "cit": "[CLS] in contrast to most of the previous approaches to combine the outputs of multiple mt systems #otherefr; #refr, which"}
{"pre": "[CLS] the first is the process of annotating the word segmentation and then the sentence length of the sentence length", "cit": "[CLS] the use of bayesian networks in the interpretation and generation of dialogue was also investigated by #refr. [SEP] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] a majority of these works have used statistical machine translation #otherefr. #refr used a finite-state framework to learn the"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] of a text with", "cit": "[CLS] thus, we also use verb-noun dependencies as evidence in learning #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to mt system combination is the one proposed by #refr, which uses a log-linear combination", "cit": "[CLS] we present a novel approach based on factorization into permutation trees (pets) #refr, and contrast it with kendall ?"}
{"pre": "[CLS] the model is trained using the giza++ implementation of ibm model 4 alignments #refr. [SEP] model 4 alignments were", "cit": "[CLS] a straightforward approach to the alignment matrix is to build a log linear model #refr for the probability of"}
{"pre": "[CLS] in the second international chinese word segmentation bakeoff #otherefr; #refr and the best results are achieved by the best", "cit": "[CLS] table 1: data split in our experiment for english, and zhang and clark?s #refr for chinese. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the feature weights were tuned using minimum error rate training (mert) #refr on the development set using minimum error", "cit": "[CLS] bayes risk minimization first appeared in the speech recognition community #otherefr and #refr achieved a sort of regularization by"}
{"pre": "[CLS] #refr use a similar approach to compute similarities between words in a sentence. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] this aspect of the refinement is inspired by the most frequent sense heuristic for word sense disambiguation #refr, in"}
{"pre": "[CLS] we use the stanford named entity recognizer #refr to extract named entities from the text and extract named entities", "cit": "[CLS] the technique has recently been re-visited #refr in the language modeling framework, where lexically related documents are used to"}
{"pre": "[CLS] the task of identifying the event descriptions of event extraction task has been addressed in the extraction task #otherefr;", "cit": "[CLS] many of the existing works focused only on clinical conditions or disease #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to deriving translation lexicons from a large corpus of text corpus #refr, which is to", "cit": "[CLS] extract ion the final stage uses a statistical bigram extractor to pick an approximation of the most fluentrealization #refrb)."}
{"pre": "[CLS] we use the berkeley parser #refr to obtain the syntactic parse trees for each sentence and the english and", "cit": "[CLS] large-scale experiments (section 5) on chinese-english translation show that our model significantly outperforms the state-ofthe-art single constituency-to-string model by"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] the patternmatching approach proposed by #refr for a similar task for phrase structure trees is extended with machine learning"}
{"pre": "[CLS] #refr use a similar approach to identify the text in which the text in which the context of a", "cit": "[CLS] token-level language models token-based #otherefr), and wikipedia vandalism detection #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the system is based on the moses toolkit #otherefr and the standard implementation of the bleu score #refr for", "cit": "[CLS] unlike bleu, or other n-gram based mt evaluation metrics, meant adopts at outset the principle that a good translation"}
{"pre": "[CLS] the system is based on the inprotk toolkit for the best system described in #refr, a system that uses", "cit": "[CLS] this process was performed using a parser made available by eugine charniak and brown university #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the task of identifying the event descriptions of event extraction and event descriptions has been well studied in the", "cit": "[CLS] there is a partial overlap between bridging and implicit noun roles #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to learn a translation probabilities from", "cit": "[CLS] finally, we have to mention the models by al- onaizan and #refr and green et al #otherefr, who predict"}
{"pre": "[CLS] the learning algorithm used in this paper is a variation of the winnow update rule based learning algorithm #otherefr;", "cit": "[CLS] systems based on perceptron have been shown to be competitive in ner and text chunking #refr we specify the"}
{"pre": "[CLS] the model is trained using the averaged perceptron algorithm #refr. [SEP] score #refr with the best alignment algorithm #otherefr.", "cit": "[CLS] the hybrid transliteration models in each class are defined by discrete mixture between the probability distribution of the two"}
{"pre": "[CLS] the task of relation extraction has received much attention in recent years #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] there has been a line of research on chinese relation extraction, where both feature-based #otherefr; #refr methods have been"}
{"pre": "[CLS] id participant cmu carnegie mellon university #otherefr cu-bojar charles university - bojar #refr cu-depfix charles university - depfix #otherefr", "cit": "[CLS] the difference with respect to all the other submissions is statistically significant at p = 0.05, using pairwise bootstrap"}
{"pre": "[CLS] the first is the polarity of a word is usually analyzed by using the polarity of a lexicon and", "cit": "[CLS] there have been some other attempts to utilize topic models in this regards, but they mainly focussed on latent"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to handle the hierarchical phrase-based model #refr", "cit": "[CLS] for machine translation (mt) between two natural languages, each being generated by a tag, the derivations of the two"}
{"pre": "[CLS] the evaluation metric is bleu #refr, which is based on the reference evaluation. [SEP] score #refr and the former", "cit": "[CLS] additionally, we used bleu #refr, a very popular machine translation evaluation metric, as a feature. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of identifying the semantic textual similarity of a target word?s token is a document #otherefr; #refr. [SEP]", "cit": "[CLS] as far as we know, only #otherefr and #refr learn rules between unary templates. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the semantic textual similarity of a text of a text of a", "cit": "[CLS] differences between models are tested for significance using stratified shuffling #refr, using a standard number of 10000 iterations. [SEP]"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to learn a translation probabilities from", "cit": "[CLS] the first evaluation metric is bleu #refr, a very common accuracy metric in smt that measures n -gram precision,"}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders and opinion holders and opinion holders in opinion holders in", "cit": "[CLS] voll and taboada #otherefr show that adjective-based sentiment classification is improved by examining topicality (whether each sentence is central"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] previous work on defining subtasks within statistical machine translation has been performed on, e.g., noun-noun pair #refr and named"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to translate the phrases as the", "cit": "[CLS] parallel data was aligned using the mttk toolkit #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of meaning that are not", "cit": "[CLS] this feature set is similar to the one used by (ng and #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the translation quality is measured by bleu #refr. [SEP] = 4. [SEP] = 4. [SEP] = n = ?", "cit": "[CLS] confusion networks #refr are the dominant method for system combination. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] specifically, for en-ja translation we use the head finalization pre-ordering method of #otherefrb), and for ja-en translation, we use"}
{"pre": "[CLS] we use the stanford named entity recognizer #refr to extract named entities from the stanford named entities from the", "cit": "[CLS] workstation hosted components #1 crawler,raw tweet buffer #2, 3 process pipeline #4 indexing buffer, indexer/querier #5 web application the"}
{"pre": "[CLS] the most common approach to paraphrase generation is to use a lexicon to paraphrase database into a target language", "cit": "[CLS] a promising alternative approach is to derive the generalizations from distributional information #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second approach is to use a small number of seed examples for example outputs #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] this idea has been developed and applied to a wide variety tasks, including morphological analysis #otherefr; #refr. [SEP] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the semantic relations between nominals of noun phrases and semantic relations #refr.", "cit": "[CLS] work such as #refr or hobbs et al #otherefr might provide a good starting point for this. [SEP] [PAD]"}
{"pre": "[CLS] the first is the discourse structure of discourse structure of discourse structure of discourse structure in the discourse structure", "cit": "[CLS] although most generation systems pipeline decisions #refr, we believe the most efficient and flexible way to integrate constraints in"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with a probability distribution", "cit": "[CLS] this broad characterization includes lexicalized parsers #otherefr, and latent variable parsers #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second approach is to use a maximum entropy model #otherefr, and a log-linear combination of the hidden markov", "cit": "[CLS] heemskerk and van heuven #otherefr; or see #refr for a probabilistic variant). [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the context of machine learning techniques have been applied to many natural language processing tasks with promising results,", "cit": "[CLS] tree kernel methods have found many applications for the task of answer reranking which are reported in #otherefr; #refr."}
{"pre": "[CLS] we use the stanford named entity recognizer #refr to extract named entities from the text and extract named entities", "cit": "[CLS] many summarization systems #otherefr; #refr) include two levels of analysis: the sentence level, where every textual unit is scored"}
{"pre": "[CLS] the first is the case for the word alignment of word alignment of word alignment of word alignment of", "cit": "[CLS] probability driven pp-disambignation #otherefrc), #refr ,#otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the averaged perceptron algorithm #refr to train the parameters of the feature weights for the log-linear models.", "cit": "[CLS] the joshua 1.0 release also included re-implementations of suffix array grammar extraction #otherefr and minimum error rate training #refr."}
{"pre": "[CLS] #refr proposed a method for identifying the polarity of a word in a target word in a target word", "cit": "[CLS] #refr looks at the syntactic fixedness of idiomatic expressions, i.e., how likely they are to take modifiers or be"}
{"pre": "[CLS] #refr use a similar approach to compute similarities between words in a sentence. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] we extracted roughly 180,000 case fl:anles from the bracketed wsj (wall street journal) corpus of the penn tree bank"}
{"pre": "[CLS] #refr use a similar approach to identify the polarity of words in a sentence. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] we use the stanford parser (de marneffe and #refr to produce a dependency graph and consider the resulting undirected"}
{"pre": "[CLS] the system is based on the inprotk toolkit for the best performing system described in #refr. [SEP] system, and", "cit": "[CLS] the atis corpus which contains air travel information data #refr has been chosen for the slu system development and"}
{"pre": "[CLS] we use the stanford corenlp suite to lemmatize and partof-speech tag each word #otherefr, and parse the sentence #refr.", "cit": "[CLS] previously researchers have focused on the interactive aspects of argumentative and decisionmaking dialogue, tackling issues such as the detection"}
{"pre": "[CLS] in the context of machine translation, itg has been explored for statistical word alignment in both the context of", "cit": "[CLS] #refr illustrates several tricks which make this computation feasible in practice. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the grammar is a grammar that is a grammar that is a given a grammar that is a grammar", "cit": "[CLS] use of recta-programming for self-monitoring to ensure generation of unambiguous utterances (neumann and #refr . [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the polarity of a sentence, a sentence, and a sentence, we use", "cit": "[CLS] the tempeval shared tasks #otherefr; #refr have been one of the key venues for researchers to compare methods for"}
{"pre": "[CLS] the idea of using word alignment is similar to that of #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] #refr", "cit": "[CLS] we used an out-of-the-box implementation of the berkeley aligner #refr, a competitive word alignment system, to construct an unsupervised"}
{"pre": "[CLS] #refr use a similar approach to compute sentence importance. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] turner and charniak #refr added some special rules and applied this method to unsupervised learning to overcome the lack"}
{"pre": "[CLS] the second approach is to use a maximum entropy model #otherefr; #refr. [SEP] model to predict the probability of", "cit": "[CLS] for arabic, which has up to quadriliteral roots (k = 5), the time complexity would be o(n15).3 this is"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] inspired by the pcfg approximation idea #refr for deep parsing, we study tree approximation approaches for graph spanning. [SEP]"}
{"pre": "[CLS] the work of #refr is similar to ours in that it is possible to use a more complex model", "cit": "[CLS] these include the potsdam commentary corpus #refr for german #otherefr (several genres; 267 documents; 2,256 sentences). [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr. [SEP]", "cit": "[CLS] examples include rte #refr; string matching #otherefr to tune the inference procedure parameters. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] in machine translation, the concept of packed forest is first used by #refr to characterize the search space of"}
{"pre": "[CLS] the learning algorithm used in this paper is a variation of the winnow update rule based statistical parsing algorithm", "cit": "[CLS] and previous research on english srl shows that combination is a robust and effective method to alleviate srl?s dependency"}
{"pre": "[CLS] the most common approach to this problem is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] we have not seen recursive neural networks (rnn) applied to qa yet, but socher has developed applications to paraphrase"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] ep?#otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the averaged perceptron algorithm #refr to train the weights for the weights for each of the weights", "cit": "[CLS] a host of discriminative methods have been introduced #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] these data-driven parsing approaches obtain state-of-the-art results on the de facto standard wall street journal data set #otherefr; #refr,"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to learn a translation models from a", "cit": "[CLS] #refr explored relabeling of nonterminal symbols to embed more information directly into the backbone of the grammar. [SEP] [PAD]"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best results for the best", "cit": "[CLS] given sufficient labelled data, there are several ?supervised? techniques of training high-performance parsers #otherefr; #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be np-hard #refr. [SEP] s", "cit": "[CLS] nevertheless, there are a number of techniques including mert #otherefr, pro #refr, and rampion #otherefr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of identifying the semantic textual similarity of a text is a text of a text that of", "cit": "[CLS] since the very beginning of computational linguistics, many studies have been devoted to the definition and the implementation of"}
{"pre": "[CLS] the model is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the em algorithm #refr with the", "cit": "[CLS] in previous work, sampling methods have been used to learn tree substitution grammar #otherefr; #refr for tsg learning. [SEP]"}
{"pre": "[CLS] the task of identifying the semantic relations between nominals of noun phrases and their arguments are also key to", "cit": "[CLS] #refr have suggested using google as a source of computing lexical distance between antecedent and definite np for mereological"}
{"pre": "[CLS] the work of #refr is similar to ours in that it is possible to use a more complex linguistic", "cit": "[CLS] we believe that the pdtb annotation can contribute to a range of linguistic discovery and language modeling tasks, such"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the sentences and the syntactic parse trees of the sentences and", "cit": "[CLS] previous work on dialogue analysis has repeatedly noted the discourse functions of particular discourse markers, and our corpus analysis"}
{"pre": "[CLS] the first is the case for the generation of referring expressions from the linguistic phenomena of referring expressions #refr.", "cit": "[CLS] the main tasks of the sentence planner are clause aggregation, sentence boundary determination and paraphrasing decisions based on context"}
{"pre": "[CLS] the system is based on the inprotk toolkit for the best performing system described in #refr. [SEP] system, which", "cit": "[CLS] in the num- gen project #refr, a corpus of numerical expressions was collected and a formal model for planning"}
{"pre": "[CLS] #refr use a similar approach to identify the polarity of words in a sentence. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] there has been a lot of interest in recent years on ?normalization? of social media such as twitter, but"}
{"pre": "[CLS] we use the berkeley parser #refr to obtain the syntactic parse trees for parse trees for parse trees for", "cit": "[CLS] rule size and lexicalization affect parsing complexity whether the grammar is binarized explicitly #refr or implicitly binarized using early-style"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] the best system #refr in conll 2008 achieved an f1- measure of 81.65% on the workshop?s evaluation corpus. [SEP]"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of semantic similarity to be", "cit": "[CLS] #refr) show that the use of framenet can potentially improve the performance of question answering systems. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] in the second category, the context of a word is used to improve the model of #refr. [SEP] accuracy", "cit": "[CLS] one family of methods that resembles incremental em includes collapsed samplers for bayesian models?for example, goldwater et al #otherefr"}
{"pre": "[CLS] we use the moses toolkit #refr to train the weights for the feature weights for the log-linear model with", "cit": "[CLS] we can see that for all language pairs (ab) constantly improves performance of lr- hiero, significantly better than lr-hiero+cp"}
{"pre": "[CLS] the second approach is to use a maximum entropy model #otherefr, and conditional random fields #refr, and support vector", "cit": "[CLS] #refr1 has adapted brill is pos tagger with extended lexical templates to itungartan. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of coreference resolution #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] ? ? ? ?", "cit": "[CLS] iii and marcu, 2005) or structured knowledge bases such as wikipedia #otherefr; #refr and yago #otherefr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the identification of the language model of the meaning of the meaning of", "cit": "[CLS] earlier examples of such approaches include lexc #otherefr, habil #refr, and multiflex discussed below. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a bootstrapping method to identify opinion holders and extract opinion holders and extract opinion expressions from the", "cit": "[CLS] other efforts are focused on targeted sentiment extraction #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr proposed a method for learning a bootstrapping method for bootstrapping a sequence of topic modeling the sequence of", "cit": "[CLS] consequently, accurate corpus annotation has been intensely investigated #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal (wsj) corpus #refr. [SEP] trees and the penn", "cit": "[CLS] the semantics of this transition system is described in #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the feature weights were tuned using minimum error rate training (mert) #refr on the development set using the development", "cit": "[CLS] #refrb) for more details. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of relation extraction has received much attention in recent years #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] traditional re systems extract specific relations for prespecified name-entity types #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a bootstrapping method to identify opinion holders and extract opinion holders and opinion expressions from documents in", "cit": "[CLS] existing works have used topic models to extract only aspects#otherefra; brody and #refr, or jointly model aspects and sentiments"}
{"pre": "[CLS] the task of identifying the semantic textual similarity of a text is a text of a text that of", "cit": "[CLS] however, for the assessment of responses to short-answer questions #otherefra; #refr and in tutorial dialog systems #otherefr deeper semantic"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a text with a text that are semantically", "cit": "[CLS] the model brings together aspects we?ve previously looked into separately: grounded semantics in #otherefr; incremental general nlu in #refr;"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for each sentence in the training corpus.", "cit": "[CLS] the two corpora were converted from ptb constituency trees into dependency trees using the stanford dependency converter (de marneffe"}
{"pre": "[CLS] the sense inventory is a very similar to the one used in the one used in the most popular", "cit": "[CLS] such coarse-grained inventories can be produced manually from scratch #otherefr or clustering #refr existing word senses. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to paraphrase generation is to use a lexicon to automatically extracted from corpora #otherefr; #refr.", "cit": "[CLS] we compare our method against the only existing decipherment baseline, an hmm-based character substitution cipher #otherefr; #refr. [SEP] [PAD]"}
{"pre": "[CLS] in the context of machine translation, syntactic mt has been explored by many researchers #otherefr; #refr, and many researchers", "cit": "[CLS] #refr performs unsupervised hierarchical clustering over a simple set of features. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the system is based on the inprotk toolkit for the best performing system described in #refr. [SEP] [SEP] [SEP]", "cit": "[CLS] for example, edits2 #refr is a distancebased rte system, which can exploit only lexical knowledge resources. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] in the literature, supervised learning methods have been applied to unsupervised learning of natural language processing #otherefr; #refr. [SEP]", "cit": "[CLS] applications have ranged from domain adaptation of part-of-speech #otherefr, unsupervised learning of pos taggers by using bilingual graph-based projections"}
{"pre": "[CLS] the feature weights were tuned using minimum error rate training (mert) #refr on the development set using the development", "cit": "[CLS] #refr applied this method to the language pair english-german with an additional special focus on word formation issues such"}
{"pre": "[CLS] #refr use a similar approach to compute similarity between words by clustering words by clustering words in a sentence.", "cit": "[CLS] recently, #refr proposed a structured vector space model where a word is represented by several vectors reflecting the words"}
{"pre": "[CLS] in the context of sentiment analysis and opinion mining has been used to determine if the polarity of words", "cit": "[CLS] #refr explore feature subsumption for opinion detection, where a given feature may subsume another feature representationally if the strings"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best derivation of the best", "cit": "[CLS] #refr investigated the role of prosodic cues in disfluency detection, although the main focus of their work was accurately"}
{"pre": "[CLS] the system uses a combination of the maximum entropy model of #refr to generate multiple reference translations for the", "cit": "[CLS] clte has as main applications content synchronization and aggregation in different languages #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of identifying the semantic textual similarity of a target word is a document #otherefr; #refr. [SEP] of", "cit": "[CLS] we use the charniak parser #refr for parsing sentences, and svm-light #otherefr for creating the fosr feature space. [SEP]"}
{"pre": "[CLS] the grammar we use is a standard grammar development set for the grammar development and test and test and", "cit": "[CLS] preservation of regularity is an important property for efficient representations and efficient algorithms [see #refr]. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the identification of the language of the language model is a language that", "cit": "[CLS] nouns related by is-a relations or par t -o f relations could also be extracted from corpora #otherefr; #refr."}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with a probability distribution", "cit": "[CLS] for scalability (and noting that some of the models require o(|v | ? |a|) constraints and variables, which, when"}
{"pre": "[CLS] the generation of referring expressions is a subject of research in natural language generation (nlg) in natural language generation", "cit": "[CLS] we differ in that we separate out the realizer, though minimize the need for backtracking in the planner by"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] pressed by at least one sentence in s #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the annotation tool which was the annotation tool for the annotation tool #refr. [SEP] project in", "cit": "[CLS] most of the annotation approaches tackling these issues, however, are aimed at performing classifications at either the document level"}
{"pre": "[CLS] in this paper, we propose a new approach to address the problem of mapping instructions and build a single", "cit": "[CLS] domain adaptation to improve smt performance has attracted considerable attention in recent years #otherefr; #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to this problem is to use a small set of seed sentences #otherefr; #refr. [SEP]", "cit": "[CLS] the modeling of syntactic/semantic based features should take into account linguistic aspects to detect the interesting context, e.g. the"}
{"pre": "[CLS] the most common approach to paraphrase generation is to be very similar to those used in #refr and machine", "cit": "[CLS] this chain performs a contextual analysis that adapts the direct context-vector approach #refr for swts to mwts. [SEP] [PAD]"}
{"pre": "[CLS] the grammar is a grammar that is a grammar that is a given a grammar that is a grammar", "cit": "[CLS] several general-purpose off-the-shelf #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] the ghkm syntax-based extraction method for learning statistical syntax-based translation rules, presented first in #otherefr and expanded on in"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal (wsj) corpus #refr. [SEP] trees and the penn", "cit": "[CLS] pcfg enrichment models #otherefr; #refr split #otherefr these transformations are performed so as to maximize data likelihood (under some"}
{"pre": "[CLS] the grammar we are extracted from the parallel corpus using the conll-x shared task #refr. [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] to get an estimate of how our realiser compares with existing published results, we revisited the test cases discussed"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of possible antecedents for the", "cit": "[CLS] there are also a number of increasingly popular resources used outside senseval and semeval, each with their own formats"}
{"pre": "[CLS] the second approach is to use a small number of manually annotated corpora such as the same data set", "cit": "[CLS] co-training #refr and classifier combination #otherefr are two technologies for training improved dependency parsers. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a text with a text that are semantically", "cit": "[CLS] and #refr use a language model for selecting a fluent sentence among the vast number of surface realizations corresponding"}
{"pre": "[CLS] the grammar is a grammar that is a grammar that is a parsing model that is a dependency grammar", "cit": "[CLS] however, so far only n-gram models and classifiers have been used for this task #otherefr; #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] in the future, we plan to investigate the use of a more sophisticated method to incorporate incomplete constituents, and", "cit": "[CLS] we kept only sentence-like runs of words (satisfying punctuation and capitalization constraints), pos-tagged with tnt #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] on the contrary, a string-to-tree decoder (e.g., #refr) is a parser that applies string-to-tree rules to obtain a target"}
{"pre": "[CLS] we use the averaged perceptron algorithm #refr to train the parameters of the averaged perceptron algorithm #otherefr. [SEP] [SEP]", "cit": "[CLS] for supervised parsing, spectral learning has been used to learn latent variable pcfgs #otherefr and hidden-state dependency grammars #refr."}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to learn a phrase pairs from a", "cit": "[CLS] syntax-based statistical machine translation (ssmt) has achieved significant progress during recent years, with two threads developing simultaneously: the synchronous"}
{"pre": "[CLS] #refr proposed a method for learning a word clustering based on a probabilistic model that learns vector space representations", "cit": "[CLS] in this paper, we use the heuristic rule approach which is introduced by #refr to extract pairwise preference data"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to learn a translation models from a", "cit": "[CLS] similar interpolation techniques have been developed for translation model interpolation #refr for phrase-based systems but have not been as"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] model with the average number of the average", "cit": "[CLS] there is a growing body of work on statistical learning for different versions of the semantic parsing problem #otherefr;"}
{"pre": "[CLS] the translation quality is measured by bleu #refr. [SEP] score #refr with the bleu score of the translation quality.", "cit": "[CLS] in a factored translation model other factors than surface form can be used, such as lemma or part-of-speech #refr."}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking approaches #otherefr; #refr to learning for dependency", "cit": "[CLS] mcclosky et at. #otherefr successfully applied self-training to domain adaptation for constituency parsing using the reranking parser of #refr."}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] we use bleu #refr and ter #otherefr to evaluate translation qualities. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to content selection, is to use a lexicon to identify and use a document #otherefr;", "cit": "[CLS] the use of caseframes is well grounded in a variety of nlp tasks relevant to summarization such as coreference"}
{"pre": "[CLS] the most popular approach to paraphrase generation has been the source sentences into a target language #otherefr; #refr. [SEP]", "cit": "[CLS] it is today common practice to use phrases as translation units #refr instead of the original word-based approach. [SEP]"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set.", "cit": "[CLS] #refra) investigated and compared merging methods inspired by popovic? et al #otherefr, where compound parts were annotated with symbols,"}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders and predict the polarity of words in a sentence and", "cit": "[CLS] d (4) obtained using the stanford parser (de marneffe and #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] zhang and kordoni #otherefr and #refr, on the other hand, include features from the grammar in a maximum entropy"}
{"pre": "[CLS] the data set contains 14,619 items and is available from the same corpus #refr. [SEP] test set of 10", "cit": "[CLS] it is straight-forward to extend these costsensitive loss functions to the structured perceptron #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to this problem is to use a stochastic parse tree #otherefr; #refr. [SEP] model to", "cit": "[CLS] the parsing system used in this study is the mst parser #refr, a state-of-the-art data-driven graph-based dependency parser. [SEP]"}
{"pre": "[CLS] the evaluation metric is bleu #refr, which is based on the reference evaluation. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] recently, #refr presented an aphttp://pdev.org.uk proach that uses probabilistic edit-distance to measure semantic similarity. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs in the sentence ordering.3 most of the sentence planning", "cit": "[CLS] los are also like discourse referents #otherefr, #refr, and others), file cards #otherefr in at least two ways. [SEP]"}
{"pre": "[CLS] the first is the task of identifying the textual entailment (te) of identifying the same textual entailment #otherefr; #refr.", "cit": "[CLS] there have also been efforts for the extrinsic evaluation of mwes for nlp applications such as information retrieval #otherefr"}
{"pre": "[CLS] the first is the task of identifying the polarity of a text with a text that is a text", "cit": "[CLS] the semantic data provided by key-phrase extraction can be used as metadata for refining nlp applications, such as summarization"}
{"pre": "[CLS] in the context of sentiment analysis #refr used a word based on the word based on the word sense", "cit": "[CLS] the details of the dataset are described in #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the semantic annotation of texts has been addressed by a number of papers on the basis of papers #otherefr;", "cit": "[CLS] semantic interpretation has been an actively investigated issue on the research agenda of the logic-based paradigm of nlp in"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr.", "cit": "[CLS] the same shallow semantic parsing task has also been considered in the work of #refr, but using a mln"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to learn a translation models from a", "cit": "[CLS] al-onaizan et al #otherefr described in #refr as a similarity measure. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal (wsj) corpus #refr. [SEP] trees and the penn", "cit": "[CLS] it turns out, however, that lexicalization is not unproblematic: first, there is evidence that full lexicalization does not carry"}
{"pre": "[CLS] the generation of referring expressions is a subject of research in natural language generation (nlg) in natural language generation", "cit": "[CLS] 4 here i propose what appears to me to be the most direct probabilistic generalization of lexiealized tag; a"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? ? ?", "cit": "[CLS] se?#refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of named entity recognition has been the focus of the focus of the focus of the subject", "cit": "[CLS] the tagging scheme is a variant of the iob scheme originally put forward by #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in this paper, we propose a new approach to use syntactic information to learn dependency trees for each sentence", "cit": "[CLS] first, our approach uses dependency relations rather than alignment, cognate and/or position heuristics previously applied #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] in this paper, we present a state-of-the-art dependency parser #refr to generate the parse tree for the sentence in", "cit": "[CLS] therefore, studies have recently resorted to other resources for the enhancement of parsing models, such as large-scale unlabeled data"}
{"pre": "[CLS] the first is the case for the word alignment of word alignment of word alignment of word alignment of", "cit": "[CLS] however, recent work has shown that high accuracy is only achieved under ideal conditions #refr, and one area that"}
{"pre": "[CLS] the model is trained using the moses decoder #refr. [SEP] score #refr with the decoder that the best alignment", "cit": "[CLS] model parameters were tuned using the mert algorithm #refr optimized for bleu metric. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the automatic evaluation of the automatic metric of the automatic evaluation of the automatic evaluation of", "cit": "[CLS] for our experiments, we used some standard statistical measures such as mutual information, pointwise mutual information, chi-square, permutation entropy"}
{"pre": "[CLS] #refr use a similar approach to identify the polarity of words in a sentence. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] finally, our work is similar in spirit to sentiment analysis #refr, emotion detection from images and speech #otherefrb). [SEP]"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a similar to the one used in the sense of the", "cit": "[CLS] the model was found especially beneficial for languages where the training dataset was small, thus suggesting that this method"}
{"pre": "[CLS] the first is the case for the identification of the language model of the meaning of the meaning of", "cit": "[CLS] many of these systems have been shown to be very effective #otherefr; di #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second approach is to use a small number of approximate inference rules, and unsupervised learning algorithms #otherefr; #refr.", "cit": "[CLS] however, the production of segmented chinese texts is time-consuming and expensive, since hand-labeling individual words and word boundaries is"}
{"pre": "[CLS] the system is based on the standard approach of #refr and the best system for machine translation (mt) systems", "cit": "[CLS] most approaches to error correction for non-native text are based on machine learning classifiers for specific error types #otherefr;"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal (wsj) corpus #refr. [SEP] parser #refr with the", "cit": "[CLS] pcfg-las to test our hypothesis, we use the grammatical formalism of probabilistic context- free grammars with latent annotations #otherefr;"}
{"pre": "[CLS] the task of event extraction has received a lot of attention in recent years #otherefr; #refr. [SEP] [SEP] [SEP]", "cit": "[CLS] the significant portion of recent work in the literature #otherefr; #refr focuses solely upon the entity linking problem. [SEP]"}
{"pre": "[CLS] the first is the automatic metric of text generation of paraphrase generation #otherefr; #refr. [SEP] is the probability mass", "cit": "[CLS] other work focuses on surface realization ? choosing among different lexical and syntactic options supplied by the lexical chooser"}
{"pre": "[CLS] we use the stanford ner tagger #refr to extract the sentences and the sentences of the sentences of the", "cit": "[CLS] such models are commonly referred to as edge-factored since their parameters factor relative to individual edges of the graph"}
{"pre": "[CLS] the first is the case for the semantic role labeling task of recognizing textual entailment (te) recognition #otherefr; #refr.", "cit": "[CLS] proposals for computing acceptability (or preference) include raw frequency counts (#refr and #otherefr). [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to identify the polarity of words in a sentence. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] blog articles: a randomly-sampled subset of the american political blog posts gathered by #refr. . [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] model with the average number of the average", "cit": "[CLS] word embeddings are popular representations for syntax #refr, semantics #otherefr and other areas. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the averaged perceptron algorithm #refr to train the parameters of the averaged perceptron algorithm with the averaged", "cit": "[CLS] across languages, dual decomposition has shown to lead to a certificate of optimality for the vast majority of the"}
{"pre": "[CLS] the similarity between two words was computed using the wordnet similarity measure proposed by #refr. [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] this line of approach was pioneered by researchers such as hindle #otherefr and #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] this approach was used by #refr for word segmentation. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the semantic relations between words in a text is a text and", "cit": "[CLS] and #refr propose a model for vector composition, focusing on the different functions that might be used to combine"}
{"pre": "[CLS] in the context of machine learning techniques have been applied to many natural language processing tasks with promising results,", "cit": "[CLS] c that partitions a given set of instances i using distributional similarity #otherefr, and labels using is-a patterns #refr."}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] for mce learning, we selected the reference compression that maximize the bleu score #refr (= argmaxr?rbleu(r, r\\r)) from the"}
{"pre": "[CLS] the feature weights were tuned using minimum error rate training (mert) #refr on the development set using minimum error", "cit": "[CLS] following the cohesion constraints described in #refr we calculate a cohesion penalty for the translation based on the dependency"}
{"pre": "[CLS] the first is the automatic evaluation of the automatic metric of the automatic evaluation of the automatic evaluation of", "cit": "[CLS] there has been some work on automatically generating percussive lyrics to accompany a given piece of musical input #otherefr;"}
{"pre": "[CLS] the system is based on the standard approach of #refr and the best system #otherefr. [SEP] system #otherefr. [SEP]", "cit": "[CLS] following our previous work (gime?nez and ma`#refr, we use shallow syntactic information to generate more precise alignments. [SEP] [PAD]"}
{"pre": "[CLS] the system uses a combination of the maximum entropy model of #refr. [SEP] model for the best derivation of", "cit": "[CLS] elissa uses a rule-based approach that relies on the existence of a dialectal morphological analyzer #refr, a list of"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with a probability distribution", "cit": "[CLS] for comparison, we include the best published results for each dataset #otherefr; #refr, graph-based parsers #otherefr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] the program takes the output of char_align #refr, a robust alternative to sentence-based alignment programs, and applies word-level constraints"}
{"pre": "[CLS] the second approach is to use a word sense disambiguation algorithm which is based on the most similar to", "cit": "[CLS] one of the major approaches to disambiguate word senses is supervised learning #otherefr, #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the english side of the parallel data was parsed using the berkeley parser #refr, trained on the penn treebank", "cit": "[CLS] the dependency trees needed for training the parsers and evaluating their uas were created from the penn treebank data"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for each sentence in the training corpus.", "cit": "[CLS] because of the cost and difficulty in treebank construction, researchers have also investigated the utilization of unannotated text, including"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with a probability distribution", "cit": "[CLS] the last years have witnessed increasingly accurate models for syntax, semantics, and machine translation #otherefr; #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the polarity of a sentence, a sentence, and a word is then", "cit": "[CLS] furthermore, crfs have been used successfully in information extraction #refr, named entity recognition #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to word sense disambiguation is to use a word sense disambiguation which is to automatically", "cit": "[CLS] therefore, we guess 37 syntactic relations in english sentences based on verb pattern information in the dictionary and the"}
{"pre": "[CLS] we use the stanford corenlp suite to lemmatize and partof-speech tag each word #otherefr; #refr, and parse the input", "cit": "[CLS] #refr treated unfilled semantic roles as special cases of anaphora and coreference resolution (cr). [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] we use the forced alignment (fa) method #refr to perform the phrase alignment training and probability estimation. [SEP] [PAD]"}
{"pre": "[CLS] we use the stanford chinese segmenter #refr to tokenize the english sentences. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] 5cdec metaparameters were set to a non-terminal span limit of 15 and standard cube pruning with a pop limit"}
{"pre": "[CLS] the system is based on the inprotk toolkit for the paradise system described in #refr. [SEP] system, which has", "cit": "[CLS] this spoken dialogue system shares a common architecture with several prior systems: commandtalk #otherefr, psa #refr, witas #otherefr. [SEP]"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for parsing #refr. [SEP] grammars with", "cit": "[CLS] this setting is reminiscent of the problem of optimizing feature weights for reranking of candidate machine translation outputs, and"}
{"pre": "[CLS] #refr proposed a method for learning a word clustering words from a target language model to target language and", "cit": "[CLS] they and their extensions have been successfully applied to many problems, including hierarchical topic extraction #otherefr; #refr. [SEP] [PAD]"}
{"pre": "[CLS] the model is trained with a maximum spanning tree (mst) algorithm #refr. [SEP] model with the entire trees in", "cit": "[CLS] we have adopted the conditional maximum entropy #otherefr and #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the grammar is a grammar that is a parsing model that of #refr and a probability distribution over the", "cit": "[CLS] this is an extension of the algorithm by #refr to which we added statistical ranking. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to word alignment is the ibm models #otherefr; #refr. [SEP] model 1 and the hmm", "cit": "[CLS] mixture models were used to optimize the coefficients to the adaptation domain. #refr proposed a model that can be"}
{"pre": "[CLS] #refr proposed a method for identifying subjective subjective words and phrases that can be used to identify the polarity", "cit": "[CLS] we address the sparsity issue pertaining to tweet data by converting our previously proposed topic modelweighted textual matrix factorization"}
{"pre": "[CLS] the most common sense of the target words to be one sense per discourse relations between two words and", "cit": "[CLS] it is commonly used for pattern recognition, also in nlp #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal (wsj) corpus #refr. [SEP] trees and the penn", "cit": "[CLS] recent studies on the automatic extraction of lexicalized grammars #otherefra) allow the modeling of syntactic disambiguation based on linguistically"}
{"pre": "[CLS] topic models have been successfully applied to a wide range of tasks including word sense disambiguation #otherefr, and word", "cit": "[CLS] other potential uses of these algorithms include better language modeling by building topic-based language models, improving nlp algorithms #otherefr,"}
{"pre": "[CLS] the first is the annotation tool which was the annotation tool for the annotation tool #refr. [SEP] project in", "cit": "[CLS] #refra,b; nakhimovsky 1987b). [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to translate the phrases as the", "cit": "[CLS] however, if the phrase almmlkp almthdp is not in the phrase dictionary, it will not be translated correctly by"}
{"pre": "[CLS] the most popular approach to translation models #otherefr; #refr rely on the source side and the target side #otherefr.", "cit": "[CLS] furthermore, lexical block-oriented reordering models have been developed in tillmann and zhang #otherefr and #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the event and event extraction of event descriptions of event descriptions of", "cit": "[CLS] temporal evaluations 2013 (tempeval-3) is the third iteration of temporal evaluations (after tempeval-1 #refr and tempeval- 2 #otherefr) which"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] all the results in table 10 and 11 marked with ?*? are statistically significant with p < 0.05 using"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] current approaches to the tasks proposed at wmt have mainly focused on three main directions, namely: i) feature engineering,"}
{"pre": "[CLS] the task of identifying the semantic relations between nominals of a document has been shown to be a text", "cit": "[CLS] although traditional research on taxonomy construction focuses on extracting local relations between concept pairs #otherefr; #refr, more recent efforts"}
{"pre": "[CLS] the first is the task of identifying the event extraction of event descriptions of event descriptions of event descriptions", "cit": "[CLS] micro-reading is an important aspect of natural language understanding #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to compute sentence importance. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] although coherence has been studied widely in a field of multi-document summarization #refr, it has not been studied enough"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] model with respect to the entire candidate set.", "cit": "[CLS] existing automatic evaluation measures such as bleu #refr and rouge #otherefr, are not designed for the coherence assessment task,"}
{"pre": "[CLS] the semantic relation between nominals in the verb classes were extracted from the wordnet similarity package #refr. [SEP] [SEP]", "cit": "[CLS] another related line of work is automated ontology construction, which aims to create lexical hierarchies based on semantic classes"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with a beam search", "cit": "[CLS] some recent work on incremental parsing #otherefr; #refr showed another way to handle this problem. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the semantic role labeling task of the predicate-argument structure from the research #otherefr;", "cit": "[CLS] we adopt a neo-davidsonian approach to semantics by a formalism that bears similarity to existing frameworks such as (r)mrs"}
{"pre": "[CLS] in the context of machine translation, itg has been explored for paraphrase generation #otherefr; #refr. [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] such corpora typically are parallel in the sense that they contain different formulations of the same facts #otherefr; #refr."}
{"pre": "[CLS] the system used in this paper is based on the c&c parser #otherefr and the stanford parser #refr. [SEP]", "cit": "[CLS] #refr focused on character n-grams. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the generation of referring expressions is a subject of research in natural language generation (nlg) and has been on", "cit": "[CLS] determinization . . . of weighted string acceptors #otherefr; #refra) . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] pcfg parsing features are generated on the output of the berkeley parser #refr trained over an english, a german"}
{"pre": "[CLS] in this paper, we propose a method for learning semantic representations of phrases that are more accurate than the", "cit": "[CLS] tree kernel methods have gained popularity in the last decade for capturing syntactic information in the structure of parse"}
{"pre": "[CLS] we use the berkeley parser #refr to obtain the syntactic parse trees for parse trees for english and the", "cit": "[CLS] in our experiments, we build a competitive baseline #refr incorporating a 5-gram lm trained on a large part of"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] parsing technologies have improved considerably in the past few years, and high-performance syntactic parsers are no longer limited to"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] we also add more than 10,000 features to hiero #refr and obtain a +1.5 b??? improvement. ?this research was"}
{"pre": "[CLS] the task of identifying paraphrases of semantic textual similarity #otherefr; #refr. [SEP] [SEP] [SEP] is a single document to", "cit": "[CLS] many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding"}
{"pre": "[CLS] the first is the task of converting natural language processing #otherefr; #refr. [SEP] [SEP] to be a given a", "cit": "[CLS] the most notable methods are based on hidden markov models#otherefr, transformation rules#refr, and multi-layer neural networks#otherefr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the first is the process of annotating the word in a language #refr. [SEP] model with the process of", "cit": "[CLS] to generate the inflected forms we make use of wiktionary and supplement it with automatically generated inflection tables based"}
{"pre": "[CLS] the work of #refr is similar to ours in that the generation of referring expressions is in that the", "cit": "[CLS] there have been recent efforts to learn information presentation and recommendation strategies using reinforcement learning #refr, and joint optimisation"}
{"pre": "[CLS] #refr use a nonparametric bayesian model to improve the performance of the performance of the performance of the performance", "cit": "[CLS] we modified existing adaptor grammar segmentation models #otherefr; #refr, to be able to learn topic-specific vocabularies. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] in addition to translation model pruning, a common solution is selecting a subset of the data to be trained"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] shared", "cit": "[CLS] in addition to domain mainstays such as support vector machines and maximum entropy models, we find increased application of"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] non-parametric bayesian methods have been successfully applied to directly learn phrase pairs from a bilingual corpus with little or"}
{"pre": "[CLS] the first is the task of identifying the polarity of a sentence, a document into a document #otherefr; #refr.", "cit": "[CLS] within this line of work, we are most closely related to the reinforcement learning approaches that learn language by"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal (wsj) treebank #otherefr; #refr. [SEP] trees and the", "cit": "[CLS] #refr, it has been somewhat overshadowed in recent years by hpsg #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] snow et al #otherefr and #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the longrange reordering model is to use of the", "cit": "[CLS] research in this direction includes utilizing large web corpora and query log #otherefr; #refr, employing large-scale n-gram models, training"}
{"pre": "[CLS] we use the stanford corenlp suite to lemmatize and partof-speech tag each word #otherefr; #refr, and extract the verb", "cit": "[CLS] computational work on antonyms includes approaches that tested the co-occurrence hypothesis #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of identifying the semantic relations between nominals of noun phrases and their semantic relations has been shown", "cit": "[CLS] such classifications are useful for a wide variety of purposes such as semantic role labelling #otherefr and metaphor identification"}
{"pre": "[CLS] the first is the case for the automatic acquisition of subcategorization frames from corpora #refr, which is the annotated", "cit": "[CLS] recently, interest in the book approach as greatly expanded because a number of mrds have become available, each causing"}
{"pre": "[CLS] the first is the case for the automatic acquisition of subcategorization frames from corpora #refr, which is the penn", "cit": "[CLS] #otherefr; marcus, kim, #refr, is aimed at a complex annotation of (a part of) the czech national corpus (cnc"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr.", "cit": "[CLS] these rules were used to identify syntactically related words for each preposition. used the state-of-the-art named entity tagger of"}
{"pre": "[CLS] the task of segmenting a document into a document into a document into a sequence of mentions and a", "cit": "[CLS] the model has been shown to work in unsupervised tasks such as pos induction #otherefrb), and morphological segmentation #refr,"}
{"pre": "[CLS] the first is the case for the identification of the language model of the general definition of the discourse", "cit": "[CLS] simply put: the spreadsheet designed by jason #refr for teaching hidden markov models is fantastic. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of learning a small number of seed examples is a small number of seed examples from corpus", "cit": "[CLS] we evaluate our approach on the aida dataset #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to identify the relations in which words of words in a window of words", "cit": "[CLS] we do not attempt to implement all the corpus pre-processing and co-occurrence extraction routines that it would require to"}
{"pre": "[CLS] the parser is trained on the penn treebank corpus and tested on the penn treebank corpus #refr. [SEP] test", "cit": "[CLS] 2 percentages are slightly below those presented by #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the event descriptions of the bionlp?09 shared task on event extraction #refr.", "cit": "[CLS] based on the original model, a few extensions have been proposed: for example, filippova and strube #otherefr and #refrb)"}
{"pre": "[CLS] the second approach is to use a small number of seed examples for example outputs #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] a standard methodology is that of two-level morphology #otherefr which is capable of handling the complexity of finnish, though"}
{"pre": "[CLS] the system is based on the standard approach of #refr and the statistical machine translation system #otherefr. [SEP] evaluation", "cit": "[CLS] from this perspective it is possible to directly employ a phrase-based smt system in the task of transliteration #otherefr;"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] msa techniques have not been widely applied in nlp, but have produced some promising results for building a generation"}
{"pre": "[CLS] the translation quality is measured by bleu #refr. [SEP] score #refr with the bleu score of the translation quality.", "cit": "[CLS] an important contribution to interactive cat technology was carried out around the transtype #otherefr; #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a similar to the one used in the sense of the", "cit": "[CLS] these methods are reported to be particularly unreliable for low frequency scfs #otherefr; korhonen, gorrell and #refr, resulting in"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] shared", "cit": "[CLS] the epigenetics and post-translational modifications #otherefr #refra). [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the data set contains 14,619 items and is available from the same data collection used in the conll shared", "cit": "[CLS] feature name example of features start/end word ws, we inside word ws, ws+1, ... , we context word ws?1,"}
{"pre": "[CLS] the system is based on the inprotk toolkit for the best performing system described in #refr. [SEP] score for", "cit": "[CLS] grishman & sterling #otherefr, #refr and others have shown that it is possible to acquire selection preferences from (partially)"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr.", "cit": "[CLS] examples of meaning representations considered in prior research include logical forms based on database query #otherefr; #refr and database"}
{"pre": "[CLS] the first is the case for the grammar formalism used by #refr, which is the most frequent agging task", "cit": "[CLS] as an attempt to ameliorate this, and as an attempt to confirm #refr and bod?s #otherefr thesis that good"}
{"pre": "[CLS] the most popular approach to translation models #otherefr; #refr rely on parallel data #otherefr. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] the use of a maximum entropy approach simplifies the introduction of several additional models explaining the translation process :"}
{"pre": "[CLS] the most popular approach to machine translation is the most widely used in statistical machine translation system combination approach", "cit": "[CLS] yaser al-onaizan (al-onaizan and #refr transliterated an ne in arabic into several candidates in english and ranked the candidates"}
{"pre": "[CLS] in the context of machine translation, itg has been explored for statistical mt #otherefr; #refr. [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] some researchers then tried to automatically extract paraphrase rules #otherefr; #refrb), which facilitates the rule-based pg methods. [SEP] [PAD]"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] shared", "cit": "[CLS] in order to tackle the contradiction problem, global approaches have been proposed by chambers and jurafsky #otherefr and #refr."}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] one approach to addressing this problem is to develop methods that automatically generate annotated data by transferring annotations in"}
{"pre": "[CLS] id institution balagur yandex school of data analysis #otherefr commercial-1,2,3 anonymized commercial systems online-a,b,g anonymized commercial systems online-a,b,g anonymized", "cit": "[CLS] id institution balagur yandex school of data analysis #otherefr umd university of maryland #refr uu uppsala university #otherefr commercial-1,2,3"}
{"pre": "[CLS] the second approach is to use a small number of seed examples for example outputs #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] adaptor grammar collocations drawing on #refr, we also utilise an adaptor grammar to discover arbitrary lengths of n-gram collocations"}
{"pre": "[CLS] we use the averaged perceptron algorithm #refr to train the parameters of the averaged perceptron algorithm #otherefr. [SEP] [SEP]", "cit": "[CLS] it has been widely used in compressive summarization #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second approach is to use a small number of seed sets of words to automatically extracted from the", "cit": "[CLS] 3note the incorrect presupposition in the cue provided by the instructor. machine translation evaluation #otherefr; #refr, and automatic grading"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr.", "cit": "[CLS] the k = 200-best parses at the top cell of the chart are calculated using the efficient algorithm of"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for learning and beam-search decoding #otherefr;", "cit": "[CLS] for example, the unsupervised dependency parsing #otherefr which is totally based on unannotated data, and the semisupervised dependency parsing"}
{"pre": "[CLS] the first is the automatic evaluation of discourse structure of discourse structure of discourse structure of discourse structure of", "cit": "[CLS] the handling of explicit connectives can be split into three tasks (pitler and #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the grammar we use is a standard grammar for the grammar development and test and test #refr. [SEP] grammars", "cit": "[CLS] head binarization always binarizes from the head outward #refrb). [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the input is to be", "cit": "[CLS] given a parse tree, we use a head percolation table #refr to create the corresponding dependency structure. [SEP] [PAD]"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for dependency parsing #refr. [SEP] grammars", "cit": "[CLS] graph-based #otherefr; #refr parsing algorithms offer two different approaches to data-driven dependency parsing. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to extract dependency relations for each word and parse trees from the target", "cit": "[CLS] figure 2: examples of ast m structured features. and automatic charniak parse trees #refr as provided for the conll"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal (wsj) corpus #refr. [SEP] trees and the penn", "cit": "[CLS] in addition to cfg-oriented approaches, a number of richer treebank-based grammar acquisition and parsing methods based on hpsg #otherefr;"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be a given by using", "cit": "[CLS] to translate the input documents into english we use phrase-based statistical machine translation systems based on the log-linear formulation"}
{"pre": "[CLS] the first is the task of identifying the semantic textual similarity of a sentence #refr. [SEP] task is a", "cit": "[CLS] frequently used cues for dialogue acts are lexical features such as the words of the utterance or ngrams of"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] in related research, matsuzaki et al #otherefr introduced latent variables to learn finergrained distinctions of treebank categories for parsing,"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to learn a translation probabilities from", "cit": "[CLS] first research on neural networks to predict word categories has been done in #refr where neural networks were used"}
{"pre": "[CLS] the first is the same as #refr, which was used to create a word in the context of a", "cit": "[CLS] generally, the syntactic structure of a sentence is represented as a tree, and parsing is carried out by maximizing"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the longrange reordering problem as a sequence of words", "cit": "[CLS] co-training has been applied to a number of nlp applications, including pos-tagging #otherefr, word sense disambiguation #refr, and base"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking approaches #otherefr; #refr to learning for dependency", "cit": "[CLS] they include word sense disambiguation, e.g., #otherefr, word segmentation, e.g., #refr, and parsing, e.g., #otherefr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the future, we plan to investigate the use of dependency parsers for constituency parsers from the conll shared", "cit": "[CLS] finally, we employ dual decomposition techniques #otherefr; #refr to find agreement between the full dependency tree and the partial"}
{"pre": "[CLS] the task of identifying the event extraction of event descriptions is a single document #otherefr; #refr. [SEP] [SEP] [SEP]", "cit": "[CLS] for instance, we include the super relation that appears in temporal annotations such as the timebank corpus #otherefr; #refr."}
{"pre": "[CLS] the work of #refr is similar to ours in that it is possible to use discourse relations that the", "cit": "[CLS] kamp 1981), but goes on to assume with grosz and sidner #otherefr, #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second approach is to use a word lattice as a sequence of a sequence of a sequence of", "cit": "[CLS] measuring homogeneity by counting word / lexeme frequencies introduces additional difficulties as it assumes that the word is an"}
{"pre": "[CLS] the task of identifying the semantic textual similarity of a target word is a target word is a given", "cit": "[CLS] for word sense disambiguation, researchers #otherefr; #refra) construct a sense similarity measure from the sentence similarity of the sense"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a text with a text corpus and then", "cit": "[CLS] a recent line of research was dedicated to this task #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a semantic role labelling task", "cit": "[CLS] also, techniques for extracting sf information fiom data can be used along with other research which aims to discover"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] syntax-based translation models engaged with scfg have been actively investigated in the literature #otherefr; #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the automatic evaluation of discourse structure of discourse structure of discourse structure of discourse relations #refr.", "cit": "[CLS] following #otherefr, text files are first parsed by the nptool noun phrase parser, which identifies noun phrases and tags"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best results for the best", "cit": "[CLS] earlier papers have taken a character-level approach to named entity recognition (ner), notably #refr, which used prefix and suffix"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking approaches #otherefr; #refr to learning for dependency", "cit": "[CLS] there is an extensive research literature on building high quality parsers for english #otherefr; #refr, however, models for parsing"}
{"pre": "[CLS] we use the stanford chinese word segmenter #refr to tokenize the english side of the parallel corpus #otherefr. [SEP]", "cit": "[CLS] as some languages such as chinese have no spaces in their writing systems, how to segment sentences into appropriate"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the longrange reordering problem as a sequence of the", "cit": "[CLS] crfs have been widely applied to tasks in natural language processing, especially those involving tagging words with labels such"}
{"pre": "[CLS] the paraphrase collection has been shown to be effective in many nlp tasks such as paraphrase detection #otherefr, paraphrase", "cit": "[CLS] following #refr, a combination system is used to automatically weight paraphrase pair candidates produced by individual systems using a"}
{"pre": "[CLS] the second approach is to use a small number of seed examples for example outputs #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] the stanford corenlp tagger16 #refr yields an accuracy of 90.4%. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the context of machine translation, itg has been explored for mt #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] also relevant is previous work that applied machine learning approaches to mt evaluation, both with human references #otherefr; #refr"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the sentence in", "cit": "[CLS] some of them #otherefr; #refr take an empirical approach, relying on formalisms equivalent to probabilistic synchronous context-free grammars (scfg)"}
{"pre": "[CLS] the wsd system used for wsd in the experiments was performed on the semeval 2007 task #refr. [SEP] [SEP]", "cit": "[CLS] many studies have already shown the validity of a cross-lingual approach to word sense disambiguation #otherefr; #refr. [SEP] [PAD]"}
{"pre": "[CLS] in the context of sentiment analysis and opinion mining has been used to determine if the word is to", "cit": "[CLS] often, methods for opinion, sentiment, and subjectivity analysis rely on lexicons of subjective #otherefr; #refr). [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the semantic textual similarity of a text of a text segments of", "cit": "[CLS] table 3: results on wiki20 and citeulike180 mihalcea and tarau, 2004; medelyan and witten, 2008; #refr, most of which"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] ? ? ? ? ?", "cit": "[CLS] several methods computed word similarity by using wordnet, such as the lesk method in #otherefrand the wup method in"}
{"pre": "[CLS] in the literature, supervised learning methods have been applied to natural language processing tasks such as part-of-speech tagging #otherefr,", "cit": "[CLS] this decomposition applies both to discriminative linear models and to generative models such as hmms and crfs, in which"}
{"pre": "[CLS] the translation quality is measured by bleu #refr. [SEP] score #refr on the test set. [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] we use the giza++ toolkit #otherefr, and combine the forward and backward viterbi alignments to obtain many-tomany word alignments"}
{"pre": "[CLS] in the second international chinese word segmentation bakeoff #otherefr; #refr and the state-of-the-art accuracies of the state-of-the-art accuracies of", "cit": "[CLS] for parser adaptation, self-training #refr, using weakly annotated data from the target domain #otherefr achieve substantial performance gains. [SEP]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] #refr extend the idea further through the use of log-linear models to learn a scoring function for relation pairs."}
{"pre": "[CLS] the first is the task of identifying the polarity of a text that is a text of a text", "cit": "[CLS] #refr determine a noun?s countability preferences from its semantic class, and show that semantics predicts (5-way) countability 78% of"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with a probability distribution", "cit": "[CLS] variations on this approach drive the widely-used, broad coverage c&c parser #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] as embedding features we use (i) the mean of the input space (r) embeddings and (ii) the mean of"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of possible senses in the", "cit": "[CLS] ing a dependency parser2 #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the generation of referring expressions that we will use the grammar and use", "cit": "[CLS] furthermore, td parsing provides explicit, relevant ?left contexts? for probabilistic conditioning #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the literature, supervised methods have been proposed for learning of natural language processing #otherefr; #refr. [SEP] [SEP] [SEP]", "cit": "[CLS] constraint-based monolingual methods by using ilp have been successfully applied to many natural language processing tasks, such as semantic"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] an approach that may seem apt in this respect is an allsubtrees approach #otherefr; #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of coreference resolution #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] ? ? ? ? ?", "cit": "[CLS] as we show, this combination of a pairwise model and strong features produces a 1.5 percentage point increase in"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] of a text in", "cit": "[CLS] our previous work focused on only the segmentation part of the voice identification task #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the work of #refr is similar to ours in that it is possible to use discourse relations that the", "cit": "[CLS] it is perhaps suggestive then of the relative complexity of essays and letters, as compared to news, that the"}
{"pre": "[CLS] the first is the task of identifying the polarity of a word is a word in a word in", "cit": "[CLS] for instance, the frequency of occurrence of a word in a domain-specific corpus can be compared with its frequency"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for parsing #refr. [SEP] grammars with", "cit": "[CLS] approach danish dutch slovene swedishlas uas las uas las uas las uas nivre et al #otherefr 84.2 - -"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set.", "cit": "[CLS] these phrases are then reordered through a lexicalized reordering model that takes into account the orientation of a phrase"}
{"pre": "[CLS] the first is the task of relation extraction #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ?", "cit": "[CLS] kernel-based methods have also been used for relation extraction #otherefr; #refr on various syntactic representations, such as dependency trees"}
{"pre": "[CLS] the second approach is to use a word lattice as a sequence of a sequence of a word segmentation", "cit": "[CLS] we use the following systems for comparison: chasen the word segmentation and pos-tagging system based on extended markov models"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the elements is to use", "cit": "[CLS] since each text is represented by a set of local histrograms/vectors, and standard svm kernels cannot work with such"}
{"pre": "[CLS] the evaluation metric is bleu #refr, which is based on the assumption that human evaluation of machine translation (mt)", "cit": "[CLS] such comparisons are done either at lexical level #otherefr; #refr, wordnet #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders and predict the polarity of words in a sentence and", "cit": "[CLS] previous work has shown the importance of citations in scientific domains and indicated that citations include survey-worthy information #otherefr;"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] the second half of the sentence is parsed by the berkeley parser #refr as: frag np-sbj nn corner advp"}
{"pre": "[CLS] the translation quality is measured by bleu #refr. [SEP] score #refr on the test set. [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] it has been shown that crowdsourcing tools, such as the amazon mechanical turk #refr, can help developers to obtain"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] such word-based lexicalizations of probability models are used successfully in the statistical parsing models of, e.g., collins #otherefr, or"}
{"pre": "[CLS] we use the stanford word segmenter #refr to extract the word pairs from the target language model for each", "cit": "[CLS] a plethora of prior work learns bilingual lexicons from monolingual and comparable corpora with many signals including distributional, temporal,"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to learn a translation probabilities from", "cit": "[CLS] it is also very similar to the joint training procedure for hmm wordalignment models in both directions described by"}
{"pre": "[CLS] the first is the task of identifying the semantic textual similarity of a text of a text of a", "cit": "[CLS] some recent work #refr; agirre et al #otherefr) attempts to change this situation and presents a directed effort to"}
{"pre": "[CLS] the first is the case for the automatic acquisition of subcategorization frames from the web #otherefr; #refr. [SEP] grammars", "cit": "[CLS] but a human reader of a text does make a distinction, based on the discourse relations established by the"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be a phrase-based system #otherefr;", "cit": "[CLS] so unlike some other studies #otherefr; #refr, we used manually annotated alignments instead of automatically generated ones. [SEP] [PAD]"}
{"pre": "[CLS] the grammar is a grammar that is a parsing system for the grammar that is a grammar that is", "cit": "[CLS] unlike stochastic parsing such as #refr#otherefr, our approach can parse sentences which fall out the current grammar and suggest"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a text with a text that are semantically", "cit": "[CLS] automated prediction of #otherefr; #refr, and identification and correction of esl errors #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the system uses a combination of the illinois named entity recognizer #refr to extract the semantic information from the", "cit": "[CLS] more recently also scores provided by the nlu system have been employed, e.g. parsing scores or discourse scores #refr."}
{"pre": "[CLS] we use the stanford ner tagger #refr to train the classifier to generate the feature templates and the feature", "cit": "[CLS] the model of haghighi and #refr incorporated a latent variable for named entity class. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to parsing is to use a linear model trained on a maximum spanning tree (mst)", "cit": "[CLS] the past few years have seen considerable improvement in the performance of unsupervised parsers #otherefr; #refr; bod, 2006a; bod,"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] dubey #otherefr showed that, for german parsing, adding case and morphology information together with smoothed markovization and an adequate"}
{"pre": "[CLS] the first is the case for the syntactic annotation of the penn treebank #otherefr; #refr, which we use the", "cit": "[CLS] the experiments reported in this paper use the english lfg grammar constructed as part of the pargram project #refr."}
{"pre": "[CLS] the translation quality is measured by bleu #refr. [SEP] score #refr on the dev test set. [SEP] [SEP] [SEP]", "cit": "[CLS] the modified powell?s method has been previously used in optimizing the weights of a standard feature-based mt decoder in"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] in contrast, research on da nlp is still in its early stages: #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] therefore, phrase reordering modeling has attracted intensive attention in the past decade #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the process of annotating the word in a sentence boundary disambiguation task #refr. [SEP] sentence boundary", "cit": "[CLS] another possibility would be to use semantic similarity information as computed in schiitze #otherefr, or #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? 2008. [SEP] ?", "cit": "[CLS] #refr augment mirkin et alfeatures with web-based features for the task of entity extraction. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to learn a translation models from a", "cit": "[CLS] vector space regression has shown impressive performance in other tasks involving string-to-string mappings #otherefr, but its application to language"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a text that is a", "cit": "[CLS] #refr employs hundreds of hand-crafted templates as features for decision tree learning. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of meaning that are not", "cit": "[CLS] they proposed that text segments with similar vocabulary are likely to be part of a coherent opic segment. hnplementations"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] of course, since we have no syntactic annotations for this corpus (and it would be infeasible to create them"}
{"pre": "[CLS] the system is based on the moses toolkit #refr that we used for the best output of the english", "cit": "[CLS] most qe system do not need so many features (only 15 for the best system in the wmt12 shared"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] event and semantic relations", "cit": "[CLS] prior approaches have employed contextual features of an instance to identify causality between events or discourse segments #otherefr; #refr."}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] #refr trained a framenet-based semantic role labeller for swedish on annotations transferred cross-lingually from english parallel data. [SEP] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] for japanese, dependency trees are trimmed instead of full parse trees #otherefr; #refr1 this parsing approach is reasonable because"}
{"pre": "[CLS] the parser is trained on the penn treebank corpus and tested on the penn treebank corpus #refr. [SEP] test", "cit": "[CLS] #refr observed a ninefold increase ill time required to parse sentences with a straightforward c, ky parser as opposed"}
{"pre": "[CLS] the translation quality is measured by bleu #refr. [SEP] score #refr on the dev test set. [SEP] [SEP] [SEP]", "cit": "[CLS] language and translation model adaptation to europarl and news-commentary have been explored in #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the semantic textual similarity of a text of a text of a", "cit": "[CLS] upon a close examination of empirical data, it is often difficult to classify a translation pair as a syntactic"}
{"pre": "[CLS] #refr use a supervised sentiment classifier to detect the polarity of words in a target words and their polarity", "cit": "[CLS] thus, we extend the model with a supervised sentiment component that is capable of embracing many social and attitudinal"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] score #refr with the entire candidate set. [SEP]", "cit": "[CLS] however, most template-filling systems #otherefr; #refr assign roles to individual textual mentions using only local context as evidence, leaving"}
{"pre": "[CLS] in the literature, supervised learning methods have been applied to natural language processing tasks such as part-of-speech tagging #otherefr,", "cit": "[CLS] motivated, in part, by the results in mue#refr, we use the task of noun phrase coreference resolution for illustration"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] model with the em algorithm #refr with the best", "cit": "[CLS] previous studies #otherefr; #refr defined a probabilistic model of unification-based grammars including hpsg as a log-linear model or maximum"}
{"pre": "[CLS] the model is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the best results for the best", "cit": "[CLS] for instance, mappings may specify how to convert traces and functional tags in penn treebank to the f-structure in"}
{"pre": "[CLS] the system uses a combination of the maximum entropy model of #refr to generate multiple reference translations for the", "cit": "[CLS] it is similar in many respects to the task of lexical substitution #refr in that it involves determining adequate"}
{"pre": "[CLS] we use the averaged perceptron algorithm #refr to train the parameters of the averaged perceptron algorithm #otherefr. [SEP] [SEP]", "cit": "[CLS] previous systems #otherefr; #refr used these word conll04.corp at http://cogcomp.cs.illinois. edu/page/resource_view/43 boundaries as they were, treated the boundaries as"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] in #refr the comparison of phrase-based , \"chiang?s style\" hirearchical system and samt is pro- 2www.cs.cmu.edu/?zollmann/samt vided. [SEP] [PAD]"}
{"pre": "[CLS] the first is the task of coreference resolution #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] ? the typed-similarity [SEP]", "cit": "[CLS] quite recently, however, rule-based approaches regained popularity due to stanford?s multi-pass sieve approach which exhibits stateof-the-art performance on many"}
{"pre": "[CLS] we use the stanford pos tagger #refr to tokenize the english pos tags for each word and the pos", "cit": "[CLS] chinese information retrieval (ir) systems benefit from a segmentation that breaks compound words into shorter ?words? #refr, paralleling the"}
{"pre": "[CLS] the second approach is to use a maximum entropy model #otherefr, and a log-linear combination of the hidden markov", "cit": "[CLS] we represent each word as a sequence of inflectional groups (igs hereafter), separated by \"dbs denoting derivation boundaries, as"}
{"pre": "[CLS] the most common approach to this problem is to use a stochastic tree kernel #otherefr; #refr. [SEP] model is", "cit": "[CLS] one such augmented parser, trained on data available from the propbank project has been recently presented in #refr. [SEP]"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a text that are semantically", "cit": "[CLS] multiword expressions are sequences of words that tend to co-occur more frequently than chance and are characterised by various"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a similar to the similarity of two sentences that is based", "cit": "[CLS] it contains nouns, verbs and adjectives that are connected by classical and nonclassical relations #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the literature, supervised learning methods have been applied to part-of-speech tagging and unsupervised #otherefr and unsupervised word segmentation", "cit": "[CLS] another direction that has been explored in the past includes bootstrapping taggers for a new language based on information"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a text with a text that are semantically", "cit": "[CLS] the number of types increases with the corpus size, and #refr propose heuristics for thesaurus building without undertaking the"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] hasegawa, et al put forward an unsupervised approach for relation extraction from large text corpora #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the grammar is a grammar that is a parsing model that of #refr and is a probability distribution over", "cit": "[CLS] more recently, transformations have been applied to a diverse set of problems, including part of speech tagging, pronunciation etwork"}
{"pre": "[CLS] the first is the case for the generation of referring expressions from the dialogue manager sends dialogue acts (ddas)", "cit": "[CLS] as to the parsing of word-graphs, it is wellknown that parsing algorithms for word strings can easily be generalized"}
{"pre": "[CLS] the first is the case for the case of the grammar development of the language model is a translation", "cit": "[CLS] although rules appear quite different from instances as used in memory-based or instancebased learning #otherefr; #refrb) there is a"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] extensive research has been done in questionanswering, e.g. #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of coreference resolution #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? the typed-similarity", "cit": "[CLS] for example, #refr present a first-order logic model that determines the probability that an arbitrary set of nps are"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a word sense disambiguation of", "cit": "[CLS] we applied pagerank and personalized pagerank on the wikipedia graph using freely available software #refr . [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of meaning that are not", "cit": "[CLS] table 1: tag-like representation f ms lexical features fipstag 3 (a government and binding chartparser #refr). [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] this decision can be delayed to decoding time by presenting several reordering options to the decoder as a lattice"}
{"pre": "[CLS] the grammar is a grammar that is a unification grammar that is a unification grammar that is a unification", "cit": "[CLS] the separation of strnctu~al information from cooccttrence r strictions is advocated in kilbury#otherefr; both #refr and phi\\]hps#otherefr of a"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] correctness(ref[],sys?) (1) the correctness measure that systems are typically tuned toward is bleu #refr, which measures the fraction of"}
{"pre": "[CLS] the system is based on the inprotk toolkit for the best performing system described in #refr. [SEP] score for", "cit": "[CLS] id participant cmu carnegie mellon university #otherefr uk charles university - #refr upc technical university of catalonia #otherefr commercial-[1,2]"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a text with a text corpus and then", "cit": "[CLS] although there has been a lot of interesting work done in natural language generation #refr, we use a simple"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] testing accuracy ? 10 ? 20 all dmv model ur-annealing 63.6 53.1 47.9 ur-annealing&prior 66.6 57.7 52.3 pr-s #otherefr"}
{"pre": "[CLS] the system is based on the moses toolkit #otherefr and the moses toolkit #refr. [SEP] test set for the", "cit": "[CLS] ter and bleu #refr scores are calculated over all the sentences in the training set. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of coreference resolution #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] ? the typed-similarity [SEP] ?", "cit": "[CLS] previous work on anaphoricity determination can be broadly divided into two categories (see #refr for an overview). [SEP] [PAD]"}
{"pre": "[CLS] the first is the case for the discourse structure of discourse structure of discourse structure of discourse structure of", "cit": "[CLS] #refr, 1985a, 1985b) has extended that into a plan-tracking model for use in interpreting pragmatic ill-formedness and intersentential el"}
{"pre": "[CLS] the system used in this paper is based on the c&c parser #otherefr shared task #refr. [SEP] [SEP] [SEP]", "cit": "[CLS] #refr implemented a double reranking model on top of the base srl models to select the most probable solution"}
{"pre": "[CLS] id participant cmu carnegie mellon university #otherefr cu-bojar charles university - bojar #refr cu-depfix charles university - depfix #otherefr", "cit": "[CLS] id participant cmu carnegie mellon university #otherefr kit karlsruhe institute of technology #refr limsi limsi #otherefr commercial-[1,2] two commercial"}
{"pre": "[CLS] the first is the process of annotating the word segmentation and then the word segmentation model of a word", "cit": "[CLS] because of its value to natural language processing, various text segmentation tasks have been automated such as topical segmentation?"}
{"pre": "[CLS] the most popular approach to machine translation is the most widely used in statistical machine translation system combination approach", "cit": "[CLS] we apply directl-p #refr for our training and testing task. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] feature weights themselves are learned via minimum error rate training as implemented in z-mert #refr with the bleu metric"}
{"pre": "[CLS] #refr proposed a method for learning a word sense disambiguation which is a word sense disambiguation method to extract", "cit": "[CLS] our work is based on #refr?s bayesian version of the dependency model with valence #otherefr, using interpolated backoff techniques"}
{"pre": "[CLS] in the domain of domain adaptation has been shown to be effective for various nlp tasks such as pos", "cit": "[CLS] wikipedia wikipedia has recently been used as a knowledge source for various language processing tasks, including taxonomy construction #otherefr,"}
{"pre": "[CLS] the work of #refr is similar to ours in that it is possible to use a discourse representation of", "cit": "[CLS] these questions are also relevant for automatic discourse relation identification: many approaches to discourse relation identification have simplified the"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr.", "cit": "[CLS] closest to our clustering approach from section 2 is the error-driven first-order probabilistic model of #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to paraphrase generation is the source sentences into a target language #otherefr; #refr. [SEP] input", "cit": "[CLS] pg shows its importance in many areas, such as question expansion in question answering (qa) #refr, text polishing in"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] to optimize the loglinear parameters, the downhill-simplex algorithm #otherefr is applied with bleu #refr as optimization criterion. [SEP] [PAD]"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the penn treebank #otherefr. [SEP] grammars", "cit": "[CLS] much of the current research into probabilistic parsing is founded on probabilistic contextfree grammars #otherefr; #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to word alignment is to use a log-linear combination of models #otherefr; #refr. [SEP] model", "cit": "[CLS] we use the stanford named entity recognizer #refr to identify named entities in s and t . [SEP] [PAD]"}
{"pre": "[CLS] the feature weights were tuned using minimum error rate training (mert) #refr on the development set using minimum error", "cit": "[CLS] #refr propose an a* search algorithm for ibm model 4, and test on sentence lengths up to 14 words."}
{"pre": "[CLS] #refr proposed a method for identifying word pairs of words in a sentence to cluster words into a sentence", "cit": "[CLS] moreover, the algorithm has proven to be up to the state of the art in several studies, with no"}
{"pre": "[CLS] the first is the one of the most popular approach to use crowdsourcing, which has been to use a", "cit": "[CLS] as examples for work that deals with the automated decipherment of cipher texts, we point to #otherefr, and #refr."}
{"pre": "[CLS] the first is the task of identifying the semantic textual similarity of a text of a text of a", "cit": "[CLS] the cosine measures the similarity of the two vectors by calculating the cosine of the angle between vectors. ?-skew"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be a difficult problem #otherefr;", "cit": "[CLS] substitution and reordering smt based approaches to paraphrasing (barzilay and #refr and to sentence simplification #otherefr have shown that"}
{"pre": "[CLS] the first is the one of the most popular approach to use a word in which is to use", "cit": "[CLS] however, wc invented novel way of using the decision tree as trmlsibrmation-t)ased rule induction #refr. l?igure, 2 shows the"}
{"pre": "[CLS] the first is the task of identifying the sentence planning problem that the user is a text generation of", "cit": "[CLS] this is a feasible problem when using a large vocabulary continuous speech recognition #otherefr; #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] mert directly optimizes the evaluation metric under which systems are being evaluated, yielding superior performance #otherefr when compared to"}
{"pre": "[CLS] the parser is trained on the penn treebank corpus and tested on the penn treebank corpus #refr. [SEP] test", "cit": "[CLS] data-driven dependency parsing has recently received extensive attention in the parsing community and impressive results have been obtained for"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be np-hard #refr. [SEP] s", "cit": "[CLS] this builds upon our previous work on estimating parameters of a ?bag-of-phrases? model for machine translation #refr. [SEP] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a text with a text that are semantically", "cit": "[CLS] summarizing spoken documents has been extensively studied over the past several years #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr proposed a method for identifying paraphrases based on a bilingual parallel corpus of news stories from a target", "cit": "[CLS] this move from word to sentence has yielded models applied to tasks such as paraphrase detection #otherefr, sentiment analysis"}
{"pre": "[CLS] the first is the nlg community #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] and (2) the problem of", "cit": "[CLS] spoken dialogue systems are emerging as an intuitive interface for providing conversational access to online information sources #otherefr; glass"}
{"pre": "[CLS] the most popular approach to machine translation is the one proposed by #refr. [SEP] model to use a log-linear", "cit": "[CLS] the training data comes from the europarl corpus as distributed for the shared task in the naacl 2006 workshop"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] following #refr, sentiment includes positive and negative evaluations, emotions, and judgments, while arguing includes arguing for or against something,"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] this idea was first explored for weakly supervised learning #otherefr and recently by #refr for multisource cross-lingual transfer. [SEP]"}
{"pre": "[CLS] the translation model is a log-linear model #refr. [SEP] model weights ? = ? ? ? ? ? ?", "cit": "[CLS] for example, models of oov words might compete with models of in-vocabulary recognition errors using #refr error model for"}
{"pre": "[CLS] in the last decade, there has been a lot of work on joint inference in english #otherefr; #refr. [SEP]", "cit": "[CLS] several bayesian inference approaches have also been proposed #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the sentences and the syntactic parse trees of the sentences and", "cit": "[CLS] most existing works on sentiment summarization focus on predicting the overall rating on an entity #otherefr; #refr). [SEP] [PAD]"}
{"pre": "[CLS] the task of identifying the overall sentiment in text has been the focus of the goal of text #otherefr;", "cit": "[CLS] while many models of text coherence have been developed in recent years #otherefr, #refr, elsner et al #otherefr), the"}
{"pre": "[CLS] the most common approach to phrase-based translation #otherefr; #refr is to phrase-based translation systems #otherefr. [SEP] systems #otherefr are", "cit": "[CLS] similarly, #refr use itg for pruning. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] while much research #otherefr; #refr has explored how to reconcile pairwise decisions to form coherent clusters, we simply take"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] the term makes explicit the algorithm?s connection to label propagation, a general framework2 for semi-supervised learning #otherefr with applications"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the words is to be represented by a word", "cit": "[CLS] mturk has been used extensively for annotating and evaluating nlp tasks and has been shown to provide data that"}
{"pre": "[CLS] the work presented in this paper connects and extends several areas of research: grounded semantics #otherefr, which aims to", "cit": "[CLS] these properties are the area of the anchor and landmark, the distance between them #refr, among others) and their"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] syntactic cohesion1 is the notion that all movement occurring during translation can be explained by permuting children in a"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr. [SEP]", "cit": "[CLS] #refr observes that in developing statistical parsing models, ?shotgun? features ? that is, myriad scattershot features that pay attention"}
{"pre": "[CLS] the system is based on the inprotk project #refr. [SEP] score for the number of the number of the", "cit": "[CLS] for example, the word about tagged rb (default category of rb: out) followed l?q'he same techifique is used in"}
{"pre": "[CLS] in this paper, we propose a method for learning translation from parallel corpora to support vector machines #otherefr; #refr.", "cit": "[CLS] this process is similar to the work in #refr, where confusion sets of contextually similar words are built initially"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with a probability distribution", "cit": "[CLS] for chinese, such a parser achieves the state-of-the-art performance and defeats many other types of parsers, including collins as"}
{"pre": "[CLS] #refr use a similar approach to identify the topics of words in a sentence and their topic boundaries in", "cit": "[CLS] confusion network decoding was applied to combine the outputs of multiple machine translation systems #otherefr; #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the system is based on the inprotk toolkit for the best performing system described in #refr, which uses a", "cit": "[CLS] each of these variants was then submitted to a parser trained on written text #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] sentence pair of words", "cit": "[CLS] by detecting sub-units of information within the message, our work may complement the works of #otherefr; #refr who propose"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr. [SEP]", "cit": "[CLS] the last few years have seen the great success of stochastic part-of-speech #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to paraphrase generation has been the source sentences into either by using a source or", "cit": "[CLS] context is often defined as local linguistic features such as surrounding words and their part-of-speech, but some works have"}
{"pre": "[CLS] the most common approach to this problem is to use a dictionary to model the decision to generate a", "cit": "[CLS] #refr developed a system for classifying hebrew learners into five proficiency levels, using features that focus on the nature"}
{"pre": "[CLS] the most common approach to word alignment is to use a log-linear combination of models #otherefr; #refr. [SEP] model", "cit": "[CLS] recently there has been lot of work addressing the problem of annotating text with links to wikipedia entities #otherefr;"}
{"pre": "[CLS] the most popular approach to translation lexicons from the source language into target language side #otherefr; #refr. [SEP] grammars", "cit": "[CLS] this problem is similar to the task of automatic translation output evaluation and so we use meteor #refr, an"}
{"pre": "[CLS] the translation quality is measured by bleu #refr. [SEP] score #refr with the bleu score of the translation quality.", "cit": "[CLS] the rali submission for wmt10 proposed a similar approach that builds queries from the monolingual news corpus in order"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within the source sentence into a target sentence into", "cit": "[CLS] the natural language applications we consider in this paper are: #otherefr; #refr, #otherefra). [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to parsing is to use a linear model trained on a maximum spanning tree (mst)", "cit": "[CLS] #refr incorporated rich local features into a tree crf model and built a competitive parser. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the entity mention extraction #otherefr; #refr) is an important step in which the most popular in the most popular", "cit": "[CLS] traditional learning-based coreference resolvers operate by training a model for classifying whether two mentions are co-referring or not #otherefr,"}
{"pre": "[CLS] the most common approach to parsing is to use a linear model trained on a maximum spanning tree (mst)", "cit": "[CLS] non-heuristic filtering techniques, on the other hand, employ reliability measures (often unrelated to the task) to predict high-precision data"}
{"pre": "[CLS] in the case of dependency parsing, previous work has shown that it is possible to improve the performance of", "cit": "[CLS] methods that use no supervision at all #otherefr or small amounts of manual supervision (haghighi and #refr have been"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best results for the best", "cit": "[CLS] in this section, we test our joint model on pku and msra datesets provided by the second segmentation bake-off"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? ? ? ?", "cit": "[CLS] 3we used the berkeley parser #refr to perform pos tagging. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in recent years, there has been a growing interest in the task of generating referring expressions, reflected in the", "cit": "[CLS] recent work #refr showed that directed graphical models (dgms) outperform other machine learning techniques such as support vector machines"}
{"pre": "[CLS] the first is the task of identifying the mentions of a text with a text at the document level", "cit": "[CLS] this ranges from simple compatibility checking for ?gender?, textual soft matching for ?names?, to sophisticated semantic matching for ?mentions?"}
{"pre": "[CLS] the first is the automatic evaluation of the automatic metric of the automatic evaluation of the automatic evaluation of", "cit": "[CLS] for example, a deployment with real callers #refr found that belief tracking sometimes degraded performance due to model mis-matches"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] #refr describe a dependency parser for japanese and yamada and matsumoto #otherefr an extension for english. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] to tune the weights for bleu #refr, we used the n-best batch mira #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the annotation tool which was the annotation tool for the annotation tool #refr. [SEP] project in", "cit": "[CLS] then i present an adaptation to danish of eckert and strube is algorithm for resolving anaphora referring to individual"}
{"pre": "[CLS] the first is the case for the generation of referring expressions that we will use the grammar and use", "cit": "[CLS] #refr uses multi-~tape two-level morphology to analyze some arabic data, butdespite the suggestive titlemust simulate prosodic operations uch as"}
{"pre": "[CLS] the first is the task of identifying the sentence planning problem that is central to a text planning problem", "cit": "[CLS] the upper mode l ' s cont r ibut ion to the so lu t ion to the in"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] we used the stanford probabilistic natural language parser #refr for constructing these parse trees, the stanford tregex utility #otherefr"}
{"pre": "[CLS] #refr use a bootstrapping method to identify opinion holders and extract opinion expressions from documents in a target words", "cit": "[CLS] #refr explored n- gram features from 1 to 3-grams to build a classifier to recognize sarcasm in dutch tweets."}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] some of the previous work on semantic parsing has focused on fairly simple domains, primarily, atis #otherefr whose semantic"}
{"pre": "[CLS] the most common approach to paraphrase generation is to use a lexicon to paraphrase generation #otherefr; #refr. [SEP] words", "cit": "[CLS] in addition, these existing approaches can only generate one or just a few new bilingual dictionaries from at least"}
{"pre": "[CLS] #refr proposed a method for learning a word segmentation and a word segmentation model that uses a word segmentation", "cit": "[CLS] unsupervised morphological induction: unsupervised morphology remains an active area of research #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to translate the phrases as the", "cit": "[CLS] we trained our systems with the following settings: a maximum sentence length of 80, growdiag-final-and symmetrization of giza++ alignments,"}
{"pre": "[CLS] we use the stanford ner tagger #refr to train the classifier to extract dependency trees for each word and", "cit": "[CLS] more recently, #refr described how sentence-level latent variables can be used to improve document-level prediction and nakagawa et al"}
{"pre": "[CLS] the model is trained with the averaged perceptron algorithm #refr. [SEP] score = ? ? ? ? ? ?", "cit": "[CLS] for more applications in parsing, see #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the context of machine translation, itg has been explored for a variety of nlp tasks, including pos tagging", "cit": "[CLS] we implemented the forest-based statistical sentence generation method of #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for parsing #refr. [SEP] grammars with", "cit": "[CLS] the performance of joint-p2+mst-2 is comparable to the system of huang and sagae #otherefr, who report a parsing speed"}
{"pre": "[CLS] the dialogue manager (dm) is the one of the most common approach to nlg systems that of the dialogue", "cit": "[CLS] movie information consultant (mimic) the dialogue system whose baseline speech generation capabilities we enhance is the mixed- initiative movie"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the longrange reordering problem as a sequence of words", "cit": "[CLS] surprisal, or the unexpectedness of a word or syntactic category in a given context, is often used as a"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] event and event and", "cit": "[CLS] the noisy contents also cause great difficulties to the traditional nlp tools such as ner and dependency parser #refr,"}
{"pre": "[CLS] #refr proposed a method for automatically learning entailment rules from the web by using a large corpus of annotated", "cit": "[CLS] tfp #refr?s model is also sensitive to selectional preferences, but to two degrees. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the one of the most popular approach to use a word in which is to use", "cit": "[CLS] our goal departs from the large body of work devoted to lightly-supervised word sense disambiguation #otherefr; #refr, which seeks"}
{"pre": "[CLS] the second approach is to use a small number of manually annotated corpora such as the same data set", "cit": "[CLS] self-training has been applied to several natural language processing #otherefr; #refr, information extraction#otherefrand so on. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in this paper, we present a state-of-the-art dependency parsing algorithm for dependency parsing #refr. [SEP] parsing and klein and", "cit": "[CLS] different from the bilingual parsing #otherefr; #refr that induces grammar from parallel text, the syntax projection aims to project"}
{"pre": "[CLS] the task of event extraction has received a lot of attention in recent years #otherefr; #refr. [SEP] [SEP] [SEP]", "cit": "[CLS] however, riedel et al?s model (like that of previous systems #refr) assumes that relations do not overlap ? there"}
{"pre": "[CLS] the most popular approach to word alignment is the hmm alignment model #refr. [SEP] model is based on the", "cit": "[CLS] #refr address the issue of lexical sparsity by presenting different preprocessing schemes for arabic-to-english smt. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the process of annotating the correct grammatical relations between the words in a sentence and the", "cit": "[CLS] in their approach, complexity is mitigated by imposing a maximum repair length #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford chinese word segmenter #refr to tokenize and cilin for chinese sentences. [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] lately, there has been increasing interest in learning translation lexicons from non parallel data with decipherment techniques #otherefr; #refr."}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be np-hard #refr. [SEP] s", "cit": "[CLS] all of these approaches are unsupervised, in that they do not require labeled alignment data; however several authors have"}
{"pre": "[CLS] the first is the same as #refr, which was used to create a word in the context of a", "cit": "[CLS] later taggers have managed to improve brill?s figures a little bit, to just above 97% on the wall street"}
{"pre": "[CLS] the translation quality is measured by bleu #refr. [SEP] score #refr with the bleu score of the translation quality.", "cit": "[CLS] mert #refr and mira #otherefr are popular solution to compute an optimal weight vector by minimizing the error on"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] given the lattice and gs,?s , lattice parsing is a straightforward generalization of the standard arc-factored dynamic programming algorithm"}
{"pre": "[CLS] the second approach is to use a word lattice as a sequence of a sequence of a sequence of", "cit": "[CLS] the largest step towards an automatically trainable spelling system was the statistical model for spelling errors #refr. [SEP] [PAD]"}
{"pre": "[CLS] the task of identifying the semantic textual similarity of a document is a document level #refr. [SEP] of a", "cit": "[CLS] words, chunks and their relations in the texts were analyzed by cabocha #otherefr, and named entities were analyzed by"}
{"pre": "[CLS] #refr proposed a method for learning a word segmentation and a word segmentation model that uses a word segmentation", "cit": "[CLS] especially, methods that perform morphological segmentation have been studied extensively #otherefrb; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] we identified these mentions of persons using stanford ner #refr and treated each person mention as a single token."}
{"pre": "[CLS] in the context of machine translation, itg has been explored for a variety of tasks in recent years #otherefr;", "cit": "[CLS] the input and all words of the summary topic signatures are words highly descriptive of the input, as determined"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] when training is finished, the weight vectors from all iterations are averaged together. #otherefr; #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the context of sentiment analysis and opinion mining has been used to extract opinion expressions #otherefr; #refr. [SEP]", "cit": "[CLS] through observations and studies of the predecessors #otherefr; #refr, review spams are divided into the following two classes: ."}
{"pre": "[CLS] the first is the automatic evaluation of discourse structure of discourse structure of discourse structure of discourse structure of", "cit": "[CLS] so far, the complex concept builder implements tokenization #otherefr, part-of-speech tagging #refr, named entity detection #otherefr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to this problem is to use a word representation of a word representation of a", "cit": "[CLS] #refr and s?aghdha and korhonen #otherefr introduced a probabilistic model to represent word meanings by a latent variable model."}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best results for the best", "cit": "[CLS] note that by construction of pairs the model is symmetric, this provides a property that will be exploited at"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] on the other hand, incremental parsers #otherefr; #refr process input in a straightforward left-to-right manner. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a task of identifying the degree of semantic relations between words", "cit": "[CLS] other researchers have identified other sets of semantic relations #otherefr, (baker, fillmore, and #refr, #otherefr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to identify the polarity of words in a sentence and their polarity in a", "cit": "[CLS] although there is much recent work focusing on the task of multi-tweet summarization #otherefr; #refr, most previous work relies"}
{"pre": "[CLS] the grammar we are extracted from the parallel corpus using the berkeley parser #refr and the berkeley parser #otherefr.", "cit": "[CLS] see #refr for discussion of the relation between constituent and dependency structures and see #otherefr for a comparison of"}
{"pre": "[CLS] the first is the case for the automatic evaluation of the automatic evaluation of english lexical substitution task #refr.", "cit": "[CLS] miller et al #refr assume that the input data is the output of a speech or optical character recognition"}
{"pre": "[CLS] in the context of machine translation, information extraction is used to extract relevant information from text from text corpora", "cit": "[CLS] in focusing on characters, we follow #refr, who analyze narratives by examining their social network relationships. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to paraphrase acquisition is to use a lexicon to automatically extracted from corpora #otherefr; #refr.", "cit": "[CLS] in automatic text summarization, synonymous words are employed to identify repetitive information in order to avoid redundant contents in"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a text with a text that are semantically", "cit": "[CLS] #refr), but for now we measure using the better-known kappa, in the following way: for each equivalence between factoids"}
{"pre": "[CLS] the most popular approach to translation lexicons from the source language side #otherefr; #refr is to target language side", "cit": "[CLS] in earlier work #refr, we investigate whether the ?one sense per discourse? heuristic commonly used in word sense disambiguation"}
{"pre": "[CLS] the first is the task of sentence compression which has been addressed by many researchers #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] finally, more recent papers propose approaches which perform both surface realisation and content selection. #refr proposes a log linear"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? 2008.", "cit": "[CLS] because of the difficulty of collecting annotated data, several procedures have been described that can be trained on unannotated"}
{"pre": "[CLS] the task of identifying the semantic textual similarity of a text is a document #otherefr; #refr. [SEP] of a", "cit": "[CLS] numerous previous studies have considered distant or weak supervision from a single relational database as an alternative to manual"}
{"pre": "[CLS] the task of identifying the overall sentiment in text has received a number of attention #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] this includes finding question answer pairs #otherefr, ranking answers #refr etc. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] a great deal of previous work has focused on the rule induction problem #otherefr; #refr, whereas relatively little emphasis"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be a synchronous context-free grammar", "cit": "[CLS] as a pcfg parser, the berkeley parser #refr was preferred, due 1data acquired from http://www.statmt.org/wmt11 to the possibility of"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the training data is to", "cit": "[CLS] in this paper, we use the charniak language model #refr for determiner selection. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the generation of referring expressions from the language of the research project #refr,", "cit": "[CLS] we follow #refr in using pustejovsky is #otherefr concept of telic role to encode the purpose of an artifact."}
{"pre": "[CLS] the first is the one of the most popular approaches to automatically identify the words in a text that", "cit": "[CLS] there are various studies both in computational linguistics and cognitive science that build resources associating words with several cognitive"}
{"pre": "[CLS] #refr proposed a method for learning a semantic inference based on a set of sentences paired with their meaning", "cit": "[CLS] in terms of domain learning in the form of template induction, heuristic methods involving multiple clustering steps have been"}
{"pre": "[CLS] in the last decade, there has been a lot of work on unsupervised learning of nlp #otherefr; #refr. [SEP]", "cit": "[CLS] trading off the global optimality of inference for additional flexibility in the design of features and long range dependencies"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] a variety of unsupervised models refined this initial work with priors #otherefr; #refr and inference constraints #otherefr. [SEP] [PAD]"}
{"pre": "[CLS] the first is the automatic metric of text with human annotation of text segments as shown in #refr. [SEP]", "cit": "[CLS] in a vast body of related work, automated methods have been explored for the generation of descriptions of images"}
{"pre": "[CLS] the translation model is a phrase-based decoder that a hierarchical phrase-based translation model #otherefr and a hierarchical phrase-based translation", "cit": "[CLS] table 1: bleu and nist scores for pb-smt and factored translation experiments for the en-ptbr language pair tion, reordering"}
{"pre": "[CLS] the system is based on the moses toolkit #otherefr and the standard phrase-based smt system described in #refr. [SEP]", "cit": "[CLS] as mert is not suitable when a large number of features are used (our system uses 19 fetures), we"}
{"pre": "[CLS] #refr use a similar approach to compute similarities between source and target languages that are not able to produce", "cit": "[CLS] 10see e. g. #refr. cates and arguments and may eventually speed up the annotation process in a semi-automatic way."}
{"pre": "[CLS] the second approach is to use mismatches and is the fact that some other approaches that some kind of", "cit": "[CLS] we have been developing a method for creating morphological taggers and analyzers of fusional languages1 without the need for"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] other works on online minimum error rate training in smt #otherefr that deserve mentioning are #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to mt system combination is the one proposed by #refr, which uses a log-linear combination", "cit": "[CLS] however, although it is standard practice in mt evaluation to measure increases in automatic metric scores with significance tests"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be np-hard #refr. [SEP] s", "cit": "[CLS] it is recognized that translating from poor to rich morphology is a challenge #refr that calls for deeper linguistic"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] event", "cit": "[CLS] we apply this system to the extraction of epi events and rel relations from all pubmed abstracts and all"}
{"pre": "[CLS] the task of identifying the semantic relations between nominals of noun phrases and their arguments are also key to", "cit": "[CLS] faceted search (stoica and #refr supports a better understanding of a target domain, by allowing exploration of data according"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the penn treebank #otherefr. [SEP] score", "cit": "[CLS] we propose and implement a modification of the #refr normal form to account for generalized composition of bounded degree,"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set.", "cit": "[CLS] for example, #refr introduce synchronous tree-adjoining grammar #otherefr uses a synchronous tree-substitution grammar (stsg), which is a restricted version"}
{"pre": "[CLS] #refr proposed a joint model for unsupervised learning of pos tagging and semantic parsing and semantic role labeling using", "cit": "[CLS] our chunker follows the system described by #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] in particular, alignment (e.g., #refr and graph matching approaches #otherefr are broadly similar to our approach. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford chinese word segmenter #refr to tokenize and cilin for each word segmenter #otherefr for each", "cit": "[CLS] to make normalization efficient, contrastive estimation #otherefr; #refr introduce neighborhood for unsupervised log-linear model, and has presented positive results"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for parsing #refr. [SEP] grammars with", "cit": "[CLS] can we combine the advantages of both approaches, that is, construct an incremental parser 1#refrb) is a notable exception:"}
{"pre": "[CLS] we use the berkeley parser #refr to obtain the syntactic parse trees for each sentence and the english and", "cit": "[CLS] after extracting phrasal pairs using the standard approach of #refr, all pairs whose target phrases are not exhaustively dominated"}
{"pre": "[CLS] the translation model is a log-linear model #refr. [SEP] model weights ? = ? ? ? ? ? ?", "cit": "[CLS] e?e #u(e)p (e|f ), (4) it could be computed efficiently using counting transducers #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of converting the temporal relation between events and event and event extraction #refr. [SEP]", "cit": "[CLS] we consider two different approaches to learning a temporal dependency parser: a shift-reduce model #otherefr and a graph-based model"}
{"pre": "[CLS] #refr use a probabilistic model to model the probability of a word in a word in a sentence length", "cit": "[CLS] carbonell and #refr used sequential sentence selection in combination with maximal marginal relevance (mmr), which gives penalty to sentences"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal, parser #refr and the penn treebank #otherefr. [SEP]", "cit": "[CLS] the morphology database \\[#refr1 was originally exlracted from 1979 edition of the collins english dictionary and oxford adwmced learner"}
{"pre": "[CLS] in this paper, we propose a dynamic programming algorithm for dependency parsing #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] for a more formal definition see previous work #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] for example, a part-of-speech tagger could constrain a ?base phrase? chunker #otherefr, or the n-best output of a parser"}
{"pre": "[CLS] #refr use a supervised sentiment classifier to detect the polarity of words in a target words in a target", "cit": "[CLS] #refr use a textual representation of words by collating all the glosses of the word as found in some"}
{"pre": "[CLS] in the last decade, there has been a lot of work on joint inference in english #otherefr; #refr. [SEP]", "cit": "[CLS] empirically, the performance of these methods has been shown to be good, and similar to that of em #otherefr;"}
{"pre": "[CLS] the first is the case for the word segmentation model of word segmentation #refr, which uses a word segmentation", "cit": "[CLS] some have assumed only partially tagged training corpora #otherefr; #refr, requiring no direct supervision in the target language). [SEP]"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] is a growing number of different", "cit": "[CLS] ted alignments given two predicates or arguments in two sentences, we attempt to align the two sentences they appear"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a task of identifying the degree of semantic relations between words", "cit": "[CLS] first, the notion of semantic verb relations is crucial for many nlp tasks and applications such as verb clustering"}
{"pre": "[CLS] the first is the task of identifying the semantic textual similarity of a text of a text similarity of", "cit": "[CLS] approaches to semantic similarity have often operated at separate levels: methods for word similarity are rarely applied to documents"}
{"pre": "[CLS] the second dataset is the same as the use of the same as the same as the same as", "cit": "[CLS] previous research has found that combining multiple knowledge sources achieves high wsd accuracy #otherefr; #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained on the penn treebank corpus and tested on the penn treebank wall street journal, parser", "cit": "[CLS] impressive results have been achieved culminating in the state-of-the-art parser of clark and curran #otherefr which has been used"}
{"pre": "[CLS] the task of identifying the semantic relations between nominals of a text is also relevant to a text to", "cit": "[CLS] these models can tell us that hella was #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] shared", "cit": "[CLS] the bionlp?09 shared task on event extraction #refr, the first large scale evaluation of biomedical event extraction systems, drew"}
{"pre": "[CLS] the most popular approach to use a word lattice representation of a word sequence of a word sequence of", "cit": "[CLS] #refr use string edit distance to identify closely-related candidate orthographic forms and then decode the message using a language"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best results for the best", "cit": "[CLS] alternating ems would be expensive here, since updates take (at least) o(l3) time, and hard em?s objective (l ="}
{"pre": "[CLS] we use the berkeley parser #refr to obtain the syntactic parse trees for each sentence and the english and", "cit": "[CLS] various efforts have been devoted to improving decoding efficiency, including hypergraph rescoring #otherefr; #refr and grammar transformations #otherefr. [SEP]"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] popular alternatives such as pairwise ranking objective (pro) #refr, mira #otherefr use surrogate optimization objectives that indirectly attempt to"}
{"pre": "[CLS] the most common approach to paraphrase acquisition is to use a lexicon to automatically extracted from corpora #otherefr; #refr.", "cit": "[CLS] the bulk of work on compositionality has been language/construction-specific and operated at the token-level, using dedicated methods to identify"}
{"pre": "[CLS] the second approach is to use a word lattice as a sequence of a sequence of a sequence of", "cit": "[CLS] it is also possible to train statistical models using unlabeled data with the expectation maximization algorithm #refr. [SEP] [PAD]"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] the optimal parse can be found using a spanning tree algorithm #otherefr; #refr. . [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of identifying the semantic textual similarity of a target word is a document #otherefr; #refr. [SEP] of", "cit": "[CLS] in recent years, a variety of approaches have been proposed for automatic induction of levin style classes from corpus"}
{"pre": "[CLS] #refr use a similar approach to compute sentence importance. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] sentence compression has been widely studied in language processing. #otherefr applied the noisy-channel framework to predict the possibilities of"}
{"pre": "[CLS] in the literature, supervised approaches have been applied to relation extraction #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] examples include using bootstrapping to amplify small seed sets of example outputs #otherefr; #refr, leveraging existing databases that overlap"}
{"pre": "[CLS] the first is the same as #refr, which was used to create a dependency parsed with the berkeley parser", "cit": "[CLS] various types of dgs are used in existing systems according to these classifications, such as non-label word dg#otherefr; #refr4,"}
{"pre": "[CLS] the most common approach to this problem is to use a dictionary to model the similarity score for each", "cit": "[CLS] recently, more sophisticated methods are innovated for sp based on topic models, where the latent variables (topics) take the"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a text that are semantically similar to the", "cit": "[CLS] some approaches have utilised the visual attributes of objects #otherefr; #refr, relied on an external corpus to predict the"}
{"pre": "[CLS] the first is the automatic metric of text with human annotation of text segments as described in #refr. [SEP]", "cit": "[CLS] there are various approaches for automatically generating back-of-the-book indices and thus potential descriptors (e.g., #refr), but these are intended"}
{"pre": "[CLS] the grammar is a grammar that is a parsing model that of #refr and a probability distribution over a", "cit": "[CLS] a number of nonconcatenative grammar formalisms has been put forward, such as head-wrapping rammars #otherefr. and tree adjoining grammars"}
{"pre": "[CLS] in this paper, we propose a new model for unsupervised learning of #refr. [SEP] [SEP] [SEP] [SEP] ? ?", "cit": "[CLS] furthermore, much recent work has focused on the challenge of joint sentence extraction and compression, also known as compressive"}
{"pre": "[CLS] #refr proposed a method for learning a word sense disambiguation which is a word sense disambiguation method to identify", "cit": "[CLS] models based on maximum entropy are therefore well suited to the sentence extraction task, and furthermore, yield competitive results"}
{"pre": "[CLS] the system is based on the moses toolkit #otherefr and the standard phrase-based smt system described in #refr. [SEP]", "cit": "[CLS] id participant cmu carnegie mellon university, usa #otherefr nrc national research council, canada #refr ntt nippon telegraph and telephone,"}
{"pre": "[CLS] the first is the task of coreference resolution #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] ? the typed-similarity [SEP] ?", "cit": "[CLS] a common approach to performance analysis is to calculate scores for nominals, pronouns and proper names separately, but this"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] as a first step in the reordering process, we parse the sentence using the parser described in #refr. [SEP]"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] this mechanism is further enhanced with an n-gram model of bilingual units built using pos tags (crego and #refr."}
{"pre": "[CLS] the first is the case for the automatic acquisition of subcategorization frames from corpora #otherefr, #refr, and many others.", "cit": "[CLS] the knowledge based methods are morphosyntactic processing \\[#refrb\\] and some shallow level of semantic categorisation. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] reports advocating the use of religious texts for general purpose clir may have been overly optimistic #refr. [SEP] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a text with a text corpus to be", "cit": "[CLS] the same annotation scheme as in our previous work on anger detection has been applied, see e.g. #refr. [SEP]"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a task of identifying the degree of semantic relations between words", "cit": "[CLS] research is only just beginning to escape from a ?time-based? mode of annotation, for instance by using ?stand-off? annotations"}
{"pre": "[CLS] the most popular approach to phrase-based translation models #otherefr; #refr rely on the source side and the target side", "cit": "[CLS] however, most existing tuning algorithms treat the decoder as a black box #otherefr; #refr, ignoring the fact that many"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] the", "cit": "[CLS] therefore, to annotate all computer science papers, we cannot develop predefined entity ontologies, which is the typical approach taken"}
{"pre": "[CLS] the grammar is a grammar that is a grammar that is a given a grammar that is a grammar", "cit": "[CLS] this has been successfully applied in combinatorial categorial grammar #otherefr; #refr; as ccg is a lexicalist framework, grammar learning"}
{"pre": "[CLS] the first is the task of identifying the polarity of a text that is a text of a text", "cit": "[CLS] we took the idea of distinctive personal attributes as a criterion for disambiguation from the work of #refr. [SEP]"}
{"pre": "[CLS] we use the stanford corenlp suite to lemmatize and partof-speech tag each word #otherefr; #refr, and parse the input", "cit": "[CLS] motivated in part by #refr, we create cluster-level features from the relational features in our feature set using four"}
{"pre": "[CLS] the most popular approach to machine translation is the most popular approach proposed in #refr, where the authors use", "cit": "[CLS] a phrase table of paraphrases is extracted from bilingual phrase tables #refr, and paraphrasing quality is improved by additional"}
{"pre": "[CLS] the first is the process of annotating the discourse structure in text with discourse structure of discourse relations #refr.", "cit": "[CLS] topic modeling methods have also been applied to the identification of topical segments in speech #otherefr #refr. [SEP] [PAD]"}
{"pre": "[CLS] the system is based on the inprotk project #refr. [SEP] system, which was originally developed for the system #refr", "cit": "[CLS] spoken dialogue systems are emerging as an intuitive interface for providing conversational access to online information sources #otherefr; #refr;"}
{"pre": "[CLS] we use the stanford named entity recognizer #refr to extract named entities from the stanford named entities from the", "cit": "[CLS] while this phenomenon is problematic for applications like scientific summarisation #otherefr, it has a particular relevance for citation sentiment"}
{"pre": "[CLS] the grammar we are extracted from the treebank using the stanford parser #refr. [SEP] parser #refr with the collins", "cit": "[CLS] also, by partially lexicalising the rule extraction process (i.e., by using some more frequent words as well as the"}
{"pre": "[CLS] the first is the case for the generation of referring expressions that we will use the discourse treebank (rst-dt)", "cit": "[CLS] it has often been pointed out, however, that there can be an additional requirement of syntactic parallelism (see for"}
{"pre": "[CLS] the model is trained using the moses decoder #refr. [SEP] score #refr with the decoder that the best alignment", "cit": "[CLS] it has been demonstrated to be a successful framework with comparable performance with other statistical frameworks and suitable for"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a task of identifying the degree of semantic relations between words", "cit": "[CLS] the acquisition of clues is a key technology in these research efforts, as seen in learning methods for document-level"}
{"pre": "[CLS] the task of identifying the semantic textual similarity of textual similarity of a target similarity between two sentences is", "cit": "[CLS] thus the cross-lingual textual entailment task #otherefr #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] shared", "cit": "[CLS] yet even under purely methodologically premises, it has gained wide-spread attention as witnessed by recent activities performed as part"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] shared", "cit": "[CLS] we built a global reranking parser model using multiple decoders from mstparser #refr; mcdonald et al, 2005b). [SEP] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a semantic role labelling task", "cit": "[CLS] many other approaches have been pursued for various languages: #otherefr for greek, #refr for czech, #otherefr for spanish, to"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the longrange reordering model is to use of the", "cit": "[CLS] an appealing alternative is to rerank the hypergraph directly #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr used a supervised method to detect sentiment in the sentiment in the context of words in the text", "cit": "[CLS] a simple technique in this approach is to start with some sentiment words as seeds which are then used"}
{"pre": "[CLS] the first is the task of converting natural language processing #otherefr; #refr. [SEP] [SEP] [SEP] to be a given", "cit": "[CLS] of course, the parsers performed very poorly on ellipsis involving two nouns (partly because np structure is absent from"}
{"pre": "[CLS] the first is the task of identifying the polarity of a text with a text that is a text", "cit": "[CLS] similarity of local contexts we regard words that have dependency relations from/to the target expression as the local contexts"}
{"pre": "[CLS] we use the moses toolkit #refr to train the weights for the feature weights for the log-linear models. [SEP]", "cit": "[CLS] scoring relies on a 30- dimensional lsa word vector space trained with the s-space software #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the automatic acquisition of subcategorization frames from the web #otherefr; #refr. [SEP] test", "cit": "[CLS] #refr extended this work by implementing a discriminative parse selection model, incorporating word sense information and achieved great improvements"}
{"pre": "[CLS] in this paper, we propose a new model for estimating the hierarchical structure of #refr. [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] fine-grained opinion analysis aims to detect the subjective expressions in a text (e.g. ?hate?) and to characterize their intensity"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to learn a translation models from a", "cit": "[CLS] for instance, the hierarchical translation system of chiang #otherefr, #refr and huang et al #otherefr use target-language syntax. [SEP]"}
{"pre": "[CLS] the first is the case for the case of the grammar formalism described in #refr. [SEP] project at the", "cit": "[CLS] we already discussed elsewhere #refr rules for focus assignment, where focus was d i rect ly compared to intonational"}
{"pre": "[CLS] #refr proposed a method for identifying the polarity of a word in a target word in a target language", "cit": "[CLS] a system is usually trained on wellformed native english text #refr, but several works incorporate into training error-tagged data"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for each sentence in the training corpus.", "cit": "[CLS] linear coefficients are optimized using simulated annealing, optimizing accuracy of the 1-best transliteration in a development set. k-best lists"}
{"pre": "[CLS] #refr proposed a method for learning a word sense disambiguation which is a word sense disambiguation method to extract", "cit": "[CLS] this work is a part of ibm?s systemt project #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr. [SEP]", "cit": "[CLS] this idea has been developed and applied to a wide variety tasks, including morphological analysis #otherefr; #refr. [SEP] [PAD]"}
{"pre": "[CLS] the most common approach to deriving selectional preference acquisition is to use a lexicon to measure the similarity between", "cit": "[CLS] distributional approaches, on the other hand, rely on text corpora, and model relatedness by comparing the contexts in which"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] li and #refr also studied a method for learning dependencies between case slots and evaluated the discovered ependencies in"}
{"pre": "[CLS] the task of identifying the semantic relations between nominals of noun phrases and their relations has been used in", "cit": "[CLS] sophisticated techniques are sometimes employed to identify the type of the expected answers of open-domain questions #refr. [SEP] [PAD]"}
{"pre": "[CLS] the system is based on the inprotk toolkit for english all-words task #refr. [SEP] test prep and test prep", "cit": "[CLS] #refr also uses tsg to judge grammaticality of a sentence written by language learners. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] first the parallel data is word-aligned, normally using the ibm models #otherefr, then phrases are extracted using some heuristics"}
{"pre": "[CLS] the second approach is to use a small number of manually annotated corpora such as the same data set", "cit": "[CLS] the part-of-speech induction challenge received two submission, #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] however, since some methods for detecting sentence boundaries have already been proposed #otherefr; #refr, we assume that they can"}
{"pre": "[CLS] the second approach is to use a morphological analyzer as a sequence of morphological analyzer for arabic morphological analyzer", "cit": "[CLS] concatenative morphology lends itself well to an analysis in terms of finite-state transducers (fsts) #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the sentence in", "cit": "[CLS] given a tree pair (f , c), whose respective parses (pif , pic) were generated by the parser described"}
{"pre": "[CLS] #refr use a similar approach to identify the polarity of words in a sentence and their polarity in a", "cit": "[CLS] previous work has studied and used citation sentences in various applications such as: scientific paper summarization #otherefr; #refr, and"}
{"pre": "[CLS] the most popular approach to translation models #otherefr; #refr rely on the source side and the target side #otherefr.", "cit": "[CLS] #refr perform morphological decomposition of arabic words, such as splitting of prefixes and suffixes. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] some methods are based on likelihood #otherefr, error rate #refr, margin #otherefr is the most popular one. [SEP] [PAD]"}
{"pre": "[CLS] the first is the process of annotating the sentence planning problem of determining which a sentence in a sentence", "cit": "[CLS] an error identification component uses an english grammar augmented with rules to cover grammatical errors commonly produced by our"}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders and opinion holders in opinion holders in opinion holders in", "cit": "[CLS] #refr, malioutov and barzilay #otherefr). [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders and opinion holders in opinion holders in opinion holders in", "cit": "[CLS] in previous work, we proposed a method that uses participantto-participant and participant-to-topic attitudes to identify subgroups in ideological discussions"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] for instance, #refr and #otherefr address training methods coping with noisy parallel data, in the sense that translations do"}
{"pre": "[CLS] the first is the task of coreference resolution #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] ? ? ? ?", "cit": "[CLS] due to the one-to-one entity alignment constraint, thef-measure here is more stringent than the accuracy #otherefr; #refr computed on"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within the source sentence into a target sentence into", "cit": "[CLS] numerous researchers have proposed a variety of heuristics for recognizing abbreviation definitions, e.g., the use of initials, capitalizations, syllable"}
{"pre": "[CLS] the first is the automatic evaluation of the automatic metric of the automatic evaluation of the automatic evaluation of", "cit": "[CLS] even for relatively frequent ncs occurring 10 times or more in the bnc, static english dictionaries provide only 27%"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr. [SEP]", "cit": "[CLS] in contrast, several unsupervised methods which do not require human transcription and annotation have been recently proposed to learn"}
{"pre": "[CLS] #refr use a similar approach to compute sentence importance. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] graph-based methods have been used to great effect in nlp, on problems such as word sense disambiguation #otherefr, and"}
{"pre": "[CLS] in the case of dependency parsing, previous work has shown that it is possible to improve the performance of", "cit": "[CLS] we show how nivre?s #otherefr swap-based reordering technique for non-projective shift-reduce-style parsing can be integrated into the non-directional easy-first"}
{"pre": "[CLS] the task of identifying the overall sentence pairs in text is a text #refr. [SEP] of the same event", "cit": "[CLS] for in-sentence negation and speculation detection, morante et al #otherefr developed scope ? i.e. content shifted text spans ?"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] event", "cit": "[CLS] for this purpose, we use the date normalization system described in #refr. . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the stanford ner tagger #refr and the stanford parser #otherefr. [SEP] shared task on event extraction", "cit": "[CLS] this motivates our desire to find an 1we parsed the documents into typed dependencies with the stanford parser #refr."}
{"pre": "[CLS] the first is the same as #refr, which we will use the same as the same as the same", "cit": "[CLS] uchimoto et al have proposed a technique for learning the dependency probability model based on a maximum entropy method"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for learning and beam-search decoding #otherefr;", "cit": "[CLS] when restricted to these three transitions, the system is equivalent to the so-called stack-based arc-standard model of #refr. [SEP]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] future works aim at transforming such non-projectable trees into projectable form #otherefr, driven by translation rules from aligned data#refr,"}
{"pre": "[CLS] the second dataset is the use of a word sense disambiguation algorithm which is based on the most similar", "cit": "[CLS] #refr present the first approach, supervised, to disambiguating all words in a sentence with sense association (or selectional) preferences"}
{"pre": "[CLS] we use the berkeley parser #refr to obtain the syntactic parse trees for each sentence and the english and", "cit": "[CLS] several researchers #otherefr; #refr have already proposed methods for binarizing synchronous grammars in the context of machine translation. [SEP]"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs in the sentence ordering.3 most of a text that", "cit": "[CLS] there has been an increasing number of published works on situated language understanding#otherefr; #refr, focusing on interpretation of referents"}
{"pre": "[CLS] in this paper, we propose a method for extracting paraphrase pairs from parallel bilingual parallel bilingual news articles from", "cit": "[CLS] phoneme based models, such as, the ones based on weighted finite state transducers (wfst) #refr and extended markov window"}
{"pre": "[CLS] the work of #refr is similar to ours in that it is possible to use a more complex model", "cit": "[CLS] the algorithm is implemented within our dante system for temporal expression interpretation #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the polarity of a sentence, a word is a word is then", "cit": "[CLS] #refr propose a framework to define the composition c = f(a, b, r,k) where r is the relation between"}
{"pre": "[CLS] the most popular approach to translation models #otherefr; #refr rely on parallel data #otherefr. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] the absolute improvement of 0.75 over using 1-best alignments #otherefr is statistically significant at p < 0.05 by using"}
{"pre": "[CLS] the work of #refr is similar to ours in that it is possible to use a more complex linguistic", "cit": "[CLS] #refr have identified similar tree structure violations in the turkish discourse bank #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to paraphrase generation has been the extraction #otherefr; #refr. [SEP] model the extraction is based", "cit": "[CLS] in addition, #refr applied mt evaluation methods (bleu, nist,wer and per) to build classifiers for paraphrase identification. [SEP] [PAD]"}
{"pre": "[CLS] the most common approach to this task is to use a similarity measure which is to automatically identify the", "cit": "[CLS] our similarity measure is based on a proposal in #refr, where the similarity between two objects is defined to"}
{"pre": "[CLS] the grammar is a grammar that is a word lattice is usually a parsing strategy that is used in", "cit": "[CLS] recognition the general trend in large vocabulary continuous speech recognition research is that of building integrated systems #otherefr; #refr"}
{"pre": "[CLS] the model is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the decoder with the decoder with", "cit": "[CLS] alternatively, hypotheses can be generated on demand with cube growing #refr, though we note that it showed little improvement"}
{"pre": "[CLS] #refr proposed a method for learning semantic parsers from a target language for a target language for a target", "cit": "[CLS] nivre and k?ubler #otherefr, #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to word alignment is to use a word alignment model that is to predict the", "cit": "[CLS] improving the syntax-based approach for synonym identification using bilingual dictionaries has been discussed in lin et al #otherefr and"}
{"pre": "[CLS] the grammar is a grammar that is a grammar that is a parsing system for the grammar that is", "cit": "[CLS] the work presented in the current article is related to previous work on corpus-based grammar specialization as presented in"}
{"pre": "[CLS] the second approach is to use a small number of seed examples for example outputs #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] in nlp, clustering has been used for virtually every semiand unsupervised task, including pos tagging #otherefr, lexical acquisition #refr,"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs in the sentence planning problem of identifying the sentence", "cit": "[CLS] therefore, for each unaccepted top-level belief, our process for selecting the focus of modificatkm involves two steps: identifying acandidate"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a text with a text corpus consisting of", "cit": "[CLS] recent works have shown that eye gaze can facilitate spoken language processing in conversational systems (qu and #refr. [SEP]"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] there are several studies on feature extraction #otherefr, #refr, kobayashi et al, 2007, scaffidi et al, 2007, stoyanov and"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] ? ? ? ? ?", "cit": "[CLS] #refr and rigau et al #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second approach is to use a small number of seed examples for example outputs #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] among these methods, semi-markov-based ones have shown good performance in terms of accuracy #otherefr; #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second dataset is the use of a word sense disambiguation algorithm which is based on the most similar", "cit": "[CLS] notably, the performance of the best sense-remapped wsi systems surpasses the performance of many supervised wsd systems in previous"}
{"pre": "[CLS] the task of identifying the semantic relations between nominals of noun phrases and their arguments are also key to", "cit": "[CLS] prior work in relation discovery #refr has investigated the problem of finding relationships between different classes. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the parsing model presented in #refr, which is the authors present a probabilistic", "cit": "[CLS] this two-dimensional parametrization has been instrumental in devising parsing models that improve disambiguation capabilities for english as well as"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] the fifth line, labeled learned alignments, shows the impact of replacing manual alignment annotations with learned model 1 alignments,"}
{"pre": "[CLS] the first is the process of annotating the word in a vector space model of a word in a", "cit": "[CLS] nc bracketing determines the syntactic structure of an nc as expressed by a binary tree, or, equivalently, a binary"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? 2008. [SEP]", "cit": "[CLS] up to now, such classifications have been used in applications such as word sense disambiguation #otherefr, and in statistical"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] ensemble semantics is a relation acquisition framework, where semantic relation candidates are extracted from multiple sources and a single"}
{"pre": "[CLS] the parser is trained on the penn treebank corpus and tested on the penn treebank wall street journal, parser", "cit": "[CLS] #refra,b) and dienes #otherefr approached the problem by preidentifying empty categories using an hmm on unparsed strings and threaded"}
{"pre": "[CLS] in the context of sentiment analysis and opinion mining has been used to extract opinion expressions #otherefr; #refr. [SEP]", "cit": "[CLS] some of the work is not related to discourse at all #otherefr and word-based measures like tf- idf #refr)."}
{"pre": "[CLS] in the literature, supervised approaches have been applied to unsupervised learning of natural language processing #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] moreover, by adding the automatically tagged data to the original ner training corpus and retraining the monolingual model using"}
{"pre": "[CLS] the task of identifying the semantic textual similarity of a target word is a target word is often used", "cit": "[CLS] also, because it has been shown #refr that negative pmi values worsen the distributional similarity performance, we bound pmi"}
{"pre": "[CLS] the second approach is to use a word clusters as described in #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] to analyze the sentences we applied the stanford parser (version 1.6.2), which is based on lexicalized probabilistic context free"}
{"pre": "[CLS] the first is the automatic evaluation of discourse structure of discourse relations #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] using the web to overcome data sparseness has been attempted before #refr; however, there are issues: misspellings, typographic errors,"}
{"pre": "[CLS] the second approach is to use a small number of seed examples for example outputs #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] semi-supervised learning #otherefr 94.32 #refr 95.15 with additional resources #otherefr 94.17 (daume. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the one of the most popular approach to use crowdsourcing, which has been to use a", "cit": "[CLS] #refr proposes to incorporate the cohesionconcerned revision process into summarization. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to word alignment is the ibm models #otherefr; #refr. [SEP] model 1 and the hmm", "cit": "[CLS] to calculate statistical significance, we use the boot-strap resampling method of #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] model with respect to the entire candidate set.", "cit": "[CLS] supervised computational systems can be trained to mark up the az structure of a text automatically #otherefr; #refr. [SEP]"}
{"pre": "[CLS] the most common approach to machine translation is to use a log-linear combination of statistical machine translation #otherefr; #refr.", "cit": "[CLS] #refr proposed a factored smt model as an extension of the traditional phrase-based smt model, which opens up an"}
{"pre": "[CLS] the second approach is to use a large set of features to extract common words from a large corpus", "cit": "[CLS] consequently, current semantic parsers are mostly restricted to quite limited domains, such as querying a specific database #otherefr; #refr."}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] shared", "cit": "[CLS] events with site arguments such as that shown in the figure are targeted in the ge, epi, and id"}
{"pre": "[CLS] the translation quality is measured by bleu #refr. [SEP] score #refr on the test set. [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] this sequential property is well suited to hmms #refr, in which the jumps from the current aligned position can"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] minimum error rate training #otherefr; #refr, dependency parsing #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the identification of the language model of the meaning of the subject of", "cit": "[CLS] in recent years, visual analytics systems have increasingly been used for the investigation of linguistic phenomena in a number"}
{"pre": "[CLS] #refr use a similar approach to identify the topics of discourse relations in a document to identify the discourse", "cit": "[CLS] the use of structured probabilistic topic models has made it possible to represent document set content with increasing complexity"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr. [SEP]", "cit": "[CLS] #refr compares several algorithms for the me estimation including gis, iis, and the limitedmemory variable metric (lmvm) method, which"}
{"pre": "[CLS] the system used for the experiments in this paper is based on the senseval-3 english lexical substitution task of", "cit": "[CLS] as a somewhat radical alternative to taxonomical relationships, other ways of measuring semantic similarity based on distributional evidence have"}
{"pre": "[CLS] the first is the task of identifying the event descriptions of coreference resolution of a text in a text", "cit": "[CLS] 4for an alternative way of using narrative chains for coreference resolution, see #refr. tecedent, respectively.5 for our running example,"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be np-hard #refr. [SEP] s", "cit": "[CLS] for evaluation, the nist bleu script (version 13) with the default setting is used to calculate the bleu score"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the word alignments for each word and then extract the word", "cit": "[CLS] one can perhaps identify these using posterior probabilities (e.g., using techniques in #refr) and generate additional morphologically valid words"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the training data is to", "cit": "[CLS] the best statistical parsers have shown good results on this benchmark, but these statistical parsers demonstrate far worse results"}
{"pre": "[CLS] the grammar we are extracted from the treebank using the stanford parser #refr. [SEP] parser #refr with the parser", "cit": "[CLS] much work has been done related to enhancing the efficiency of deep interpretation systems #otherefr; #refr, which forms the"}
{"pre": "[CLS] the first is the automatic metric of subjectivity analysis tasks #refr. [SEP] quality estimation or phrase or phrase or", "cit": "[CLS] with the rapid growth of sms and social media content, text normalization system has drawn increasing attention in the"}
{"pre": "[CLS] the first is the task of identifying the semantic relations between nominals of noun phrases and the verb classes", "cit": "[CLS] a lot of noise needed to be manually discarded. search, but a starting point would be the approach by"}
{"pre": "[CLS] the second method is based on the word similarity score #refr and the similarity between two words in a", "cit": "[CLS] others have adopted the same underlying idea, using alternative methods and techniques #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with the one of", "cit": "[CLS] syntactic parsing using dependency structures has become a standard technique in natural language processing with many different parsing models,"}
{"pre": "[CLS] the first is the task of identifying the mentions of a single document to be a text that is", "cit": "[CLS] this is sometimes called a switching fhmm #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the penn treebank #otherefr. [SEP] score", "cit": "[CLS] this technique has been shown to provide significant improvements in accuracy for both english and german (?#refr, and a"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? 2008. [SEP]", "cit": "[CLS] there are already applications that utilize such knowledge (e.g., #refr for textual entailment). [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] mert has been successfully used in practical applications, although, it is known to be unstable #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal (wsj) treebank #otherefr; #refr. [SEP] trees with the", "cit": "[CLS] this can be seen in state-of-the-art constituency-based parsers such as collins #otherefr, #refr, and petrov et al #otherefr. [SEP]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] one or both of these limitations have led to recent introduction of alternative optimization strategies, such as minimum-risk #refr,"}
{"pre": "[CLS] in the context of machine translation, itg has been explored for paraphrase generation #otherefr; #refr. [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] previous studies have shown that paraphrasing can play important roles in plenty of areas, such as machine translation #otherefr,"}
{"pre": "[CLS] the first is the task of identifying the sentence planning problem that the user is a text generation of", "cit": "[CLS] the mre is underpinned by an approach to the modelling of interaction in terms of obligations that different utterance"}
{"pre": "[CLS] the first is the automatic metric of text with human annotation of text #refr. [SEP] quality estimation (qe) and", "cit": "[CLS] recent studies #refr have demonstrated the viability of amazon?s me-chanical turk as a data collection platform for tasks including"}
{"pre": "[CLS] #refr proposed a method for extracting relations from corpora and noun phrases in the british national corpus of wikipedia", "cit": "[CLS] in order to automatically obtain large domain glossaries, in recent years computational approaches have been developed which extract textual"}
{"pre": "[CLS] the most popular approach to paraphrase generation has been the extraction #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] such a parallel data-text resource could then be used to train an existing data-to-text generation system, or even to"}
{"pre": "[CLS] the grammar is a grammar that is a grammar that is a given a grammar that is a grammar", "cit": "[CLS] in most formal theories of human sentence comprehension, input recognition and syntactic analysis are taken to be distinct processes,"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking approaches #otherefr; #refr to learning for dependency", "cit": "[CLS] however, recently #refrb) demonstrated that web markup can successfully guide hierarchical syntactic structure discovery, observing, for example, that anchors"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the sentence in", "cit": "[CLS] state-ofthe-art methods for edit region detection such as #otherefr; #refr model speech disfluencies as a noisy channel model, though"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] deterministic methods for dependency parsing have now been applied to a variety of languages, including japanese #refr, english #otherefr."}
{"pre": "[CLS] the second approach is to use a small number of seed examples for example outputs #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] several techniques for nested ner in genia where presented in #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to learn a translation probabilities from", "cit": "[CLS] this is particularly true in the framework of phrase-based smt #otherefr; #refr, an approach that remains highly competitive despite"}
{"pre": "[CLS] the task of identifying the semantic textual similarity of a target word is a target word is a sentence", "cit": "[CLS] in order to produce the training data for the classifiers, we first tokenized the text for all six languages"}
{"pre": "[CLS] in the context of machine translation, itg has been explored for a variety of nlp tasks such as semantic", "cit": "[CLS] although statistical approaches to automatic term recognition, e.g. #refr, have achieved relative success over the years, the addition of"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] the syntax-based statistical machine translation (smt) models #refr use rules with hierarchical structures as translation knowledge, which can capture"}
{"pre": "[CLS] the task of relation extraction has received much attention in recent years #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] work on supervised relation extraction has mostly employed kernel-based approaches, e.g. #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] in order to highlight the data selection work, we used an out-of-the-box moses framework using giza++ #refr and mert"}
{"pre": "[CLS] the first is the task of converting the greedy search algorithm #otherefr; #refr. [SEP] score of the best performing", "cit": "[CLS] recently, #refr combined multiple decoders for a dependency parser with a reranker, achieving 48.6 in f-score. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to machine translation is to use a log-linear combination of statistical machine translation #otherefr; #refr.", "cit": "[CLS] for discourse relations and dcs especially, more and more annotated resources have become available in several languages, such as"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] 4 million words of new york times newswire, manning acquires 4900 subcategorisation frames for 3104 verbs, an average of"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr.", "cit": "[CLS] among distributional models of word, there are some approaches that address polysemy, either by inducing a fixed clustering of"}
{"pre": "[CLS] the task of relation extraction has received much attention in recent years #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] while traditional information extraction #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the system used in this paper is based on the c&c parser #otherefr shared task #refr. [SEP] [SEP] [SEP]", "cit": "[CLS] seven languages, english plus catalan, chinese, czech, german, japanese and spanish, are involved #otherefr; #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of meaning that are not", "cit": "[CLS] as explained in #refr, there are generally two approaches to unsupervised wsd. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the last decade, there has been a lot of work on learning semantic parsers #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] to overcome the false positive problem caused by the distant supervision assumption, researches in #otherefr#refr#otherefr proposed multi-instance models to"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] since there is no guarantee that a goal hypothesis will be found in polynomial time, we apply a robustness"}
{"pre": "[CLS] we use the stanford ner tagger #refr to identify the text in the text and extract the text and", "cit": "[CLS] for example, one of the best performing general-purpose named entity recognisers (hereon referred to as stanford ner) is based"}
{"pre": "[CLS] the most common approach to statistical machine translation is to use syntactic information to enhance the source sentences #refr.", "cit": "[CLS] #refr for english; sarkar and zeman #otherefr for french. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to parsing is to use a word lattice parsing model that of a dependency parser", "cit": "[CLS] in this paper, we propose a method for adapting an hpsg parser #otherefr; #refr trained on the wsj section"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of possible senses in the", "cit": "[CLS] analysis of metonymies as a linguistic phenomenon dates back at least to the 1930s #otherefr, and are increasingly recognized"}
{"pre": "[CLS] the first is the case for the semantic role labeling task of translating natural language processing #otherefr; #refr. [SEP]", "cit": "[CLS] current work in story understanding is focusing on the use of logical forms, yet these are not extracted from"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] this property makes our approach especially promising for aligning freely, or erroneously, translated sentences, and for coping with syntactic"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of meaning that are not", "cit": "[CLS] the most widely known criteria functions used with hierarchical agglomerative algorithms are single link, complete link, and average link,"}
{"pre": "[CLS] the first is the case for the automatic evaluation of the automatic evaluation of the automatic evaluation of the", "cit": "[CLS] we also plan to experiment with techniques that acquire semantic knowledge automatically (e.g., #refr) to generate bigger and better"}
{"pre": "[CLS] we use the berkeley parser #refr to obtain the syntactic parse trees for parse trees for english and the", "cit": "[CLS] hierarchical approaches to machine translation have proven increasingly successful in recent years #refr, and often outperform phrase-based systems #otherefr"}
{"pre": "[CLS] the feature weights were tuned using mert #refr on the development set. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] a relatively simple and straight-forward method is the linear interpolation of the language model, as we explored previously #refr."}
{"pre": "[CLS] the most popular approach to paraphrase generation has been the extraction #otherefr; #refr. [SEP] model the extraction from comparable", "cit": "[CLS] abundantly available bilingual parallel corpora have been shown to address both these issues, obtaining paraphrases via a pivoting step"}
{"pre": "[CLS] the task of identifying the semantic textual similarity of a target word is a given a semantic similarity of", "cit": "[CLS] the cross-lingual textual entailment #otherefr and #refr, is an extension of the textual entailment task #otherefr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the generation of referring expressions that we will use the rst discourse treebank", "cit": "[CLS] useful properties concerning monotonicity and scope (see #refr) are thus guaranteed for a range of grammars. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a task of identifying the semantic relations between words in a", "cit": "[CLS] anna rounded the corner at elm), and other phenomena interact to produce a broad range of choices for most"}
{"pre": "[CLS] the first is the task of identifying the textual entailment (te) of identifying the same textual entailment #otherefr; #refr.", "cit": "[CLS] our predicate-argument structure-based thesatmis is based on the method proposed by hindie #refr, although hindle did not apply it"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a text with a text that are semantically", "cit": "[CLS] the work presented in this paper connects and extends several areas of research: grounded semantics #otherefr), but typically neither"}
{"pre": "[CLS] the first is the task of learning a semantic parser that is a complete parse of a semantic role", "cit": "[CLS] ltag derivation tree kernels were previously used for parse re-ranking by #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford named entity recognizer #refr to extract named entities from the text and extract named entities", "cit": "[CLS] we filtered users via language id #refr to better ensure english content. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to identify the polarity of words in a sentence. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] #refr were among the first to use mechanical turk to collect deceptive and truthful opinions ? personal stances on"}
{"pre": "[CLS] the first is the task of learning semantic parsers from a treebank of inferring dependency parser #otherefr; #refr. [SEP]", "cit": "[CLS] there have been many different proposals on how to maintain syntactic locality #otherefr and srl locality #refr when extracting"}
{"pre": "[CLS] the first is the task of word sense disambiguation which is based on the idea of a word sense", "cit": "[CLS] candito and #refr use brown clusters to replace words in a generative pcfg-la framework, obtaining substantial parsing improvements for"}
{"pre": "[CLS] the work of #refr is similar to ours in that it is possible to use a more complex model", "cit": "[CLS] marcu and soricut focussed on sentence-level parsing and developed two probabilistic models that use syntactic and lexical information #refr."}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] given the need for high-quality dependency parses in applications such as statistical machine translation #otherefr, there is a corresponding"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for parsing #otherefr; #refr for parsing", "cit": "[CLS] unfortunately, the parsing accuracies of all models have been reported to drop significantly on outof-domain test sets, due to"}
{"pre": "[CLS] we use the averaged perceptron algorithm #refr to train the weight for each of the weight of the weight", "cit": "[CLS] bold gray boxes show links gained after fully connecting the alignment. has motivated much recent work in discriminative modeling"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the penn treebank #otherefr. [SEP] s", "cit": "[CLS] active learning techniques also require a scoring function for parser confidence #otherefr, and often use uncertainty scores of parse"}
{"pre": "[CLS] the feature weights were tuned using mert #refr on the development set. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] in the literature, we can find different techniques to deal with this problem, ranging from heuristic and fast (as"}
{"pre": "[CLS] we use the stanford chinese word segmenter #refr to tokenize the english pos tagger #otherefr for chinese and the", "cit": "[CLS] #refr used pos-tags as the basis of a fully unsupervised parsing method, both for dependency and constituency parsing. [SEP]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the training data is to", "cit": "[CLS] in recent years, considerable efforts have been made in joint modeling and learning in natural language processing #refr. [SEP]"}
{"pre": "[CLS] the weights of the log-linear model are optimized with minimum error rate training (mert) #refr on the development set", "cit": "[CLS] recent proposals that use itg constraints include #refr just to name a few. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the sentence in", "cit": "[CLS] it has been the subject of much recent research since the release of the penn discourse treebank 2.0 #otherefr,"}
{"pre": "[CLS] in the second category, the context of the context of a word is used to improve the performance of", "cit": "[CLS] #refr took a semi-supervised approach, using a set of rules such as ?a noun is usually the parent of"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of meaning that are not", "cit": "[CLS] with news documents from chinese websites, collected from 10 different categories, we design an artificial test corpus in the"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be np-hard #refr. [SEP] s", "cit": "[CLS] for example, this approach has the following two applications. (1) reranking: when log-linear models #refr are employed, some features"}
{"pre": "[CLS] the model is trained using the moses decoder #refr. [SEP] model with the decoder with the decoder that we", "cit": "[CLS] to this end, we use an efficient algorithm by #refr, algorithm 3) that solves the general k-best derivations problem"}
{"pre": "[CLS] #refr proposed a method for extracting relations from wikipedia articles from corpora and the web by using wikipedia articles", "cit": "[CLS] hearst #otherefr uses a manually prepared set of initial lexical patterns in order to discover hierarchical categories, and utilizes"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] we use the srilm toolkit #otherefr to train the lm and rely on kenlm #refr for language model scoring"}
{"pre": "[CLS] the first is the same as #refr, which is the most widely used in the authors present in the", "cit": "[CLS] k&c 10=#refr; z&n 11=zhang and nivre #otherefr. achieved with a purely greedy system, with a statistically significant improvement over"}
{"pre": "[CLS] in the context of machine translation, itg has been explored for paraphrase generation #otherefr; #refr. [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] one of the most competitive dictionaries available today is hb-dict+ghm-dict+s-dict used by han et al#otherefr, which combines a manuallyconstructed"}
{"pre": "[CLS] the first is the task of identifying the semantic relations between nominals of noun phrases and semantic relations #otherefr;", "cit": "[CLS] many other previous work have been proposed to construct a knowledge base, including relation expressions #otherefr; #refr. [SEP] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] related approaches are used in hall #otherefr and #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the stanford parser #refr and then applied to obtain the grammatical relations between texts and the", "cit": "[CLS] from the view point of semantic roles of nouns, there have been several related research conducts: the mental space"}
{"pre": "[CLS] in the second international chinese word segmentation bakeoff #otherefr; #refr and the best results are achieved by the best", "cit": "[CLS] #refr ? ? ? 92.7 ? ? . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to coreference resolution is to use a coreference system #otherefr; ng and #refr. [SEP] [SEP]", "cit": "[CLS] to address the second weakness, researchers have investigated the acquisition of entity-mention coreference models (e.g., #refr, yang et al"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to learn a translation probabilities from", "cit": "[CLS] our previous work #refr developed language model state refinement for bottomup decoding in syntatic machine translation. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? ? ?", "cit": "[CLS] featurebased methods #refr transform relation instances into a large amount of linguistic features like lexical, syntactic and semantic features,"}
{"pre": "[CLS] the first is the task of relation extraction #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ?", "cit": "[CLS] various approaches to confidence estimation of learned rules have been proposed as well as methods for identifying ?so-called? negative"}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders and opinion holders in opinion holders in opinion holders and", "cit": "[CLS] we evaluate the approach using the existing mpqa corpus #refr, which we extend with manual annotations that encode topic"}
{"pre": "[CLS] the first is the automatic evaluation of the automatic metric of discourse structure of discourse structure of discourse structure", "cit": "[CLS] very large corpora obtained from the web have been successfully utilized for many natural language processing #otherefr; #refr. [SEP]"}
{"pre": "[CLS] the first is the polarity of a polarity of a polarity of a polarity of a polarity of a", "cit": "[CLS] sentiment analysis includes a variety of different problems, including: sentiment classification techniques to classify reviews as positive or negative,"}
{"pre": "[CLS] #refr proposed a method for learning a word clustering based on a word clustering words from a target language", "cit": "[CLS] #refrb) introduced a multi-prototype vsm where word sense discrimination is first applied by clustering contexts, and then prototypes are"}
{"pre": "[CLS] the first is the case for the generation of referring expressions that we will use the lexical semantics of", "cit": "[CLS] following the lessons learned from the wordnetbased inference of gricean implicatures, reported in #refr, anovel methodology ofproducing metonymic paths"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal (wsj) treebank #otherefr; #refr. [SEP] trees and the", "cit": "[CLS] to study the influence of language and treebank specific properties we will use data from arabic, czech, dutch, and"}
{"pre": "[CLS] we use the averaged perceptron algorithm #refr to train the parameters of the feature weights for the log-linear models.", "cit": "[CLS] the summation over target word sequences and alignments given fixed ? t bears a resemblance to the inside algorithm,"}
{"pre": "[CLS] the first is the polarity of a word is analyzed by using the polarity of a lexicon and then", "cit": "[CLS] for example, ?the cat ate the mouse? produces a positive affect state for the cat and a negative affect"}
{"pre": "[CLS] the first is the case for the generation of referring expressions that we will use the discourse treebank (rst-dt)", "cit": "[CLS] recently, #refr has interpreted such rules directly, using prolog as an implementation language. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to parsing is to use a word lattice parsing model that of a dependency parser", "cit": "[CLS] the choice to include two different dependency parsers but only one constituency-based parser is motivated by the study of"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] margin infused relaxed algorithm (mira) has been widely adopted for the parameter optimization in smt with a large feature"}
{"pre": "[CLS] in the future, we plan to investigate the use of dependency parsers from the conll shared task #refr. [SEP]", "cit": "[CLS] we have applied the sign test #otherefr and approximate randomization tests #refr to all pairs of systems outputs. [SEP]"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] since 1995, a few statistical parsing algorithms #otherefr; #refr demonstrated a breakthrough in parsing accuracy, as measured against he"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a task of identifying the degree of semantic relations between words", "cit": "[CLS] bos and markert #otherefr extract semantic representations with boxer #refr and incorporate background knowledge from external re- 1http://www.wiktionary.org/ 2a"}
{"pre": "[CLS] the most popular approach to word alignment is the ibm models #otherefr; #refr. [SEP] model 1 and the hmm", "cit": "[CLS] while large n-gram language models do lead to improved translation quality, they still lack any generalization beyond the surface"}
{"pre": "[CLS] the task of identifying the semantic textual similarity of a text is a text of a text #refr. [SEP]", "cit": "[CLS] cross-lingual textual entailment (clte) has been recently proposed by #refr as an extension of textual entailment #otherefr. [SEP] [PAD]"}
{"pre": "[CLS] the most common approach to this problem is to use a small number of seed examples #otherefr; #refr. [SEP]", "cit": "[CLS] in this paper, we propose a machine learning algorithm for shallow semantic parsing, extending the work of gildea and"}
{"pre": "[CLS] the second approach is to use a discriminative model #otherefr; #refr. [SEP] model to predict the probability of klein", "cit": "[CLS] 2.3.1 setting penn chinese treebank #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] model with the average number of the average", "cit": "[CLS] while some methods focus on predicting a complete formal representation of meaning #otherefr; ge and #refr, others consider more"}
{"pre": "[CLS] #refr use a similar approach to identify the polarity of words in a sentence. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] for example, #refr found that unigrams outperformed bigrams, and unigrams outperformed the combination of unigrams plus bigrams. [SEP] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to tag the one of the source language side of the target language to", "cit": "[CLS] one way of grammar refinement is data-driven state-split methods #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of learning a semantic parser that is a semantic role labeling task in the", "cit": "[CLS] more recently, several groundedlearning approaches have been proposed to alleviate the annotation burden #otherefr; #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second dataset is the same as the use of the same as the same as the same as", "cit": "[CLS] in semeval? 2010 we demonstrated that the v-measure #refr had an overwhelming bias towards systems that produce larger numbers"}
{"pre": "[CLS] the most popular approach to mt system combination is the one proposed by #refr, which uses a log-linear combination", "cit": "[CLS] #refr proposed stm, a constituent tree based approach, and hwcm, a dependency tree based approach. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] sentence pair of a", "cit": "[CLS] while ner from formal texts has been well studied, relatively little work on ner for twitter was reported. #otherefr"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set.", "cit": "[CLS] given the input sentence #otherefr; #refr as in (4), where # indicates that the morpheme is a prefix. (4)"}
{"pre": "[CLS] the task of identifying the semantic relations between nominals of noun phrases and their semantic relations has been shown", "cit": "[CLS] in recent years there has been a substantial amount of work on term extraction, including semantic class learning #otherefr;"}
{"pre": "[CLS] the wsd system used for wsd in the experiments was the experiments in the experiments described in #refr trained", "cit": "[CLS] we considered three taggers: the elworthy bigram tagger #refr within the rasp parser #otherefr, an enhanced 4http://www.cis.upenn.edu/?xtag 5http://gposttl.sourceforge.net 6we"}
{"pre": "[CLS] the second dataset is the same as the use of the same as the same as the same as", "cit": "[CLS] other attempts in this vein include mappings between wordnet and propbank #otherefr. #refr presents an automatic approach for mapping"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to learn a translation rules for", "cit": "[CLS] parsingbased translation models are implemented by joshua #otherefr, samt #refr, and cdec #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the system used in this paper is based on the c&c parser #otherefr shared task #refr. [SEP] [SEP] [SEP]", "cit": "[CLS] from the point of view of learning architectures and study of feature relevance, it is also worth mentioning the"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with the one of", "cit": "[CLS] these distributions are then used to assign a set of lexical categories to each word #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford named entity recognizer #refr to extract named entities from the text and extract named entities", "cit": "[CLS] in the ami corpus, an ap pair consists of a source utterance and a target utterance, produced by different"}
{"pre": "[CLS] the task of identifying the event descriptions of event extraction and event descriptions has been well studied in the", "cit": "[CLS] it has been previously shown by socher et al. #otherefr and #refrb) among others that rnn can effectively deal"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] we feed both correct and incorrect parses licensed by the grammar to the tadm toolkit #refr, and learn a"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the longrange reordering model is to use of the", "cit": "[CLS] collins #otherefr, #refr and ratnaparkhi #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] other works have integrated argument classification and identification into one step #otherefr, while others went further and combined the"}
{"pre": "[CLS] the first is the task of identifying the sentence planning problem that the user is a text generation of", "cit": "[CLS] some tasks where it might be helpful are ? negotiation training to induce stress in a human trainee as"}
{"pre": "[CLS] the most common approach to paraphrase acquisition is to use a lexicon to automatically extracted from corpora #otherefr; #refr.", "cit": "[CLS] accurately estimating the semantic distance between concepts or between words in context has pervasive applications in computational linguistics, including"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a series of text in which the target words are represented", "cit": "[CLS] the ontology base is a synthesis of usc/isi is pen- man upper model \\[#refr\\] and cmu is on- tos"}
{"pre": "[CLS] the semantic annotation of texts has been addressed by a number of papers on the basis of ?nlp for", "cit": "[CLS] datr \\[evans and gazdar 1989a, 1989b, 1990\\]; lrl \\[#refr\\]; \\[russell et al 1991\\]. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the grammar formalism used by #refr, which is the most likely parse, and", "cit": "[CLS] #refra) went on to describe admissible heuristics and an a* framework for parsing. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the literature, supervised approaches have been applied to part-of-speech tagging and unsupervised #otherefr and unsupervised #refr, semi-supervised learning", "cit": "[CLS] features our feature set consists of #otherefr bengali pos tagger and #refr hindi pos tagger, as well as (2)"}
{"pre": "[CLS] the bionlp 2011 shared task #refr focuses on event extraction and event extraction and event extraction and event extraction", "cit": "[CLS] over the past few years, with the development of the time- bank corpus #otherefr; #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of meaning that are not", "cit": "[CLS] with these arguments in mind, we decided to choose: (i) 15 nouns from the senseval-3 lexical sample dataset, which"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a similar to the similarity of words in the similarity of", "cit": "[CLS] several approaches to automatic detection of metaphors have been proposed #refr, all of which rely on the availability of"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for parsing #refr. [SEP] grammars with", "cit": "[CLS] moreover, the study of formal grammars is only partially relevant for research on datadriven dependency parsing, where most systems"}
{"pre": "[CLS] the most popular approach to smt is the one proposed by #refr, who use a large set of features", "cit": "[CLS] pivoting has also been used for paraphrasing and lexical adaptation #otherefr; #refr. #otherefr investigate pivot languages for resource-poor languages"}
{"pre": "[CLS] the generation of referring expressions is a subject of research in natural language generation (nlg) in natural language generation", "cit": "[CLS] #refr suggests that ?a word similarity-based ranker could align the generation output (i.e. the highest-ranked candidate) with previous utterances"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] we use the paradigm of #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the sentence in", "cit": "[CLS] recently, there has been an explosion of interest in conditional random fields #otherefr, syntactic chunking #refr and discourse chunking"}
{"pre": "[CLS] the most popular approach to word alignment is the ibm models #otherefr; #refr. [SEP] model 1 and the hmm", "cit": "[CLS] word alignment is done using giza++ #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the event extraction of a single document to be a single document", "cit": "[CLS] in ar, the research trend has been shifting from rulebased approaches #otherefr; ng and #refr because the latter are"}
{"pre": "[CLS] the first is the task of coreference resolution #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] ? the typed-similarity [SEP] ?", "cit": "[CLS] one outlier is the #refr system, which makes very few mistakes in five categories, but many in the last"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best results for the best", "cit": "[CLS] discriminative methods for training probabilistic models have enjoyed wide popularity in natural language processing, such as in part-of-speech tagging"}
{"pre": "[CLS] #refr proposed a method for learning a word clustering based on a probabilistic model that uses a hidden markov", "cit": "[CLS] the features we use are shown in table 2, which are based on the features used by #refr and"}
{"pre": "[CLS] the segmentation problem is addressed by using a large number of features #otherefr; #refr. [SEP] features for the number", "cit": "[CLS] another group of basic algorithms makes use of technique of dotplotting, originally proposed by raynar in #refr. [SEP] [PAD]"}
{"pre": "[CLS] topic models have been successfully applied to a wide range of nlp tasks including word sense disambiguation #otherefr, and", "cit": "[CLS] the studies of #otherefr; #refr continued in this vein. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] the detection of pairs of words co-occurrences is generally very simple, it is either based on the direct adjacency"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to translate the words and the", "cit": "[CLS] in order to adapt smt systems to a specific domain, recent research focuses on model adaptation techniques that adjust"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to translate the phrases as the", "cit": "[CLS] the cambridge university engineering department statistical machine translation system follows the transducer translation model #refr, a phrase-based generative model"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] algorithm with the best derivation of the best derivation", "cit": "[CLS] we nd that this induction method is an improvement over the em-based method of #refr, and that the induced"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a similar to the one used in the sense of the", "cit": "[CLS] another example is the use of patterns such as np, np * ,and othernp to find lexical semantic information"}
{"pre": "[CLS] #refr use a similar approach to identify the polarity of words in a sentence. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] unlike the regular pos taggers designed for well-written newswire-like text, social media pos taggers provide a broader set of"}
{"pre": "[CLS] we use the stanford named entity recognizer #refr to extract named entities from the stanford named entities from the", "cit": "[CLS] most work on unsupervised role induction focuses on learning verb-specific roles, starting with seed examples #otherefr; #refr. [SEP] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for each sentence in the source sentence", "cit": "[CLS] in the case of ranking models for surface realization, by far the most comprehensive experiments involving linguistically motivated features"}
{"pre": "[CLS] the model is trained using the averaged perceptron algorithm #refr. [SEP] score #refr with the best alignment model with", "cit": "[CLS] our model is derived from the hidden-markov model for word alignment #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] leveraging information from one language for the task of disambiguating another language has received considerable attention #otherefr; #refr. [SEP]"}
{"pre": "[CLS] the parser is trained on the penn treebank corpus and tested on the penn treebank corpus #refr. [SEP] test", "cit": "[CLS] some, including webanno #refr and brat #otherefr are client-side applications. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the last decade, there has been a lot of work on learning parsing with models #otherefr; #refr. [SEP]", "cit": "[CLS] the applications range from simple classification tasks such as text classification and history-based tagging #refr to more complex structured"}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders and predict the polarity of words in a sentence and", "cit": "[CLS] the idea has been formalized in the construct of lexical chains #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to joint inference is to use beam search #otherefr; #refr. [SEP] model the most probable", "cit": "[CLS] the features used to score, while based on the previous work in dependency parsing #refr, introduce some novel concepts"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for parsing #refr. [SEP] grammars with", "cit": "[CLS] table 2 shows the results on the conll 2006/2007 data sets #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be np-hard #refr. [SEP] s", "cit": "[CLS] we prepare the training data by splitting compounds in two steps, following the technique of #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the task of relation extraction has received much attention in recent years #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] open ie systems have achieved a notable measure of success on massive, open-domain corpora drawn from the web, wikipedia,"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? ? ? ?", "cit": "[CLS] we show that our models are very effective in producing features for both answer selection and extraction by experimenting"}
{"pre": "[CLS] the first is the case for the semantic interpretation of the english lexical substitution task of the penn treebank", "cit": "[CLS] casper has been used in an upgraded version of plandoc#refr, a robust, deployed system which generates reports for justifying"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for each sentence in the training corpus.", "cit": "[CLS] reranking appears extremely interesting if coupled with kernel methods #otherefr; #refr, as the latter allow for extracting from the"}
{"pre": "[CLS] we use the stanford parser #refr to extract dependency relations for each sentence and parse trees from the sentence", "cit": "[CLS] e l l ips i s and in terc lausa l coherence it has been noted in previous work"}
{"pre": "[CLS] the system uses a combination of the maximum entropy model of #refr to generate a set of features from", "cit": "[CLS] the french nerc system has been implemented with the use of a rule-based inference engine #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the first is the one of the most popular approach to use a word in which is to use", "cit": "[CLS] turkers were presented sentences from the test portion of the word sense induction task of semeval-2007 #refr, covering 2,559"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best derivation of the best", "cit": "[CLS] the field of natural language processing has recently seen some changes by the introduction of new statistical techniques that"}
{"pre": "[CLS] the first is the task of identifying the semantic relations between nominals of noun phrases and semantic relations #refr.", "cit": "[CLS] now it has the attention of many researchers in natural language processing, as shown by recent research in semantic"}
{"pre": "[CLS] the system is based on the inprotk toolkit for the best performing system described in #refr. [SEP] score for", "cit": "[CLS] indeed, a 'light' version of the #-tbl system., consisting of just one page of prolog code, has been implemented"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for parsing #refr. [SEP] grammars with", "cit": "[CLS] the two main approaches to dependency parsing are transition based dependency parsing #otherefr; #refr and maximum spanning tree based"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] event", "cit": "[CLS] tees handles all these preprocessing steps via a pipeline of tool wrappers for the ge- nia sentence splitter #otherefr,"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] in applications like the one described by #refr, where several different segmenters for chinese are combined to create the"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with the one of", "cit": "[CLS] it was first used for unlabeled dependency parsing by #refr #otherefr (for english). [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the identification of semantic roles in the context of framenet project #refr. [SEP]", "cit": "[CLS] previous studies on evaluating discourse processing (e.g., #refr have involved subjectively examining test cases to determine correctness. [SEP] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for each sentence in the training corpus.", "cit": "[CLS] this is different from our situation, as there exist high performance parsers for czech #otherefr; #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best results for the best", "cit": "[CLS] similarly to some recent successful attempts #refr, our model is typebased, arranging word types into hard clusters. [SEP] [PAD]"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal (wsj) treebank #otherefr; #refr. [SEP] trees with the", "cit": "[CLS] to study the influence of parsing methodology, we will compare two different parsers: maltparser #refr and mstparser #otherefr. [SEP]"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] several researchers attempted to use word lattices to handle generalized representation #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] consecutive siblings and grandparent parts introduce horizontal and vertical markovization #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second approach is to use a word lattice as a feature set of a feature set of similar", "cit": "[CLS] about 6,000 words are automatically classified into one of 12 domain categories by distributions in web sites #refr and"}
{"pre": "[CLS] the most popular approach to translation models #otherefr; #refr rely on parallel corpora to extract from parallel corpora from", "cit": "[CLS] ebmt systems, like other data-driven approaches, require large amounts of data to function well #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the process of annotating the one of annotating the following the following four language in the", "cit": "[CLS] further determiners of style are the domain or genre #refr, the modalities #otherefr which are more or less determined"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best derivation of the best", "cit": "[CLS] the redwoods treebank provides deeper semantics expressed in the minimum recursion semantics formalism #refr, but in the present experiments"}
{"pre": "[CLS] the first is the case for the generation of the grammar used by #refr, which is the grammar and", "cit": "[CLS] a similar technique for handling both syntactic and semantic variations can be found in #otherefr; #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford chinese word segmenter #refr to tokenize the english pos tagger #otherefr for chinese and the", "cit": "[CLS] one way to approach reordering is by extending the translation model, either by adding extra models, such as lexicalized"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with a probability distribution", "cit": "[CLS] in order to optimize the results towards a more globally optimal solution, johansson and nugues #otherefr first applied beam"}
{"pre": "[CLS] the wsd system used for wsd in the experiments was based on the senseval-3 english lexical sample task #refr.", "cit": "[CLS] raw corpora #otherefr #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in this paper, we propose a method for automatically learning paraphrases from parallel corpora of comparable corpora of bilingual", "cit": "[CLS] however, this may still be too expensive as part of an mt model that directly optimizes some performance measure,"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] while she is a fluent english speaker, she does not know at all the target language, but uses an"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the longrange reordering model is to use a word", "cit": "[CLS] in the monolingual setting, there is a wellknown tree sampling algorithm #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to parsing is to use a log-linear parsing model and maximise the conditional random field", "cit": "[CLS] morphology is determined by large inventory of word forms #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the word segmentation bakeoff #otherefr, which we use the stanford parser #refr. [SEP]", "cit": "[CLS] more recently, there has been a rule- 2in turkish, all adjectives can be used as nouns, hence with very"}
{"pre": "[CLS] the task of identifying the semantic relations between nominals of noun phrases and their arguments are also key to", "cit": "[CLS] the modeling of frame and event transitions in profinder is similar to a sequential topic model #otherefr, and is"}
{"pre": "[CLS] the task of coreference resolution is a well-studied problem #otherefr; #refr. [SEP] [SEP] task is addressed by a single", "cit": "[CLS] since we use the bart package in our experiments, 1http://www.sfs.uni-tuebingen.de/ ? versley/bart/ we include the results of the original"}
{"pre": "[CLS] the second dataset is the same as the use of the same as the same as the same as", "cit": "[CLS] like mccarthy 2this resource will be made publicly available for research purposes in the near future. et al #otherefr"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] models that employ syntax or syntaxlike representations #otherefr; #refr handle long-distance reordering better than phrase-based systems #otherefr but often"}
{"pre": "[CLS] we use the stanford ner tagger #refr to train the english and the chinese sentences. [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] other approaches have extracted parallel data from similar or comparable corpora #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to translate the phrases as the", "cit": "[CLS] word alignments were generated using the model 2 variant described in #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr proposed a method for identifying the polarity of a word in a target word in a target word", "cit": "[CLS] popular work in this area include the use of point-wise kl-divergence between multiple language models for scoring both phrase-ness"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to learn a translation probabilities from", "cit": "[CLS] many statistical translation models #refr try to model word-toword correspondences between source and target words. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained on the penn treebank corpus and tested on the penn treebank wall street journal, parser", "cit": "[CLS] it achieves its speed in part because it uses the #refr algorithm for n3 bilexical parsing, but also because"}
{"pre": "[CLS] in the case of dependency parsing, previous work has shown that it is possible to improve the performance of", "cit": "[CLS] projectivity can be relaxed in some parsers #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the generation of the generation of referring expressions from the linguistic phenomena of", "cit": "[CLS] this makes the search for an optimal tree an np-hard problem #refr as all possible trees must be considered"}
{"pre": "[CLS] the similarity between two sentences is computed using the wordnet #otherefr and the wordnet similarity measure proposed by #refr.", "cit": "[CLS] further work will include richer similarity measures, like quasi-synchronous grammars #otherefr and random walks #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the word alignment of word alignment of word alignment of word alignment using", "cit": "[CLS] thenagain, the methods proposed here are not dependent on the underlying translation model, and similar wa methods could be"}
{"pre": "[CLS] the parser is trained on the penn treebank corpus and tested on the penn treebank corpus #refr. [SEP] test", "cit": "[CLS] 1https://github.com/bllip/bllip-parser 2http://nlp.cs.nyu.edu/evalb/ 3note that this approach differs to that outlined in #refr who only perform one self-training iteration. [SEP]"}
{"pre": "[CLS] the feature weights were tuned using minimum error rate training (mert) #refr on the development set using the development", "cit": "[CLS] a character-level correction model has also been considered to reduce the out-of-vocabulary rate in translation systems #refr. [SEP] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to induce a set of candidate answers to the target word is used in", "cit": "[CLS] figure 1: a discourse tree #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best derivation of the best", "cit": "[CLS] machine learning methods were employed by #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second approach is to use mismatches in which the same lexical and syntactic features as the context #refr.", "cit": "[CLS] active learning has been applied to a number of natural language processing tasks like pos tagging #otherefr; #refr and"}
{"pre": "[CLS] the most common approach to paraphrase generation is to use a lexicon to paraphrase database into a single sentence", "cit": "[CLS] we thus build syntactic dependencies for both the original and rewritten sentence, using the french version #otherefr of the"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a similar to the one used in the sense of the", "cit": "[CLS] the method for natural language generation implemented in d2s is hybrid in nature #otherefr; #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a task of identifying the degree of semantic relations between words", "cit": "[CLS] although there exist several manually-created verb lexicons or ontologies, including levin?s verb taxonomy, verbnet, and framenet, automatic verb classification"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a text with a text corpus consisting of", "cit": "[CLS] many researchers have tackled this problem by developing asr confidence measures based on utterance-level information and dialogue-level information #otherefr;"}
{"pre": "[CLS] the model is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the em algorithm #refr with the", "cit": "[CLS] a variety of concept-totext generation systems have been engineered over the years, with considerable success #otherefr, #refr). [SEP] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to induce a set of seed words from a dictionary to determine the meaning", "cit": "[CLS] #refr likewise cluster tweets using k-means but predict location only at the country level. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the context of sentiment analysis and opinion mining has been used to improve the performance of sentiment analysis", "cit": "[CLS] for polarity classification, some previous work used spectral techniques #refr or co-training #otherefr to mine the reviews in a"}
{"pre": "[CLS] #refr proposed a method for learning a word segmentation and a word segmentation model that uses a word segmentation", "cit": "[CLS] taking chinese word segmentation for example, the state-of-the-art models #otherefr; #refr are usually trained on human-annotated corpora such as"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a text with a text corpus consisting of", "cit": "[CLS] following #refr we can distinguish between dataand knowledge-driven word sense disambiguation (wsd). [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] some scfg rule extraction techniques require only viterbi word alignment links between the source and target sides of the"}
{"pre": "[CLS] the second approach is to use a word lattice as a sequence of word segmentation and its context #otherefr;", "cit": "[CLS] aside from the language specific information and the good initialization, they also manually reduce the noise in the word?tag"}
{"pre": "[CLS] the first is the process of annotating the sentence planning problem of determining which a sentence in a sentence", "cit": "[CLS] the domain of such systems is usually a small visual scene, e.g. a number of objects, such as cups"}
{"pre": "[CLS] in the context of machine translation, itg has been explored for paraphrase generation #otherefr; #refr. [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] paraphrase acquisition is mostly done at the sentence-level, e.g., #otherefr; #refr, which is not straightforward to be used as"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] shared", "cit": "[CLS] table 1: an overview of bionlp-st?11 tracks in the follow-up event to bionlp?09 shared task on event extraction, organizers"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a text that is a", "cit": "[CLS] iii and #refr; daume. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a similar to the one used in the sense of the", "cit": "[CLS] other work in automatic lexical semantic lassification has taken an approach in which clustering over statistical features is used"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? 2008. [SEP]", "cit": "[CLS] we use tinysvm2 along with yam- cha3 #otherefr #refr as the svm training and classification software. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] johnson #otherefr; #refr; the best results we are aware of are due to schmid #otherefr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the averaged perceptron algorithm #refr to train the weight for each system. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] we evaluated the bleu scores #refr of translations from chinese into english and from english into chinese, as well"}
{"pre": "[CLS] the most common approach to machine translation is to use a log-linear combination of statistical machine translation #otherefr; #refr.", "cit": "[CLS] a more detailed description of the epps data can be found in #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] optimization with regard to the bleu score is done using minimum error rate training as described in #refr. [SEP]"}
{"pre": "[CLS] the first is the automatic evaluation of the automatic metric of the automatic evaluation of the automatic evaluation of", "cit": "[CLS] recently, two corpora with text revision information have been compiled #refr, but neither contain feedback from language teachers. [SEP]"}
{"pre": "[CLS] the first is the annotation tool which was the annotation tool #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] the identification of the expected answer type is based either on binary semantic dependencies extracted from the syntactic parse"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the longrange reordering problem as a sequence of words", "cit": "[CLS] #refr used this method for word sense disambiguation. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the literature, slight variations of this problem are also referred to as unsupervised and semi-supervised learning #otherefr, #refr,", "cit": "[CLS] almost all of the work in the area of automatically trained taggers has explored markov-model based part of speech"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] selective sampling has typically been applied to classification tasks, but has also been shown to reduce the number of"}
{"pre": "[CLS] the task of identifying the semantic relations between nominals of noun phrases and their semantic relations has been shown", "cit": "[CLS] they have proved useful for various nlp application tasks, e.g. parsing, word sense dishttp://www.nlm.nih.gov/research/umls ambiguation, semantic role labeling, information"}
{"pre": "[CLS] we use the stanford chinese word segmenter #refr to tokenize the english pos tagger #otherefr for chinese and chinese", "cit": "[CLS] to improve performance, we apply maximum marginal decoding #refr #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the sentence in", "cit": "[CLS] there has been some work on producing partial parses for utterances for which a full hpsg analysis is not"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] shared", "cit": "[CLS] the charniak-johnson reranking parser #refr, along with a self-trained biomedical parsing model #otherefr, has been used for tokenization, pos-tagging"}
{"pre": "[CLS] the most popular approach to word alignment is the ibm models #otherefr; #refr. [SEP] model 1 and the hmm", "cit": "[CLS] a different approach was presented in #refr, which again concentrated on the combination of two sets of alignments, but"}
{"pre": "[CLS] #refr use a similar approach to identify the polarity of words in a sentence. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] content analysis methods, which rely on patient speech transcripts or texts authored by patients, have been leveraged for understanding"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the words is to the input and the input", "cit": "[CLS] while originally developed to reduce the number of parameters required in n-gram language models, brown clusters have been found"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] unfortunately, global inference and learning for graph-based dependency parsing is typically np-hard #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the translation quality is measured by bleu #refr. [SEP] bleu score #refr with the bleu score of the mt", "cit": "[CLS] unlike #refr who use discriminative training to tune the weight on each feature, ulc uses uniform weights. [SEP] [PAD]"}
{"pre": "[CLS] the system is based on the c&c parser #refr and the collins parser #otherefr. [SEP] shared task on the", "cit": "[CLS] the resulting parser is efficient, constructing a parse forest in roughly quadratic time (empirically), and efficiently returning the ranked"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is not to be useful for the purpose", "cit": "[CLS] his ccl parser has since been improved via a ?zoomed learning? technique (reichart and #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] several such methods have been applied to dependency parsing #otherefr, constituent parsing (sarkar, 11we see in it a milder"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the sentence in", "cit": "[CLS] this representation is inspired by the earlier works #refr and offers a flexibility to accommodate complex inputs. [SEP] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the semantic relations between nominals of noun phrases and the verb classes", "cit": "[CLS] by contrast, we have chosen to represent the metaphoric space using wordnet senses which have been shown in previous"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] previous work while work has been done on various sorts of collocation information that can be obtained from text"}
{"pre": "[CLS] the task of identifying the semantic relations between nominals of noun phrases and their arguments are also key to", "cit": "[CLS] for example, in noun compounds many different semantic relationships are encoded by the same simple form #otherefr: ?dog food?"}
{"pre": "[CLS] the first is the case for the syntactic analysis of the english lexical substitution task of the penn treebank", "cit": "[CLS] to this effect, as mentioned above, in recent years, some approaches have been developed in order to (semi)automatically detect"}
{"pre": "[CLS] the semantic parser used in this paper is the conll shared task on dependency parsing #refr. [SEP] [SEP] [SEP]", "cit": "[CLS] we plan to test how well we are doing by building amrs on top of large, manually-constructed paraphrase networks"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] shared", "cit": "[CLS] the bionlp shared task (st) series has been instrumental in encouraging the development of methods and resources for the"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] in #refr it is shown that syntactic patterns can be used to extract potential definition sentences from wikipedia, and"}
{"pre": "[CLS] the system is based on the moses toolkit #otherefr and the standard minimum error rate training #refr for tuning", "cit": "[CLS] bootstrap resampling was one of the early randomized methods proposed for statistical significance testing of mt #otherefr; #refr, to"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the training data is to", "cit": "[CLS] #refr shows that a word appears with much higher probability when seen elsewhere in an article. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the event descriptions of the bionlp?09 shared task on event extraction #refr.", "cit": "[CLS] this is the phenomenon that is addressed by theories such as focus spaces #otherefr, #refr. *rhetorical structure, which segments"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] model with the average number of the average", "cit": "[CLS] they, in turn, follow #refr and others in labeling only the heads of syntactic chunks rather than all words."}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for parsing #otherefr; #refr for parsing", "cit": "[CLS] table 5 shows the comparative results for english, where y&m2003 refers to the parser of yamada and matsumoto #otherefr,"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the training data is to", "cit": "[CLS] traditionally, parallel corpora have been a mainstay of multilingual parsing #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] the parser of #refr received sentence as input and conducted word segmentation and syntactic parsing at the same time,"}
{"pre": "[CLS] the first is the same as #refr, which we will use the inside-outside algorithm for the inside-outside algorithm for", "cit": "[CLS] approaches like the ones in ha?nig #otherefr and #refr could be combined with the udop approach using this simple"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the decoder that is not a", "cit": "[CLS] for example, the model of #refr uses an itg constraint and beam-based viterbi decoding for tractability, but is still"}
{"pre": "[CLS] the work of #refr is similar to ours in that it is possible to use a more complex linguistic", "cit": "[CLS] #refr define the task of argumentative zoning (az) as a sentence-by-sentence classification with mutually exclusive categories from the annotation"}
{"pre": "[CLS] the second approach is to use mismatches and is the one of the most popular choice of the largest", "cit": "[CLS] the tagger has been trained for tagging english texts with an accuracy of 97% #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] event and semantic orientation", "cit": "[CLS] #refr segmented internet relay chat, clustered segments into subtopics, and identified responding segments of the first segment in each"}
{"pre": "[CLS] the second approach is to use a maximum entropy model #otherefr; #refr. [SEP] model for the probability distribution over", "cit": "[CLS] one line of research uses np bayes methods on whole tree structures, in the form of adaptor grammars #otherefr;"}
{"pre": "[CLS] the first is the case for the parsing of the penn treebank #refr, which we use the berkeley parser", "cit": "[CLS] to determine the attachments of ambiguous coordinate noun phrases, #refr applied a cooccurrence-based probabilistic model, and nakov and hearst"}
{"pre": "[CLS] topic models have been successfully applied to a wide range of nlp tasks including word sense disambiguation #otherefr, and", "cit": "[CLS] in parallel, researchers extensively study the relationship between discourse structure and intonational variation #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] twitter is widely used for conversations #refr, and prior work has looked at twitter for different aspects of conversations"}
{"pre": "[CLS] the first is the task of coreference resolution #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] ? ? ? ? ?", "cit": "[CLS] several uncertainty metrics have been proposed that follow that route to adapt single instance selection to multiple instance scenarios"}
{"pre": "[CLS] the first is the task of converting natural language generation #otherefr; #refr. [SEP] [SEP] [SEP] to be a single", "cit": "[CLS] this algorithm and its many variants are widely used in the computational linguistics community #otherefr; #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a similar to the one used in the sense of the", "cit": "[CLS] we rely on gsearch to provide moderately accurate information about verb frames in the same way that hindle and"}
{"pre": "[CLS] the feature weights were tuned using minimum error rate training (mert) #refr on the development set using minimum error", "cit": "[CLS] finally, the system trains the feature weights for the decoding model using minimum error rate training #refr to maximize"}
{"pre": "[CLS] the second approach is to use a small number of seed examples for example outputs #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] although semi-supervised approaches have been shown to reduce the need for manual annotation #otherefr; #refr, these methods still require"}
{"pre": "[CLS] in the second international chinese word segmentation bakeoff #otherefr; #refr and the state-of-the-art accuracies of the state-of-the-art accuracies of", "cit": "[CLS] recent research on dependency parsing usually overlooks this issue by simply adopting gold pos tags for chinese data #otherefr;"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to learn a translation probabilities from", "cit": "[CLS] similar to #refr, we apply it in both translation directions with separate scaling factors for the three orientation classes,"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for learning and beam-search decoding #otherefr;", "cit": "[CLS] projectivity can be relaxed in some parsers #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr proposed a method for identifying word pairs of words in a target language and phrases in a target", "cit": "[CLS] over the years, several approaches for mining translations from non-parallel corpora have emerged #otherefr; #refr, all sharing the same"}
{"pre": "[CLS] the first is the process of annotating the correct grammatical relations between the words in a sentence and the", "cit": "[CLS] while the model elegantly captures intuitions about parallelism in repairs and modelling fluency, it relies on stringmatching, motivated in"}
{"pre": "[CLS] in the context of machine translation, the machine translation, itg has been explored for a variety of nlp tasks", "cit": "[CLS] there have been several attempts to obtain paraphrases. #otherefr used mutual information of word distribution to calculate the similarity"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] the third major approach involves extrinsic evaluation, where the parser?s output is used in a downstream task, such as"}
{"pre": "[CLS] the first is the case for the parsing of the penn treebank #refr, which we use the berkeley parser", "cit": "[CLS] lexicalized statistical parsing models, such as those built by #refra), magerman #otherefr, have been enormously successful, but they also"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] there are many ways of incorporating syntax into mt systems, including the use of string-to-tree translation (s2t) to ensure"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] of a text with", "cit": "[CLS] there has been successful and fruitful effort by researchers in the nlp community to share their experiences and course"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr. [SEP]", "cit": "[CLS] dataset we use the dataset provided by the ?affective text? shared task in semeval-2007 #refr, which is composed of"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr.", "cit": "[CLS] several approaches have recently been proposed in this context for the semantic role labeling task. #otherefr, #refrb) and #otherefra)."}
{"pre": "[CLS] in the context of discourse relations are used to identify the discourse relations and discourse relations and discourse relations", "cit": "[CLS] while there has been some prior work on detecting agreements and disagreements in multiparty discussions #refr, which is related"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] finding answers with term matching and partial tree matching has been used in the literature of question answering #otherefr;"}
{"pre": "[CLS] the feature weights were tuned using minimum error rate training (mert) #refr on the development set using minimum error", "cit": "[CLS] following #refr, we propose an original method for oracle decoding based on lagrangian relaxation. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second approach is to use a small number of manually annotated corpora such as the same data set", "cit": "[CLS] to the best of our knowledge, this is the first tagging study that reaches a 98% accuracy level for"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] in recent times, there has been a huge interest to mine and understand the opinions and sentiments that people"}
{"pre": "[CLS] the translation quality is measured by bleu #refr. [SEP] score #refr with the bleu score of the translation quality.", "cit": "[CLS] the transliteration systems were modeled using the minimum error rate training procedure introduced by #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford ner tagger #refr to train the feature weights. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] extending #refr and finkel andmanning #otherefr?s work, we exploit loose transitivity constraints on coreference pairs. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to identify stances in which a coherent discourse. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] recently, topic modeling approaches have been investigated and achieved state-of-the-art results in multi-document summarization #refr; celiky- 1these drdas are"}
{"pre": "[CLS] the first is the one of the most popular method for building word sense disambiguation in the wsd task", "cit": "[CLS] 1due to space limits, we unfortunately cannot present the statistics of the training and test data, such as the"}
{"pre": "[CLS] the first is the task of identifying the sentence planning problem that the user is a text generation of", "cit": "[CLS] however, except for a few systems that use dialog to address complex questions #otherefr; #refr, the general dialog capabilities"}
{"pre": "[CLS] in this paper, we present a state-of-the-art dependency parsing algorithm for dependency parsing #refr. [SEP] parsing is the one", "cit": "[CLS] in previous work on discriminative natural language parsing, one approach has been to define features centered around lexicalized local"}
{"pre": "[CLS] #refr proposed a joint model for unsupervised learning of selectional preferences based on a target language and unlabeled data", "cit": "[CLS] we show that using adaptive na??ve bayes improves on state of the art classification using the bitter lemons corpus"}
{"pre": "[CLS] in the second international chinese word segmentation bakeoff #otherefr; #refr and the state-of-the-art accuracies of the state-of-the-art accuracies of", "cit": "[CLS] recent research on dependency parsing usually overlooks this issue by simply adopting gold pos tags for chinese data #otherefrb;"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] an annotation task consists of a set of items, each of which is associated with a set of possible"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] its original pos tag set is very coarse and the pos and the word stem information is not very"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within the source sentence into a target sentence into", "cit": "[CLS] to explore this, we tested our model in conjunction with a recent l2p system that has been shown to"}
{"pre": "[CLS] the first is the polarity of a document #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] , s ?", "cit": "[CLS] #refr). [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the system is based on the inprotk toolkit for instance in the standard implementation of #refr. [SEP] bleu score", "cit": "[CLS] this help is provided by predictive parsing, a technique recently developed for gf #refr8. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in this paper, we propose a discriminative model for dependency parsing by #refr which is used to extract the", "cit": "[CLS] in that paper, qg was applied to word alignment and has since found applications in question answering #otherefr, and"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the training data is to", "cit": "[CLS] hence, state-of-the-art parsers either supplement the part-of-speech #otherefr; #refr, manually split the tagset into a finer-grained one #otherefr. [SEP]"}
{"pre": "[CLS] the model is trained using the averaged perceptron algorithm #refr. [SEP] score #refr with the best results for the", "cit": "[CLS] it is used for word segmentation #otherefr, semantical parsing #refr and other nlp tasks. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of semantic textual similarity (sts) task #refr is a similar to the similarity of the similarity of", "cit": "[CLS] in this work, we built on previous research and our submission to semeval?2012 #refr to create a sentence-level sts"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be np-hard #refr. [SEP] is", "cit": "[CLS] our n-best parser uses an efficient algorithm developed originally by #otherefr, and subsequently improved by #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] some examples include text categorization #otherefr, part-of-speech tagging #refr, word-sense disambiguation #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] inspired by rst, #refr proposed the cross-document structure theory (cst) for multidocument analysis, such as multi-document summarization, and topic"}
{"pre": "[CLS] the tempeval community focused on the classification of the temporal relations between events and their arguments and their arguments", "cit": "[CLS] it also has the highest overall relaxed f1, slightly higher than heidel- time (stro?tgen and #refr (cleartk had the"}
{"pre": "[CLS] the semantic role labeling task is the process of annotating the predicate-argument structure in text with se- ?this research", "cit": "[CLS] a tree kernel #refr is used to exploit the deep syntactic processing obtained using the charniak parser #otherefr. [SEP]"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] sentence and thus a", "cit": "[CLS] in general english, #refr note that acronym disambiguation is not widely studied because acronyms are not as prevalent in"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be np-hard #refr. [SEP] s", "cit": "[CLS] we implemented our confusion forest system combination using an in-house developed hypergraph-based toolkit cicada which is motivated by generic"}
{"pre": "[CLS] the first is the task of identifying the semantic textual similarity of a word is a word sense disambiguation", "cit": "[CLS] a companion paper introduces the task, presents an unsupervised isd model, drawing on web page text and image features,"}
{"pre": "[CLS] we use the averaged perceptron algorithm #refr to train the parameters of the weights for the weights for the", "cit": "[CLS] word alignment is a crucial early step in the training of most statistical machine translation (smt) systems, in which"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a similar to the one used in the sense of the", "cit": "[CLS] consequently, the integration of information from distinct mrd sources through simple word-sense matches is likely to fail in a"}
{"pre": "[CLS] we use the stanford named entity recognizer #refr to extract named entities from the text to identify named entities", "cit": "[CLS] for this task, normally supervised methods are used #otherefr; #refr, which require sufficient labeled training data. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] in the context of machine translation, itg has been explored for statistical word sense disambiguation #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] in recent years qe gained increasing interest in the mt community, resulting in several datasets available for training and"}
{"pre": "[CLS] #refr use a probabilistic model to model the meaning of the meaning of a sentence length of the probability", "cit": "[CLS] #refr treat content selection as an optimization problem. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] furthermore, wasp ?1 ++ employs minimum error rate training #refr to directly optimize the evaluation metrics. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to word alignment is the hmm alignment model #refr. [SEP] model is based on the", "cit": "[CLS] similarly, #refr cast word alignment as a maximum weighted matching problem and propose a framework for learning word pair"}
{"pre": "[CLS] the semantic similarity between two sentences is computed using the similarity measure proposed by #refr. [SEP] score computed using", "cit": "[CLS] the syntactic metric computes bleu #refr, a machine translation evaluation metric, over a labels of basephrases (chunks). [SEP] [PAD]"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] recently, the negation and speculation in nlp workshop #otherefr shared task #refr targeted negation mostly on those subfields. [SEP]"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] either pruning #otherefr or lossy randomizing approaches #refr may result in a compact representation for the application run-time. [SEP]"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with the one of", "cit": "[CLS] there has been some recent work on dependency length minimization in natural language sentences #refr, but the relationship between"}
{"pre": "[CLS] the first is the task of converting the extraction task of a sentence, from the bionlp?09 shared task on", "cit": "[CLS] in order to obtain ccg derivations for all sentences in the ace corpus, we used the ccg parser introduced"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] other methods tackle this problem in languagedependent ways #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for dependency parsing #refr. [SEP] grammars", "cit": "[CLS] with the use of rich non-local features, transition-based dependency parsers achieve state-of-the-art accuracies that are comparable to the best-graph-based"}
{"pre": "[CLS] the second approach is to use a small number of seed examples for example outputs #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] although semi-supervised approaches have been shown to reduce the need for manual annotation #refr, these methods still require a"}
{"pre": "[CLS] the system is based on the moses toolkit #otherefr and the moses toolkit #refr. [SEP] decoder with the best", "cit": "[CLS] we compare our scores against the cmu-uka isl phrase-based submission, a state-of-the art phrase-based smt system with part-of-speech (pos)"}
{"pre": "[CLS] the wsd system used for wsd in the semeval 2013 task #refr was the semeval 2013 task #otherefr. [SEP]", "cit": "[CLS] several researchers subsequently continued and improved this line of work#refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a graph to identify the polarity of words and their polarity of words in a sentence. [SEP]", "cit": "[CLS] therefore, we adapt a standard machine learning-based approach to np coreference resolution #otherefr; ng and #refr for our purposes."}
{"pre": "[CLS] the feature weights were tuned using mert #refr on the development set. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] a variety of approximate decoding algorithms have been explored for syntax-based translation systems, including cube-pruning #otherefr; #refr, left-to-right decoding"}
{"pre": "[CLS] the second approach is to use a word clusters as described in #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] however, #refr suggested that annotators often gave high ratings to more than one wordnet sense for the same occurrence."}
{"pre": "[CLS] the unsupervised pos induction task has been extensively studied in the nlp community, and has been studied in the", "cit": "[CLS] previous work in domain adaptation can be classified into two categories: [s+t+], where a small, labeled target domain data"}
{"pre": "[CLS] the model is trained with a maximum entropy model #otherefr, which is trained on the penn treebank grammar #refr.", "cit": "[CLS] recent work that incorporated dirichlet process #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the semantic role labeling task of syntactic structure from the conll shared task", "cit": "[CLS] the choice of rmrs also ensures that the semantic bank can be used for comparative evaluation of hpsg grammars"}
{"pre": "[CLS] the grammar is a grammar that is a grammar that is a grammar that is a grammar that is", "cit": "[CLS] in particular, we developed an architecture inspired by the earley deduction work of #refr but which generalized that work"}
{"pre": "[CLS] the first is the automatic evaluation of discourse structure of discourse structure of discourse structure of discourse structure of", "cit": "[CLS] we also annotate content slots, including general classes automatically created by named entity recognition (ner) #refr and hand-crafted topic"}
{"pre": "[CLS] the second approach is to use a word clusters as described in #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] in several recent proposals #otherefr; #refr, statistical and machine learning techniques were used to extract classifiers from hand-tagged corpus."}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a text that is a", "cit": "[CLS] examples of the former take on classification include #refr; girju, 2007; o. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the case of the syntactic mt system described in #refr. [SEP] model is", "cit": "[CLS] ckk uses the dubey and keller #otherefr parser, which is trained on the negra corpus #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] #refr proposed a method for identifying the polarity of a word in a target word in a target word", "cit": "[CLS] pang et al proposed a method of classifying movie reviews into positive and negative ones #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the first is the process of annotating the correct parse trees of the sentence in the sentence is sentence", "cit": "[CLS] numerous pos taggers exist, although we use the stanford tagger here #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to translation lexicons from parallel corpora #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] machine transliteration methods are divided into grapheme-based #otherefr, phoneme-based #refr and combined techniques #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the grammar is a synchronous grammar formalism which is a synchronous grammar (scfg) used in grammar formalisms that is", "cit": "[CLS] rank reduction algorithms for stringbased translation devices have also been discussed by zhang et al #otherefr and #refr. [SEP]"}
{"pre": "[CLS] in recent years, there has been a growing interest in the task of generating referring expressions, reflected in the", "cit": "[CLS] more generally, the nlg problem of non-deterministic decision making has been addressed from many different angles, including penman-style choosers"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to learn a translation models from a", "cit": "[CLS] in contrast to previous attempts at using paraphrases to improve statistical machine translation, which require external data in the"}
{"pre": "[CLS] the semantic relation between nominals in the verb frame acquisition and the semantic role labeling task #refr are not", "cit": "[CLS] reverb #refr is a web-scale information extraction system that automatically acquires binary relations from text. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr proposed a method for sentiment classification that is a sentiment classification of words and thus not a given", "cit": "[CLS] continuous representation of words and phrases are proven effective in many nlp tasks #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to word alignment is the ibm models #otherefr; #refr. [SEP] model 4 alignments #otherefr; #refr.", "cit": "[CLS] unfortunately, as was shown by fraser and marcu #otherefr aer can have weak correlation with translation performance as measured"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal, parser #refr and tested on the penn treebank", "cit": "[CLS] it allows us to define supertags on different levels of granularity #otherefr; #refr, thus facilitating a fine grained analysis"}
{"pre": "[CLS] #refr use a similar approach to compute similarity between two words in a sentence. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] also, they can be used directly for testing paraphrase applicability #refr, a task that has recently become prominent in"}
{"pre": "[CLS] the weights of the log-linear model are optimized with minimum error rate training (mert) #refr on the development set", "cit": "[CLS] empirically, we observe a bleu score improvement of 1.2 over the best unsupervised baseline and 0.8 over the block"}
{"pre": "[CLS] the system uses a combination of natural language processing (nlp) systems to produce various types of syntactic and semantic", "cit": "[CLS] in fact, more and more systems adopt architectures in which the semantics of the questions are captured prior to"}
{"pre": "[CLS] the first is the automatic metric of text with human annotation of text segments as described in #refr. [SEP]", "cit": "[CLS] morris and hirst #otherefr and #refr find topic boundaries in the texts by using lexical cohesion. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a supervised approach to identify the noun phrases. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] we compare this approach with three others: a heuristic rule-based approach #otherefr, the snow (sparse network of winnows) system?s"}
{"pre": "[CLS] in the second international chinese word segmentation bakeoff #otherefr; #refr and the state-of-the-art accuracies of the state-of-the-art accuracies of", "cit": "[CLS] an appealing alternative to a similar approximation to the above would use loopy belief propagation to efficiently approximate the"}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders and predict the polarity of words in a sentence and", "cit": "[CLS] it uses named entity recognition #otherefr; #refr as a subroutine to identify named person entities, though we are also"}
{"pre": "[CLS] the second approach is to use a word lattice as a word alignment model that is based on the", "cit": "[CLS] the wsd system used here is based on the model that achieved the best performance on the senseval-3 chinese"}
{"pre": "[CLS] the first is the task of converting natural language processing #otherefr; #refr. [SEP] [SEP] [SEP] to be a number", "cit": "[CLS] in doing so, we provide first results on the application to french parsing of wordnet automatic sense ranking (asr),"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a task of identifying the degree of semantic relations between words", "cit": "[CLS] recent attempts in the area of nc interpretation have taken two basic approaches: analogy-base interpretation #otherefr and semantic disambiguation"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to learn a translation probabilities from", "cit": "[CLS] several papers have discussed the first issue, especially the problem of word alignments for bilingual corpora #otherefr, #refr. [SEP]"}
{"pre": "[CLS] the model is trained with the averaged perceptron algorithm #refr. [SEP] model with the entire sequence model with the", "cit": "[CLS] as in recent work on using supertagging in parsing, the hypertagger operates in a multitagging paradigm #refr, where a"}
{"pre": "[CLS] #refr use a similar approach to identify the polarity of words in a sentence. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] furthermore, it is considerable simpler and obtains better and easier to reproduce results than previous automatic approaches #otherefr; #refr."}
{"pre": "[CLS] the most common approach to parsing is to use a parse tree #otherefr; #refr. [SEP] is to be learned", "cit": "[CLS] among them, the best parsers are the svm-based dependency analyzers #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to nlg is the task of identifying the one of a sentence in a sentence", "cit": "[CLS] for the past decade statistical approaches to dialog modeling have shown positive results for optimizing a dialog strategy with"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the sentence in", "cit": "[CLS] several discoursebased summarization methods have been developed, such as #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] therefore, domain adaptation methods have recently been proposed in several nlp areas, e.g., word sense disambiguation #otherefr, and lexicalized-grammar"}
{"pre": "[CLS] the first is the annotation tool which was the annotation tool for the annotation tool #refr. [SEP] project at", "cit": "[CLS] esg often provides more than one parse, ranked according to a specific numerical ranking system #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] #refr proposed a method for learning a word sense disambiguation which is a word sense disambiguation method to extract", "cit": "[CLS] gamon #otherefr back off words in n-grams (and semantic relations) to their respective pos tags (e.g., great-movie to adjective-noun);"}
{"pre": "[CLS] in this paper, we propose a method for extracting parallel bilingual news articles from monolingual news articles from the", "cit": "[CLS] we also present the 1ways to extend n-gram measures to account for paraphrases have been proposed #otherefr; #refr, but"}
{"pre": "[CLS] the grammar is a grammar that is a given an input string, is a text that the input string,", "cit": "[CLS] our task, then, is to make this inferential structure 2see #refr for some promising initial work in applying statistical"}
{"pre": "[CLS] the second approach is to use a maximum entropy model #otherefr; #refr. [SEP] model for the probability distribution over", "cit": "[CLS] the lexicalized pcfg that sits behind model 2 of #refr has rules of the form p ~ lnln-i\"'\" l"}
{"pre": "[CLS] we use the berkeley parser #refr to train the feature weights for each model with a 5-gram language model", "cit": "[CLS] it encodes semantic relations directly, and has the best inter-lingual phrasal cohesion properties #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] the", "cit": "[CLS] we obtain all such grammatical information by first preprocessing the input file using the alembic workbench tagger, lemmatizer, and"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] recently, many researchers have shown that replacing 1-best tokenizations with lattices improves translation performance significantly #otherefr; #refr. [SEP] [PAD]"}
{"pre": "[CLS] the work of #refr is similar to ours in that it is possible to use a more complex model", "cit": "[CLS] these methods are the information retrieval inspired precision-recall #otherefr; #refr and krippendorff?s ?, a variant of the ? family"}
{"pre": "[CLS] we use the averaged perceptron algorithm #refr to train the parameters of the averaged perceptron algorithm #otherefr. [SEP] [SEP]", "cit": "[CLS] in nlp, #refr minimized risk using k-best lists to define the distribution over output structures. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders and opinion holders in opinion holders in opinion holders in", "cit": "[CLS] with the rapid proliferation of microblogging, there is growing amount of emotion analysis research on newly available datasets of"}
{"pre": "[CLS] the conll shared tasks on dependency parsing in 2006 and 2007 #refr have been the task on dependency parsing", "cit": "[CLS] use of global features for structured prediction problem has been explored by several nlp applications such as sequential labeling"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] shared", "cit": "[CLS] as future work, we will reconsider the architecture of the neural network and we will refocus on creating a"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] sentence pair of a", "cit": "[CLS] there also has been extensive work on modeling conversational interactions on twitter #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to machine translation is to use a log-linear combination of statistical machine translation #otherefr; #refr.", "cit": "[CLS] our approach can be regarded as converse to the more common way of using an smt system to automatically"}
{"pre": "[CLS] the most common approach to selectional preference acquisition is to use a similarity measure #otherefr; #refr. [SEP] model to", "cit": "[CLS] our scheme can be applied on top of any context-insensitive ?base? similarity measure for rule learning, which operates at"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for english sentences. [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] svm classifiers can handle very large feature spaces, and produce state-of-the-art results for nlp applications #otherefr; #refr). [SEP] [PAD]"}
{"pre": "[CLS] in the context of machine translation, semantic role labeling has been explored by many researchers #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] #refr the pioneering paper in the field used a small number of hand selected patterns to extract instances of"}
{"pre": "[CLS] the first is the case for the automatic acquisition of subcategorization frames from the penn treebank #otherefr; #refr. [SEP]", "cit": "[CLS] the attachments (e.g. of the pp) will be postponed to the module of resolution of nlp problems, which could"}
{"pre": "[CLS] the first is the case for the word alignment of word alignment of word alignment motivated by the desire", "cit": "[CLS] evidence for this difficulty is the fact that there has been very little work investigating the use of such"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] to score word-to-word links, we use the posterior predictions of a jointly trained hmm alignment model #refr. [SEP] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the sentence in", "cit": "[CLS] for example, the drama scheme #otherefr; #refr include labels for features such as bridging relation type and np type"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to learn a phrase pairs of", "cit": "[CLS] hierarchical mt systems #refr construct a syntactic hierarchy during decoding, which is independent of linguistic categories. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the interpretation of quantifiers and computational linguistics is a growing interest in natural language generation (nlg) in natural language", "cit": "[CLS] #refra) has convincingly argued that this problem arises because dsp do not distinguish between merely co-referential nd co-indexed (in"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model trained on the target language model #refr", "cit": "[CLS] it handles scfgs of the kind extracted by hiero #otherefr) and closely related formalisms like synchronous tree substitution grammars"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] prosodic information such as pause length, duration of words and phones, pitch contours, energy contours, and their normalized values"}
{"pre": "[CLS] the second dataset is the use of a word sense disambiguation algorithm which is based on the most frequent", "cit": "[CLS] the system developed by #refr uses two cascaded memory-based classifiers, combined with the use of a genetic algorithm for"}
{"pre": "[CLS] the most common approach to semantic role labeling is to use syntactic information #otherefr; #refr. [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] for the constituent-based models, constituent information was obtained from the output of #refr for english and dubey?s parser #otherefr"}
{"pre": "[CLS] the translation quality is measured by bleu #refr. [SEP] score #refr on the dev test set. [SEP] [SEP] [SEP]", "cit": "[CLS] for example, hierarchical #refr and syntax-based #otherefr systems have recently improved in both accuracy and scalability. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the first is the process of annotating the correct parse trees of the sentence in the sentence is sentence", "cit": "[CLS] a sel)arate paper #refr looks in detail at the model is performance for particular categories of mori)hology, in particular,"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] we will also utilize a state of the art pcfg-la parser #otherefr; #refr to examine the effect of prosodic"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] shared", "cit": "[CLS] our participation in the genia event extraction task of bionlp 2013 #refr was motivated by the desire of testing"}
{"pre": "[CLS] we use the minimum error rate training #refr to tune the feature weights for the feature weights of the", "cit": "[CLS] for years, the standard way to do statistical machine translation parameter tuning has been to use minimum error-rate training,"}
{"pre": "[CLS] the most popular approach to machine translation is the most popular approach of the one proposed by #refr. [SEP]", "cit": "[CLS] #refr show that unsupervised methods for learning da tokenization can outperform msa tokenizers on mt from levantine arabic to"}
{"pre": "[CLS] the first is the case for the word alignment of word alignment of word alignment of word alignment of", "cit": "[CLS] the joint prior probability, p (w,l), can be decomposed into: p (w,l) = p (l|w)p (w) (3) where p"}
{"pre": "[CLS] the system used for the experiments in this paper is based on the senseval-3 english lexical substitution task of", "cit": "[CLS] the features f25 to f67 correspond to the set of 43 part-of-speech tags of the nltk english pos tagger"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] score is a variant of the best performing", "cit": "[CLS] #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] of a text with", "cit": "[CLS] whereas work on emotion classification from the point of view of natural speech and humancomputer dialogues is fairly extensive,"}
{"pre": "[CLS] the model is trained with the averaged perceptron algorithm #refr. [SEP] score = ? ? ? ? ? ?", "cit": "[CLS] all scores 2ftp://ftp.cs.brown.edu/pub/nlparser/ 3http://nlp.stanford.edu/software/lex-parser.shtml system f1#otherefr 80.85 stanford parser 78.47 26.44 berkeley parser 83.22 31.32 #refr 84.24 combination zhang"}
{"pre": "[CLS] in the context of part-of-speech tagging, chunking, #refr proposed a joint model for unsupervised learning of pos tagging and", "cit": "[CLS] since sentence structure is underdetermined by raw text, there have been efforts to simplify the task, via #otherefr; #refr;"}
{"pre": "[CLS] the first group is a user to use dialogue acts to be able to help users to help users", "cit": "[CLS] furthermore, methods have also been proposed to change the dialogue initiative based on various cues #otherefr; #refr. [SEP] [PAD]"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] sentences were tokenized and part-of-speech tagged with cmu twitter nlp tool #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the grammar is a grammar that is the grammar of the grammar of the grammar of the grammar of", "cit": "[CLS] tfs #refr also offered type constraints and relations and to our knowledge was the first working typed feature systems."}
{"pre": "[CLS] we use the averaged perceptron algorithm #refr to train the weights for the weights for each training and the", "cit": "[CLS] goldwater and griffiths #otherefr and #refr show that modifying an hmm to include a sparse prior over its parameters"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the sentence in", "cit": "[CLS] community question answering (cqa) is the task of identifying question?answer pairs in a given thread, e.g. for the purposes"}
{"pre": "[CLS] the second dataset is the same as the use of the same as the same as the same as", "cit": "[CLS] our method is based heavily on the wsi methodology proposed by #refr for novel word sense detection. [SEP] [PAD]"}
{"pre": "[CLS] the most popular approach to word alignment is the ibm models #otherefr; #refr. [SEP] model 1 and the hmm", "cit": "[CLS] setup we tested these features on two machine translation systems: a hierarchical phrasebased (string-to-string) system #refr and a syntax-based"}
{"pre": "[CLS] the second approach is to use a word sense disambiguation algorithm which is based on the most similar to", "cit": "[CLS] the next significant hand tagging task was reported in #refr, where 2,476 usages of interest were manually assigned with"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] semisupervised learning from partial annotations may be sufficient to learn complete parsers #refr. . [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained with a maximum entropy model #otherefr, which is trained on the penn treebank wall street", "cit": "[CLS] implementations of left-corner parsers such as that of #refr adopt a arc-standard strategy, essentially always choosing analysis (b) above,"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the penn treebank #otherefr. [SEP] s", "cit": "[CLS] most existing computational natural language acquisition models also assumed various kinds of the extra input (e.g. semantic associations \\[#refr;"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be a difficult problem #otherefr;", "cit": "[CLS] several types of errors can be fixed by employing rule-based post-editing #refrb), which can be seen as being orthogonal"}
{"pre": "[CLS] the system used in this paper is based on the stanford ner tagger #refr. [SEP] parser #otherefr shared task", "cit": "[CLS] in ubiu, we use a pairwise mention model #otherefr; #refr since this model has proven more robust towards multiple"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be a difficult problem #otherefr;", "cit": "[CLS] data from the shared task of the eacl 2009 workshop on statistical machine translation: www.statmt.org/wmt09 use an english corpus"}
{"pre": "[CLS] the first is the case for the generation of the generation of referring expressions from the linguistic phenomena of", "cit": "[CLS] in all annotation studies we are aware of,1 the fact that an expression may not have a unique interpretation"}
{"pre": "[CLS] the second dataset is the same as the use of the same as the same as the same as", "cit": "[CLS] a variety of synset similarity measures based on properties of wordnet itself have been proposed; nine such measures are"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal (wsj) corpus #refr. [SEP] trees and the penn", "cit": "[CLS] maltparser #refr is a languageindependent system for data-driven dependency parsing which is freely available.7 it is based on a"}
{"pre": "[CLS] the second dataset is the same as the use of the same as the same as the same as", "cit": "[CLS] there have been previous works on portability of hand-tagged corpora that show how some constraints, like the genre or"}
{"pre": "[CLS] in the case of dependency parsing, previous work has shown that it is possible to improve the performance of", "cit": "[CLS] moreover, an indirect comparison indicates that our ap- 2ctb5 is converted to dependency structures following the standard practice of"}
{"pre": "[CLS] the task of identifying the overall sentence pairs in a sentence is extracted from the target sentence using the", "cit": "[CLS] on the other hand, event as a high-level construct has proved useful in mds content selection #refr. [SEP] [PAD]"}
{"pre": "[CLS] the system is based on the inprotk toolkit for evaluating mt evaluation of #refr, which is a machine translation", "cit": "[CLS] current automatic metrics of machine translation, such as bleu #otherefr and ter #refrb), which have greatly accelerated progress in"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] the model weights ?i are usually discriminatively learned on a development data set via minimum error rate training (mert)"}
{"pre": "[CLS] the first is the case for the generation of referring expressions from the air travel information system #refr, a", "cit": "[CLS] subsequent work explored ways of exploiting linguistically annotated data for trainable generation models #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] a key idea in this paper, following a pilot study in #refr, is to perform manual annotation only at"}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders and opinion holders in opinion holders in opinion holders and", "cit": "[CLS] previous research has shown that in general the performance of the former tend to be superior to that of"}
{"pre": "[CLS] the first is the case for the syntactic analysis of the english genitive, the penn treebank #refr, which we", "cit": "[CLS] binators in our runtime system #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for parsing #otherefr; #refr for parsing", "cit": "[CLS] datadriven dependency parsers such as those by nivre et al#otherefr, #refr, titov and henderson #otherefr are accurate and efficient,"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] of a sentence, and", "cit": "[CLS] the basis for our evaluation follows #refr and guevara #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders and predict the polarity of words in a sentence and", "cit": "[CLS] twitter is a fantastic data resource for many tasks: measuring political #otherefr, studying linguistic variation #refr and detecting earthquakes"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within the meaning representation of the meaning of the", "cit": "[CLS] although bi-alignments are known to exhibit high precision #refr, in the face of sparse annotations we use unidirectional alignments"}
{"pre": "[CLS] the first is the case for the case of the grammar formalism used by #refr, which is the most", "cit": "[CLS] in order to realize, automatic ustomizatiou of existing linguistic knowledge to each applicat;ion domain, we proposed a new approach"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] a very good candidate seems to be the intersective levin classes #refr that can be found as well in"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to learn a translation probabilities from", "cit": "[CLS] figure 1: 2-scfg systems such as hiero are unable to independently generate translation units a, b, c, and d"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] to learn the classifiers for each translation task, the training set and development set were put together to obtain"}
{"pre": "[CLS] in this paper, we propose a method for learning semantic parsing by using a large number of features from", "cit": "[CLS] previous work on syntax projection mostly focuses on unsupervised grammar induction where no labeled data exists for target language"}
{"pre": "[CLS] the first is the case for the semantic role labeling of the predicate-argument structure of the predicate-argument structure of", "cit": "[CLS] in particular, each of/hayes et al 1988/,/#refr/, and/rau and jitcobs 1988/ describes ystems that characterize news reports with results"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] is a growing number of", "cit": "[CLS] possible features include goodness of fit relative to pre-computed preferences #refr, named entities #otherefr, or broad ontological classes like"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of meaning that are not", "cit": "[CLS] generally, wsd methods use the context of a word for its sense disambiguation, and the context information can come"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for parsing #refr. [SEP] grammars with", "cit": "[CLS] the graph-based approach employs #refr?s algorithm where spurious ambiguities are eliminated by the notion of split head automaton grammars"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] more recent compositional approaches to sentiment analysis can outperform lexicon and ngrambased methods (e.g., #refr, socher et al. #otherefr)."}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with the search algorithm", "cit": "[CLS] a look at the performance sheet in the contest shows that two systems with quite different approaches #otherefr; #refr."}
{"pre": "[CLS] the first is the one of the most popular approach to use a word in which is to use", "cit": "[CLS] to reduce this effect, attempts have been made to adapt nlp tools to microblog data #otherefrb; #refr. [SEP] [PAD]"}
{"pre": "[CLS] the parser is trained on the penn treebank corpus and tested on the penn treebank corpus #refr. [SEP] test", "cit": "[CLS] the corpus was split into training and test data as described in #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] later research has focused on extending the document representation with more complex features such as structural or syntactic information"}
{"pre": "[CLS] the most common approach to paraphrase generation is to use a lexicon to paraphrase database into a target language", "cit": "[CLS] automatic paraphrasing has been recognized as an important component for nlp systems, and many methods have been proposed to"}
{"pre": "[CLS] the first is the annotation tool which was the annotation tool for the annotation tool #refr. [SEP] project in", "cit": "[CLS] since the standard training and test set for english parsing is a phrase structure #otherefr; #refr, the usual approach"}
{"pre": "[CLS] the feature weights were tuned using mert #refr on the development set. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] it is worth noting that the german parse trees #refr tend to be broader and shallower than those for"}
{"pre": "[CLS] id institution balagur yandex school of data analysis #otherefr commercial-1,2,3 anonymized commercial systems online-a,b,g anonymized commercial systems online-a,b,g anonymized", "cit": "[CLS] we obtained large numbers of rankings from two groups: researchers (who 1the traditional metrics task is evaluated in a"}
{"pre": "[CLS] the most popular approach to paraphrase generation has been the source sentences into a target sentence into a source", "cit": "[CLS] significant progress has also been made in paraphrase extraction, where most recent methods produce large numbers of paraphrasing rules"}
{"pre": "[CLS] we use the stanford ner tagger #refr to train the training data with the feature weights. [SEP] [SEP] [SEP]", "cit": "[CLS] dataset for the sentiment classification task, we use the dataset provided in #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with the one of", "cit": "[CLS] in latent variable parsing #otherefr; #refr, we learn rule probabilities on latent annotations that, when marginalized out, maximize the"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the sentence in", "cit": "[CLS] there are cleverer ways to reduce the complexity (e.g., see #refr for three such ways). [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] to learn the classifiers for each translation task, the training set and development set were put together to obtain"}
{"pre": "[CLS] the work of #refr is similar to ours in that it is possible to use a more complex model", "cit": "[CLS] in recent years, there have been many studies on discourse parsing on the basis of the above handannotated corpora"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] we tune the feature weights for every configuration with 10 rounds of hypergraph-based minimum error rate training (mert) #refr."}
{"pre": "[CLS] the first is the process of annotating the word segmentation and then the word segmentation bakeoff #refr. [SEP] task", "cit": "[CLS] in this manner, previous studies, e.g. #otherefr; #refr, obtained a level of inter-annotator agreement that is statistically significant. [SEP]"}
{"pre": "[CLS] the learning algorithm used is a variation of the winnow update rule incorporated in snow #otherefr; #refr, a multi-class", "cit": "[CLS] to the best of our knowledge, this combination represents the current state-of-the-art for semantic role labelling following the prop-"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a semantic role labelling task", "cit": "[CLS] this research is interesting, since lexical classifications, when tailored to the application and domain in question, can provide an"}
{"pre": "[CLS] the model is trained using the averaged perceptron algorithm #refr. [SEP] score #refr with the best results for the", "cit": "[CLS] in each case the input to the network is a sequence of tag-word pairs.5 5we used a publicly available"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best derivation of the best", "cit": "[CLS] we aim to allow the estimation of large scale distributed models, similar in size to the ones in #refr."}
{"pre": "[CLS] the model is trained using the moses decoder #refr. [SEP] score #refr with the final translation quality measured by", "cit": "[CLS] a related but different approach is to enrich the source language items with grammatical features #otherefr; #refr. [SEP] [PAD]"}
{"pre": "[CLS] dependency parsing has been intensively studied in recent years #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] during the testing phase, we used automatically assigned pos and chunk tags by tsuruoka?s tagger4#otherefr and yamcha chunker5#refr. [SEP]"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within the meaning representation of the meaning of the", "cit": "[CLS] #refr examined wikipedia simple articles looking for features that characterize a simple text, with the hope of informing research"}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders and opinion holders in opinion holders and opinion holders in", "cit": "[CLS] #refr, dave et al #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] recent work has explored reasoning about sets using packed representations such as lattices and hypergraphs #otherefr; #refr, or sampling"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] the alternative we propose is primarily motivated by the research on annotation projection #otherefr and direct transfer #refr. [SEP]"}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders and opinion holders in opinion holders in opinion holders in", "cit": "[CLS] #refr present a supervised method for stance classification. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] in the longer term, we hope to compare different types of parsers in both the preposition selection and error"}
{"pre": "[CLS] the most popular approach to word alignment is the ibm models #otherefr; #refr. [SEP] model 1 and the hmm", "cit": "[CLS] some approaches use pivot languages #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the system is based on the inprotk toolkit for the best performing system described in #refr. [SEP] system, which", "cit": "[CLS] pattern-based approaches to extract hyponym/hypernym relationships range from hand-crafted lexico-syntactic patterns #refr to the automatic discovery of such patterns"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to learn a translation probabilities from", "cit": "[CLS] since this trade-off is also affected by the settings of various pruning parameters, we compared decoding time and translation"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr. [SEP]", "cit": "[CLS] as per #refr, each layer?s chunk bracketing of cascaded markov models is dependent because the output of a lower"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] another line of related work is unsupervised semantic parsing or semantic role labeling #otherefrb; #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the syntactic annotation of the penn treebank #otherefr; #refr, which we use the", "cit": "[CLS] the wildcards generation started to be refined with #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of paraphrase extraction is similar to the one described by #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] cross-lingual textual entailment #otherefr; #refr as an extension of the textual entailment task #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the one of the most popular approach to use crowdsourcing, which has been to use a", "cit": "[CLS] bulgarian computational morphology has developed as the result of local #otherefr and international activities for the compilation of sets"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with a probability distribution", "cit": "[CLS] the feature sets of the completion model described above are mostly based on previous work #otherefr; #refr. [SEP] [PAD]"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a lexicon to measure translation #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] there are several sentiment analysis approaches that make use of manually annotated review datasets #otherefr; #refr and wei and"}
{"pre": "[CLS] #refr use a similar approach to identify the polarity of words in a sentence. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] #refr showed that, when considering full documents, good performance on just european languages does not necessarily imply equally good"}
{"pre": "[CLS] the most common approach to this problem is to use a lexicon to model or syntactic structure #otherefr; #refr.", "cit": "[CLS] the knowledge about the likelihood of external causation might be helpful in the task of detecting implicit arguments of"}
{"pre": "[CLS] the english side of the parallel corpus was parsed using the berkeley parser #refr trained on the penn treebank", "cit": "[CLS] we report the translation quality using the case-insensitive bleu-4 metric #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of meaning that are not", "cit": "[CLS] this omission is also done in applications such as word sense dismnbiguation #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders and opinion holders in opinion holders in opinion holders in", "cit": "[CLS] a recent paper by #refr examines agreement and disagreement in quote/response pairs in idealogical and nonidealogical online forum discussions,"}
{"pre": "[CLS] id participant cmu carnegie mellon university #otherefr cu-bojar charles university - bojar #refr cu-depfix charles university - depfix #otherefr", "cit": "[CLS] id participant cmu-uka carnegie mellon university, usa #otherefr limsi limsi-cnrs, france #refr liu university of linko?ping, sweden#otherefr systran systran,"}
{"pre": "[CLS] the translation quality is measured by bleu #refr. [SEP] bleu score #refr on the test sets. [SEP] [SEP] [SEP]", "cit": "[CLS] #refr propose an approach for phrase-based translation models: a phrase is a sequence of contiguous words and is extracted"}
{"pre": "[CLS] we use the stanford named entity recognizer #refr to extract named entities from the stanford named entities from the", "cit": "[CLS] for each word in a comment, we used the nrc emotion word lexicon #refr to discover if the word"}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders and opinion holders in opinion holders in opinion holders in", "cit": "[CLS] therefor, several studies proposed using contextual features to determine the subjectivity of a given word within its context #otherefr;"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] sentence in a text", "cit": "[CLS] abstractive summarization has been explored to some extent in recent years: sentence compression #otherefr, and a generationbased approach that"}
{"pre": "[CLS] the feature weights were tuned using minimum error rate training (mert) #refr on the development set using minimum error", "cit": "[CLS] another smart reordering model was proposed by #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to paraphrase extraction is to use a lexicon to extract paraphrases from comparable corpora #otherefr;", "cit": "[CLS] ed#otherefr h7: katakana variants whether one japanese term is a katakana variant for another #refr [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of converting natural language processing #otherefr; #refr. [SEP] [SEP] [SEP] to be able to", "cit": "[CLS] we thus explore annotation error correction and its feasibility for dependency annotation, a form of annotation that provides argument"}
{"pre": "[CLS] the model is trained using the averaged perceptron algorithm #refr. [SEP] score #refr with the best alignment algorithm #otherefr.", "cit": "[CLS] previous work #otherefr has presented a phrase-based monolingual aligner for nli (manli) that has been shown to significantly outperform"}
{"pre": "[CLS] #refr proposed a method for learning a probabilistic model that learns latent dirichlet allocation (lda) as a latent variable", "cit": "[CLS] recent years have seen ilp applied to many structured nlp applications including dependency parsing #otherefr; #refr and many previous"}
{"pre": "[CLS] the second approach is to use a word lattice as a sequence of a sequence of a sequence of", "cit": "[CLS] many researchers have proposed practical methods to resolve this problem such as #otherefr, #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of identifying the overall sentence level of a sentence is a text that is a text in", "cit": "[CLS] one could rely on existing trainable sentence selection #otherefr or even phrase selection #refr strategies to pick up appropriate"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? 2008. [SEP] ?", "cit": "[CLS] in this paper, we apply the traditional boundary (tbc) and role (trc) classifiers #refra), which are based on binary"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] ? ? ? ? ?", "cit": "[CLS] the use of collaborative contributions from volunteers has been previously shown to be beneficial in the open mind word"}
{"pre": "[CLS] in the context of machine translation, itg has been explored for mt #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] some recent work tapping annotated bitexts #otherefr, if for nothing more than to provide information about linguistic structure (e.g.,"}
{"pre": "[CLS] the most common approach to paraphrase generation is to either by using a lexical resource based on lexical similarity", "cit": "[CLS] we demonstrate the method on a sentence compression task #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] there has been growing interest in exploring the space between treebank-trained probabilistic grammars #otherefr; #refr). [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the automatic evaluation of the automatic evaluation of the automatic evaluation of the", "cit": "[CLS] this system has achieved competitive results with an f -measure of 82.7 when trained on the seven main types"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr. [SEP]", "cit": "[CLS] finally, we apply our language models to the task of re-ranking the n-best list from hiero #refr, a state-of-the-art"}
{"pre": "[CLS] the most popular approach to translation lexicons from parallel corpora #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] in such models, an explicit word sense disambiguation (wsd) is not necessarily required; rather, an implicit sense-match is sought"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of meaning that are not", "cit": "[CLS] such methods are highly scalable #refr and have been applied in information retrieval #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the sentence planning problem that the central role of determining the sentence", "cit": "[CLS] one of these approaches is the information state (is) approach (larsson and #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the translation model is a log-linear model #refr. [SEP] model with the decoder that we use the baseline for", "cit": "[CLS] for the system cims-syntax-rori, english data parsed with egret was reordered using scripts written for parse trees produced by"}
{"pre": "[CLS] we use the stanford named entity recognizer #refr to extract named entities from the stanford named entities from the", "cit": "[CLS] #refr use quoted speech attribution to reconstruct the social networks of the characters in a novel. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] the standard process for creating a parse selection model is: 1. parse the training set, recording up to 500"}
{"pre": "[CLS] the most common approach to this problem is to use a small number of seed examples #otherefr; #refr. [SEP]", "cit": "[CLS] we will be reporting on results using propbank1 #otherefr, a 300k-word corpus in which predicate argument relations are marked"}
{"pre": "[CLS] the task of identifying the event descriptions of event extraction and event extraction of event descriptions has been well", "cit": "[CLS] recently, #refr proposed an unsupervised generative model for inducing such templates. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to word alignment is the hmm alignment model #refr. [SEP] model is based on the", "cit": "[CLS] #refr modify model 4 to be a log-linear combination of 11 submodels (5 based on standard model 4 parameters,"}
{"pre": "[CLS] in the second category, the context of the context of a word is used to improve the performance of", "cit": "[CLS] #refr further extended this idea by using a single set of rules which globally applies to six different languages."}
{"pre": "[CLS] the model is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the em algorithm #refr with the", "cit": "[CLS] following #refr, we trained on 4,962 scenarios and tested on atis nov93 which contains 448 examples. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the first is the process of annotating the correct grammatical relations between the words in a text and the", "cit": "[CLS] the segmental structure of discourse has been claimed to constrain and be constrained by disparate phenomena: cue phrases #otherefr;"}
{"pre": "[CLS] we use the averaged perceptron algorithm #refr to train the parameters of the averaged perceptron algorithm #otherefr. [SEP] [SEP]", "cit": "[CLS] when data have distinct sub-structures, models exploiting latent variables are advantageous in learning #otherefr; #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] shared", "cit": "[CLS] in #refr, event annotations are converted into pseudo-syntactic representations and the task is solved as a syntactic extraction problem"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] model with the average number of the average", "cit": "[CLS] we compare pipeline-based processing with discrete optimization modeling used in the field of computer vision and image recognition #otherefr,"}
{"pre": "[CLS] the discourse structure of discourse relations are useful for applications such as summarization #otherefr, information retrieval #refr, and question", "cit": "[CLS] the fact that current anaphora resolution systems rely exclusively on the linear nature of texts in order to determine"}
{"pre": "[CLS] #refr use a similar approach to identify the relations in which words of words in a sentence and their", "cit": "[CLS] for example, #refr and cahill and riester #otherefr study the conditions for the use of different types of referring"}
{"pre": "[CLS] the second approach is to use a small number of seed examples for example outputs #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] support vector machines #otherefr) are a different kind of kernel method that, unlike kpca methods, have already gained high"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] the", "cit": "[CLS] the semeval-2010 task 10: linking events and their participants in discourse #refr targeted cross-sentence missing core arguments in both"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] levenberg et. al. #otherefr proposed an incremental adaptation technique for the core generative component of the smt system, word"}
{"pre": "[CLS] the first is the same as #refr, which was used for the penn treebank #otherefr for the penn treebank", "cit": "[CLS] the earliest work on chunking based on machine learning goes to #otherefr use memory based phrase chunking with accuracy"}
{"pre": "[CLS] in this paper, we propose a new model for unsupervised learning of #refr. [SEP] [SEP] [SEP] [SEP] ? ?", "cit": "[CLS] a number of diverse approaches have been proposed for deletion-based sentence compression, including techniques that assemble the output text"}
{"pre": "[CLS] the translation system is evaluated using bleu #refr, which is calculated by the quality of the case-insensitive bleu-4 metric", "cit": "[CLS] the cube pruning #refr decoding algorithm was chosen in order to speed-up the tuning step and the translation of"}
{"pre": "[CLS] in this paper, we propose a method for learning a synchronous context-free grammar induction of synchronous context-free grammars from", "cit": "[CLS] different from the bilingual parsing #otherefr; #refr that improves parsing performance with bilingual constraints, and the bilingual grammar induction"}
{"pre": "[CLS] the second approach is to use a small number of seed examples to score the output of the output", "cit": "[CLS] an efficient algorithm to compute tree kernels was given by #refra) which runs in close to linear time in"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] more recent research also focuses on mechanisms to define if two structures, e.g. graphs, are enough similar, as explored"}
{"pre": "[CLS] the most popular approach to word alignment is the ibm models #otherefr; #refr. [SEP] model 1 and the hmm", "cit": "[CLS] it is important because a wordaligned corpus is typically used as a first step in order to identify phrases"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a task of identifying the degree of semantic relations between words", "cit": "[CLS] we introduce a novel approach to verb clustering which involves the use of (i) a recent subcategorization frame (scf)"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best results for the best", "cit": "[CLS] as all distributions are conditioned with five or more features, they are all heavily backed off using chen back-off"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr. [SEP]", "cit": "[CLS] muc #otherefr, and entity-based ceaf (ceafe) #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] the use of pos information for improving statistical alignment quality of the hmm-based model is described in #refr. [SEP]"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of possible antecedents for the", "cit": "[CLS] the few publically available implementations of individual disambiguation algorithms, such as senselearner #otherefr, ukb #refr, and ims #otherefr, are"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the sentences and the syntactic parse trees of the sentences in", "cit": "[CLS] for example, gatica et al #otherefr and wrede and #refr automatically identify the level of emotion in meeting spurts"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the input is to be", "cit": "[CLS] at present, most high-performance parsers are based on probabilistic context-free grammars #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best results for the best", "cit": "[CLS] this observation echoes the findings for czech #refr, where case was also the most difficult to disambiguate. [SEP] [PAD]"}
{"pre": "[CLS] #refr used a supervised sentiment classification approach to classify reviews as well as sentiment classification and classify reviews as", "cit": "[CLS] other methods have been proposed which utilize composition of sentences #otherefr; #refr, but these methods use rules to handle"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] from the presented data, we can see that indirect reranking on las may not seem as good as direct"}
{"pre": "[CLS] the first is the case for the automatic acquisition of subcategorization frames from corpora #otherefr, #refr, and many others.", "cit": "[CLS] this perspective is related to that underlying the variation-n-gram approach for detecting errors in the linguistic annotation of corpora"}
{"pre": "[CLS] the second dataset is the use of a word sense disambiguation algorithm which is based on the most similar", "cit": "[CLS] bayes #otherefr 73.3 senseval-3 best system#refr 72.9 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the longrange reordering model is to use of the", "cit": "[CLS] traditionally, parallel corpora have been a mainstay of multilingual parsing #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the system is based on the inprotk toolkit for incremental dialogue systems developed by #refr. [SEP] system, which is", "cit": "[CLS] recently, incremental speech synthesis #otherefr; #refrb) which allows to start partial utterances that are then smoothly extended during verbalization."}
{"pre": "[CLS] the first is the task of identifying the mentions of a single sentence and a sentence #refr. [SEP] sentence", "cit": "[CLS] parallel bilingual data is often exploited to solve well-known tasks such as part-of-speech tagging #refr, named entity recognition #otherefr."}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within the source sentence into a target sentence into", "cit": "[CLS] nevertheless, most existing approaches to sentence alignment follow the monotonicity assumption that coupled sentences in bitexts appear in a"}
{"pre": "[CLS] the first is the process of annotating the sentence pairs in a sentence pair into a sentence and then", "cit": "[CLS] to measure the agreement among the annotators, similarly to mohammad #otherefr and #refr we calculated the majority class for"}
{"pre": "[CLS] the task of identifying the semantic relations between nominals of noun phrases and their semantic relations has been shown", "cit": "[CLS] few have attempted to use, or approximate, diathesis features directly for verb classification although manual classifications have relied on"}
{"pre": "[CLS] the first is the one of the most popular approaches to automatically identify the words in a text that", "cit": "[CLS] we collected a biomedical training corpus of approximately 513,000 medline abstracts using the following query composed of mesh terms"}
{"pre": "[CLS] in the context of machine translation, itg has been explored for statistical word alignment in both the context of", "cit": "[CLS] iii and marcu, 2009), generating bid-phrases in online advertising #otherefr and query expansion #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of identifying the semantic relations between nominals of a text is also relevant to the target language", "cit": "[CLS] this suggests that distributional models and logicbased representations of natural language meaning are complementary in their strengths #otherefr; #refr,"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a similar to the one used in the sense of the", "cit": "[CLS] yet there have been few attempts to learn finegrained lexical classifications from the statistical analysis of distributional data, analogously"}
{"pre": "[CLS] the grammar is a grammar that is a grammar that is a grammar that is a grammar that is", "cit": "[CLS] 1 according to #refr, a grammar is reversible if the parsing and generation problem is computable and the relation"}
{"pre": "[CLS] the task of identifying the semantic textual similarity of a text is a text of a text #refr. [SEP]", "cit": "[CLS] interestingly, recent works #otherefr; #refr have shown that such systems can be efficiently trained under indirect and imperfect supervision"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] zens and ney #otherefr and #refr utilized contextual information to improve phrase reordering. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the english side of the parallel corpus was parsed using the berkeley parser #refr, and the berkeley parser #otherefr", "cit": "[CLS] #refr use a loglinear classifier with linguistic features in order to disambiguate the chinese particle ?de? that has five"}
{"pre": "[CLS] #refr proposed a method for learning a word clustering based on a probabilistic model that learns vector space representations", "cit": "[CLS] a promising avenue for future work would be to incorporate wikipedia data into qanta by transforming sentences to look"}
{"pre": "[CLS] the grammar is a grammar that is a grammar that is a given a grammar that is a grammar", "cit": "[CLS] some proposals have been made for subgoal reordering at compile-time, .g. \\[minnen et al, 1993\\] elaborating on the work"}
{"pre": "[CLS] the grammar is a grammar that is a parsing system for the grammar that is a grammar that is", "cit": "[CLS] jacobs notes that other systems ~ have shared at least part of the linguistic information for parsing and generation;"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to learn a translation models from a", "cit": "[CLS] previous methods of reducing the size of smt model try to identify infrequent entries #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in recent years, there has been a growing interest in corpus-based approaches to anaphora resolution #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] automated approaches to metaphor detection involve both supervised and unsupervised approaches, some of which include: i) supervised classification on"}
{"pre": "[CLS] the most popular approach to translation lexicons from the source language into target language side #otherefr; #refr. [SEP] grammars", "cit": "[CLS] an ?oracle? telling us which variant is best is not available in the real world, of course, but in"}
{"pre": "[CLS] the first is the case for the automatic acquisition of subcategorization frames from corpora #otherefr, #refr, and many others.", "cit": "[CLS] #refr for one such algorithm). [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the averaged perceptron algorithm #refr to train the parameters of the averaged perceptron algorithm #otherefr. [SEP] [SEP]", "cit": "[CLS] this efficient and data-driven approach gives the best reported tagging accuracy for type-supervised sequence models, outperforming the minimized model"}
{"pre": "[CLS] in the second international chinese word segmentation bakeoff #otherefr; #refr and the best results are achieved by the best", "cit": "[CLS] cluster features, finally, are features over word clusters, as first used by #refr, which replace part-of-speech tag features.2 we"}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders in which the polarity of words are represented as subjective", "cit": "[CLS] a substantial body of work has been done on determining the affect #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the generation of referring expressions from the dialogue acts of the incremental processing", "cit": "[CLS] work on data-driven approaches has led to insights about the importance of linguistic features for sentence linearization decisions #otherefr."}
{"pre": "[CLS] #refr use a bootstrapping method to identify the polarity of words in a sentence. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] some of this work focuses on classifying the semantic orientation of individual words or phrases, using linguistic heuristics or"}
{"pre": "[CLS] the first is the one of the most popular approaches to automatically identify the text #otherefr; #refr. [SEP] text", "cit": "[CLS] segmentation may be particularly beneficial when working with documents without overt structure: speech transcripts #otherefr or novels #refr. [SEP]"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] english pvs are identified in the data using the jmwe library #refr as well as a post-processing module implemented"}
{"pre": "[CLS] the system is based on the standard approach of #refr and the best system for machine translation (mt) evaluation", "cit": "[CLS] much research effort has been made to address the transliteration issue in the research community #otherefr; #refr; oh and"}
{"pre": "[CLS] the first is the task of identifying the polarity of a word is a word in a word in", "cit": "[CLS] also, comparisons of bayesian, informationretrieval, neural-network, and case-based methods on word-sense disambiguation have also demonstrated similar performance (leacock, towell,"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] similar consideration motives a text-to-speech research on producing pronunciation for an mflulown words through morphological decomposition #refr. [SEP] [PAD]"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of meaning that are not", "cit": "[CLS] they can beat a random or lesk #otherefr, #refra)). [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to learn a translation rules from a", "cit": "[CLS] the data-oriented translation (dot) #refr model is a tree-structured translation model, in which linked subtree fragments extracted from a"}
{"pre": "[CLS] we use the berkeley parser #refr to obtain the syntactic parse trees for english and the english and the", "cit": "[CLS] #refr compare the rule templates of lexicalized tree adjoining grammars extracted from treebanks in english, chinese, and korean. [SEP]"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] shared", "cit": "[CLS] we use the event analyses created by the uturku #otherefr and uconcordia #refr systems for the bionlp 2011, the"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr. [SEP]", "cit": "[CLS] following most previous works in this area #otherefr; ge and #refr, we consider mrs in the form of tree"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] an important issue that we have left open is the coreference problem for holder extraction, which has been studied"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to learn a translation models from a", "cit": "[CLS] #refr propose factored translation models, which extend phrase-based statistical machine translation by allowing the integration of additional morphological features"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] shared", "cit": "[CLS] first we re-implement the rule-based approach of #refr using resources provided in the shared task. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the data set contains 14,619 items and is available from the same data collection used in the conll shared", "cit": "[CLS] we discarded all other entity annotations originally contained in the corpus assigning the outside class.2 the first data set"}
{"pre": "[CLS] the first is the case for the automatic evaluation of the automatic evaluation of the automatic evaluation of the", "cit": "[CLS] the semantic onstruction is represented by lud, language for underspecified discourse representation structures #refr, which takes discourse representation this"}
{"pre": "[CLS] the second dataset is the use of a word sense disambiguation task as a word sense disambiguation task #refr,", "cit": "[CLS] therefore, our proposed sense coherence measures focus on the semantic quality of a sense, adapted from topic coherence measures"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr.", "cit": "[CLS] reranking has been successfully employed to improve syntactic parsing #otherefr, semantic role labeling #refr, and named entity recognition #otherefrc)."}
{"pre": "[CLS] the system is based on the moses toolkit #otherefr and the moses toolkit #refr. [SEP] test set for the", "cit": "[CLS] the evaluation results show that we rank second in bleu #refr and ter #otherefr for spanish-english, and in the"}
{"pre": "[CLS] the translation quality is evaluated by case-insensitive bleu-4 #refr. [SEP] quality measured by bleu score #refr with the bleu", "cit": "[CLS] for example, #refr proposes features like amount of reordering, the morphological complexity of the target language, and historical relatedness"}
{"pre": "[CLS] the idea of using semantic information is used in natural language processing #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] to", "cit": "[CLS] in this paper, we combine the data from framenet with the lth semantic parser #otherefr, until very recently #refr"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a task of semantic relations between words and phrases and logical", "cit": "[CLS] in contrast, classifiers for semantic tagging #otherefr; #refr) label word instances and focus on the local context surrounding each"}
{"pre": "[CLS] we use the stanford ner tagger #refr to generate the sentences and the sentences in the text with the", "cit": "[CLS] both martins and smith #otherefr and #refr build models that jointly extract and compress, but learn scores for sentences"}
{"pre": "[CLS] the task of identifying the semantic relations between nominals of noun phrases and their semantic relations has been shown", "cit": "[CLS] we also used the following resources: the charniak parser #refr to carry out the syntactic analysis; the wn::similarity package"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] a great deal of researches have been conducted on this topic with promising progress #otherefr; #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] topic models have been successfully applied to a wide range of tasks including word sense disambiguation #otherefr, and sentence", "cit": "[CLS] a good candidate for multi-lingual topic analyses are polylingual topic models #refr, which learn topics for multiple languages, creating"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of meaning that are not", "cit": "[CLS] #refr use a clustering algorithm which compares test instances to two automatically constructed seed sets (one literal and one"}
{"pre": "[CLS] the first is the task of identifying the word sense disambiguation of word sense disambiguation of word sense disambiguation", "cit": "[CLS] argument-mapped wordnet #refr provides explicit mappings of arguments between verbs to alleviate the difficulty of tracking argument changes. [SEP]"}
{"pre": "[CLS] the first is the case for the semantic role labeling task of syntactic parsing, we use the predicate-argument structure", "cit": "[CLS] figure 1: a sample syntactic structure with function labels. duce trees annotated with bare phrase structure labels #otherefr; #refr."}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] for example, citation-based summarization systems #otherefr and survey generation systems #refr can benefit from citation purpose and polarity analysis"}
{"pre": "[CLS] the first is the task of identifying the polarity of a sentence, a sentence, and a document into a", "cit": "[CLS] we use a modified version of the edge-factored parser of #refr to predict vdrs over a set of annotated"}
{"pre": "[CLS] the discourse structure of discourse relations are useful for a number of nlp applications such as information extraction #otherefr,", "cit": "[CLS] recently a number of researchers have indicated that temporal expressions such as tense morphemes and temporal adverbials can be"}
{"pre": "[CLS] in the second international chinese word segmentation bakeoff #otherefr; #refr and the best results are achieved by the best", "cit": "[CLS] by extending the firstorder model, mcdonald and pereira #otherefr and #refr exploit second-order features over two adjacent arcs in"}
{"pre": "[CLS] the translation quality is evaluated by case-insensitive bleu-4 #refr. [SEP] quality measured by bleu score #refr with the bleu", "cit": "[CLS] prior work on bilingual lexicon induction has shown that a variety of signals derived from monolingual data, including distributional,"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] for example, in the context of syntactic disambiguation, black #otherefr and #refr proposed statistical parsing models based-on decision-tree l"}
{"pre": "[CLS] in recent years, there has been a growing interest in the task of identifying the task of identifying the", "cit": "[CLS] in later years, there has been more interest in problems such as sentence compression #otherefr; #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] this component has been integrated into our machine translation system #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the one of the most popular approach to use crowdsourcing, which has been to use a", "cit": "[CLS] #refr consider the nli task as a sub-task of the authorship attribution task. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the moses toolkit #refr to train the weights for the feature weights for each system. [SEP] [SEP]", "cit": "[CLS] as multiple derivations are used for finding optimal translations, we extend the minimum error rate training (mert) algorithm #refr"}
{"pre": "[CLS] the first is the case for the generation of referring expressions that we will use the rst bank #refr", "cit": "[CLS] the learning of syntax could probably be supported by the integration of parsers, which could be of particular interest"}
{"pre": "[CLS] we use the averaged perceptron algorithm #refr to train the parameters of the model for the model parameters. [SEP]", "cit": "[CLS] the summarizer uses the following features, as reported in previous work #otherefr; #refra): ? querysimilarity: sentence similarity to the"}
{"pre": "[CLS] in the context of machine translation, itg has been explored for statistical word sense disambiguation #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] most of the applications of comparable corpora focus on discovering translation equivalents to support machine translation, such as bilingual"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] shared", "cit": "[CLS] for reference, the reported performance of the taskwinning system in 2009, kilicoglu09#refr, is shown in the top. [SEP] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a text that are semantically similar to the", "cit": "[CLS] we try the new similarity matrix on two publicly available similaritybased segmenters aps #refr and mincutseg #otherefr. [SEP] [PAD]"}
{"pre": "[CLS] the first is the automatic evaluation of discourse structure of discourse relations #refr. [SEP] quality in the fact that", "cit": "[CLS] the corpus is parsed using bikel?s parser #refr and the verb-object collocations are extracted. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] the simplest of these #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the automatic evaluation of the automatic metric of the automatic evaluation of the automatic evaluation of", "cit": "[CLS] word alignment still requires the study of various approaches, e.g. #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the automatic evaluation of discourse structure of discourse relations #refr. [SEP] discourse markers in the discourse", "cit": "[CLS] some studies focus on content-related questions such as the analysis of plot or characterization and the exploration of relations"}
{"pre": "[CLS] we use the stanford parser #refr to extract dependency relations for each sentence and parse trees from the biomedical", "cit": "[CLS] as required for features, we use stanford corenlp?s tokenizer, part of speech tagger #otherefr, and use the charniak johnson"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] algorithm with the best derivation = exp ( m?", "cit": "[CLS] this evaluation scheme was the same as used in previous evaluations of lexicalized grammars #refr; clark the hpsg treebank"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] we partof-speech tagged our train and test data using stanford tagger #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the automatic metric of discourse structure of discourse structure of text #refr. [SEP] is the discourse", "cit": "[CLS] vector based techniques have been exploited in a wide array of natural language processing applications #otherefr; #refr. [SEP] [PAD]"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] shared", "cit": "[CLS] fbk-irst #otherefr complutense university of madrid, spain svm classifier (weka smo) scai#refr fraunhofer scai, germany svm classifier #otherefr humboldt"}
{"pre": "[CLS] the second approach is to use a small number of manually annotated corpora such as the same data set", "cit": "[CLS] we follow #refr in defining the head of a noun phrase as the rightmost noun, or if there is"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] shared", "cit": "[CLS] the infectious diseases #otherefr #refra) is an information extraction task focusing on the biomolecular mechanisms of infectious diseases. [SEP]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] figure 2: difference-histogram of word order distortions for english?german. systems #refr with and without source reordering and evaluated on"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] the", "cit": "[CLS] particularly, the latter made team ?09 task people reference faust ? 12- 3c #otherefr uturku ? 123 1bi (bj#refr"}
{"pre": "[CLS] in the second international chinese word segmentation bakeoff #otherefr; #refr and the state-of-the-art accuracies of the state-of-the-art accuracies of", "cit": "[CLS] work on dual decomposition for nlp is related to the work of #refr who apply belief propagation to inference"}
{"pre": "[CLS] in the second category, the context of a word is used to improve the performance of a state-of-the-art statistical", "cit": "[CLS] as an extension, we allow the interpolation weights to be a function of the current tag: ?(ti), since class-dependent"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for dependency parsing #refr. [SEP] grammars", "cit": "[CLS] in a way, our work is then analogous to the work of hall and #refr who apply a post-processing"}
{"pre": "[CLS] the most common approach to paraphrase acquisition is to use a lexicon to automatically extracted from corpora #otherefr; #refr.", "cit": "[CLS] in the literature, this task has been proposed quite recently #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the syntactic dependency parsing of the english lexical substitution task #refr. [SEP] and", "cit": "[CLS] #refr reports a larger experiment, also using a pos tagged corpus and a finite-state np parser, attempting to recognise"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] meanwhile, the target of the training corpus is parsed with charniak?s parser #refr, and each phrase is annotated with"}
{"pre": "[CLS] we use the stanford ner tagger #refr to train the feature weights. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] other studies (yu and #refr built global probabilistic graphical models. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr.", "cit": "[CLS] showing the benefits of vector composition for language modelling, #refr emphasize its potential to become a standard method in"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] #refr described a technique for word alignment in a multi-parallel sentence-aligned corpus and showed that this technique can be"}
{"pre": "[CLS] the first is the automatic evaluation of the automatic metric of machine translation evaluation metrics such as bleu #refr,", "cit": "[CLS] 2.2.2 decision list the decision list classifier uses the log-likelihood of correspondence between each context feature and each sense,"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the longrange reordering problem as a sequence of the", "cit": "[CLS] qg has been applied to some nlp tasks other than mt, including answer selection for question-answering #refr, paraphrase identification"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with a beam search", "cit": "[CLS] the last two rows in the table show the accuracies of fowler and penn #otherefr (f&p), who applied the"}
{"pre": "[CLS] in the last decade, there has been a lot of work on learning parsing with semantic parsers #otherefr; #refr.", "cit": "[CLS] current studies on processing microblogs have focused mainly on the difference between the quality of microblogs #otherefr; #refr, and"}
{"pre": "[CLS] the dialogue manager (dm) is the one of the most common tasks of the one proposed in the field", "cit": "[CLS] another evaluation which was controlled and was done at least partially in a real-world context was the evaluation of"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] iii and #refr]), which finds that only 2.7% of human summary sentences are extracts. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the words is to the", "cit": "[CLS] #refr identified hebrew multiword expressions by searching for misalignments in an english?hebrew parallel corpus. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a text that are semantically", "cit": "[CLS] a number of papers have suggested that paraphrase knowledge plays a very important role #otherefr; #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the parsing model of #refr, which uses a word segmentation and a language", "cit": "[CLS] hwa et al, 2004; yarowsky and #refr which are difficult to find, especially for lesser studied languages. [SEP] [PAD]"}
{"pre": "[CLS] the second approach is to use a small number of seed examples for example outputs #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] see #otherefr; #refr for related studies in statistical natural language parsing. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] event and semantic role", "cit": "[CLS] a similar approach has been advocated for the interpretation of discourse relations by #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of identifying the overall sentiment in text is a text #otherefr; #refr. [SEP] of the same event", "cit": "[CLS] 4 we compare the end clustering quality across a variety of thresholds and for various system flavors using three"}
{"pre": "[CLS] in this paper, we propose a method for learning a dependency parser based on the generative dependency parser of", "cit": "[CLS] specifically, all models use an arc-eager transition strategy and are trained using the averaged perceptron algorithm as in #refr"}
{"pre": "[CLS] the generation of referring expressions is a subject of research in natural language generation (nlg) in natural language generation", "cit": "[CLS] until recently, gre algorithms have focussed on the generation of distinguishing descriptions that are either as short as possible"}
{"pre": "[CLS] #refr use a similar approach to identify the polarity of words in a text in a text in a", "cit": "[CLS] in the domain of synchronous conversations, prosodic features such as duration, speech rate and pause have been used for"}
{"pre": "[CLS] the most popular approach to word alignment is the ibm models #otherefr; #refr. [SEP] model 1 and the hmm", "cit": "[CLS] motivated by the work in both #refr and #otherefr, we propose the two following simplest versions of extended hmms"}
{"pre": "[CLS] in the context of machine translation, itg has been explored for a variety of nlp tasks, including semantic role", "cit": "[CLS] for example, #refr aligned attributes in wikipedia infoboxes based on cross-page links. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford ner tagger #refr to train the training data with the feature weights. [SEP] [SEP] [SEP]", "cit": "[CLS] for further testing, we used the remaining mul- text languages, as well as the languages of the connl-x #refr"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best results for the best", "cit": "[CLS] the mechanism we employ for incorporating morphology into the pcfg model (the model 1 parser in #refr) is the"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] ? ? ? ? ?", "cit": "[CLS] applications using the knowledge contained in wikipedia include, among others, text categorization #otherefrb), multi-document summarization #refr, and text generation"}
{"pre": "[CLS] in the context of machine translation, itg has been explored for a variety of nlp tasks, including pos tagging", "cit": "[CLS] methods that use no supervision at all #otherefr; #refr have been extensively studied, but still do not perform well"}
{"pre": "[CLS] the task of identifying the semantic relations between nominals of noun phrases and their semantic relations has been used", "cit": "[CLS] #refr presented initial work, clustering nouns using their noun-verb co-occurrence information. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of identifying the event descriptions of event extraction and event descriptions has been studied in the goal", "cit": "[CLS] names of these kinds, generalized names #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the grammar is a grammar that is a grammar that is a given a grammar that is a grammar", "cit": "[CLS] efd-closure bears some resemblance in its intentions to greibach normal form, but: (1) it is far more conservative in"}
{"pre": "[CLS] the task of coreference resolution is a well-studied problem #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] next, we define l(m) to be the set con- 2a head word is assigned to every mention with the"}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders and opinion holders in opinion holders and opinion holders in", "cit": "[CLS] although sentiment analysis involves various different problems such as identifying subjective sentences or identifying positive and negative opinions in"}
{"pre": "[CLS] the first is the task of learning a semantic parser that is a component of the semantic role labelling", "cit": "[CLS] we use the inference process introduced by #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr proposed a method for identifying the polarity of a document by a phrase based on a phrase based", "cit": "[CLS] existing methods for sentence ordering are divided into two approaches: making use of chronological information #otherefr; and learning the"}
{"pre": "[CLS] in the literature, discriminative reranking has been widely used in nlp tasks such as pos tagging #otherefr, parsing #refr,", "cit": "[CLS] #refr use neighborhoods of related instances to figure out what makes found instances ?good?. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking approaches #otherefr; #refr to learning for dependency", "cit": "[CLS] unfortunately, the parsing accuracies of all models have been reported to drop significantly on outof-domain test sets, due to"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking approaches #otherefr; #refr to learning for dependency", "cit": "[CLS] this model has been adapted and extended by a number of authors and currently represents the stateof-the-art for dependency"}
{"pre": "[CLS] the second approach is to use a small number of seed examples for example outputs #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] we follow kawahara and kurohashi #otherefr to extract a pair of an argument noun and a predicate (cf), and"}
{"pre": "[CLS] the task of identifying positive and negative sentiments and negative sentiments from a passage, #otherefr; #refr. [SEP] of the", "cit": "[CLS] similar strategies have been widely used in unsupervised information extraction #otherefr; #refr and selectional preference each whole conversation usually"}
{"pre": "[CLS] the translation quality is evaluated by case-insensitive bleu-4 #refr. [SEP] quality measured by bleu score #refr with the bleu", "cit": "[CLS] following phrase-based methods in statistical machine translation #refr and machine transliteration #otherefr, we model substitution of longer sequences. [SEP]"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a text with a text that are semantically", "cit": "[CLS] table 1: # of books available per genre at gutenberg with download thresholds used to define more successful #otherefr),"}
{"pre": "[CLS] we use the stanford core nlp suite #refr to train a translation model for each document with its default", "cit": "[CLS] the results show that, as compared to bleu, several recently proposed metrics such as semantic-role overlap (gimenez and ma#refr,"}
{"pre": "[CLS] the most common approach to wsd is to use a lexicon to measure which is to automatically identify the", "cit": "[CLS] more recently, context-based models of disambiguation have been shown to represent significant improvements over the baseline #otherefr, #refr. [SEP]"}
{"pre": "[CLS] the system is based on the moses toolkit #otherefr and the standard phrase-based smt system described in #refr. [SEP]", "cit": "[CLS] id participant cmu carnegie mellon university, usa #otherefr rali rali, university of montreal, canada #refr systran systran, france uedin-birch"}
{"pre": "[CLS] the best performing systems were ukp #otherefr and the best performing system #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] the recent bionlp 2009 shared task (bionlp09st) on event extraction #refr focused on event types of varying complexity. [SEP]"}
{"pre": "[CLS] the first is the automatic evaluation of the automatic metric of the automatic evaluation of the automatic evaluation of", "cit": "[CLS] word alignment still requires the study of various approaches, e.g. #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] in previous work, #refr model the reordering/deletion of source-side semantic roles in a tree-to-string translation model. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for each sentence with dependency trees for", "cit": "[CLS] because of the cost and difficulty in treebank construction, researchers have also investigated the utilization of unannotated text, including"}
{"pre": "[CLS] the most popular approach to word alignment is the hmm alignment model #refr. [SEP] model is based on the", "cit": "[CLS] aligning documents from different languages arises in a range of tasks such as parallel phrase extraction #otherefr, mining translations"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] for languages with non-projective dependencies, graphs therefore need to be projectivized for training and deprojectivized for testing #otherefrb; #refr."}
{"pre": "[CLS] the task of relation extraction has received much attention in recent years #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] the second line of work designs kernel functions on some structured representation #otherefr; #refr; bunescu and mooney, 2005a; bunescu"}
{"pre": "[CLS] the first group (asr features) includes a variety of tasks, such as sentence planning and sentence planning and sentence", "cit": "[CLS] after experimenting with several tagging methods, we concluded that the approach presented in walker and whittaker #otherefr adopted from"}
{"pre": "[CLS] the second approach is to use a small number of manually annotated corpora such as the same data set", "cit": "[CLS] several implementations of the tbl approach are freely available on the web, the most wellknown being the so-called brill"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] in the case of targets, the work by #refr exhibits a pragmatic focus as well. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the averaged perceptron algorithm #refr. [SEP] score #refr with the best alignment model with", "cit": "[CLS] this stands in contrast to a competing approach (sherif and #refr that is inspired by phrase-based machine translation #otherefr"}
{"pre": "[CLS] #refr use a similar approach to compute similarities between words in a sentence. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] following the prior work in #otherefr, and a similarity function (simcost) #refr which is defined as the ratio of"}
{"pre": "[CLS] the grammar is a synchronous grammar formalism which is a synchronous grammar (scfg) used in grammar formalisms that is", "cit": "[CLS] in particular, #refr find that the coverage of a translation model can increase dramatically when one allows a bilingual"}
{"pre": "[CLS] the first is the task of identifying the sentence planning problem that the user is a text generation of", "cit": "[CLS] we duly notice that, for the give challenge in particular (and probably for human evaluations in general) the success"}
{"pre": "[CLS] the translation model is a log-linear model #refr. [SEP] model with the decoder that we use the baseline for", "cit": "[CLS] state-of-the-art smt systems model the translation distribution pr(t|s) via the log-linear approach #refr: t? = argmax t pr(t|s) (1)"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] is a growing number of", "cit": "[CLS] our unsupervised approach follows a self training protocol #refr; mcclosky et al, 2006; reichart and rappoport, 2007b) enhanced with"}
{"pre": "[CLS] the most popular approach to paraphrase generation has been the source sentences into a target language #otherefr; #refr. [SEP]", "cit": "[CLS] figure 2: using an smt system used to translate large amounts of monolingual data. set #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] moreover, rather than predicting an intrinsic metric such as the parseval f- score, the metric that the predictor learns"}
{"pre": "[CLS] the first is the case for the semantic role labeling task of the predicate-argument structure of the process of", "cit": "[CLS] similarly, #refr and #otherefr learn sentence level paraphrase templates from a corpus of news articles stemming from different news"}
{"pre": "[CLS] the most common approach to dependency parsing is to use a projective dependency parser #refr and a projective dependency", "cit": "[CLS] as a light-weight formalism offering syntactic information to downstream applications such as smt, the dependency grammar has received increasing"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a task of identifying the degree of semantic relations between words", "cit": "[CLS] this seems to be due to lack of topical context #refr as well as local context #otherefr. [SEP] [PAD]"}
{"pre": "[CLS] the system is based on the moses toolkit #refr that we used for the best output of the best", "cit": "[CLS] previous evaluation of addicter shows that hypothesis-reference alignment coverage (in terms of discovered word pairs) directly influences error analysis"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] all evaluation is in terms of the bleu score on our test set #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] statistical parsers trained on the penn treebank #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best results for the best", "cit": "[CLS] decomposition various mixing approaches have been proposed to combine the above two approaches #otherefr; #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the generation of referring expressions that we will use the rst bank #refr", "cit": "[CLS] we adopt :the conditions for tooic:assignment.-:proposed \" in ' \\[#refr\\]. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr proposed a method for learning a word clustering based on a probabilistic model that learns vector space representations", "cit": "[CLS] since research has demonstrated the applicability of concreteness to a range of other nlp tasks #refr, it is important"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to learn a translation probabilities from", "cit": "[CLS] in hierarchical phrase-based translation, some other types of lexicalized reordering models have been investigated recently #otherefrb; #refr; huck et"}
{"pre": "[CLS] the most popular approach to paraphrase generation has been the extraction #otherefr; #refr. [SEP] input text is based on", "cit": "[CLS] other efforts blur the lines between the two, applying techniques from statistical machine translation to monolingual data or extracting"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? ? ? ?", "cit": "[CLS] it generalizes a subset tree kernel (stk) #refr that maps a tree into the space of all possible tree"}
{"pre": "[CLS] the first is the automatic evaluation of discourse structure of discourse structure of discourse structure of discourse relations #refr.", "cit": "[CLS] vp ellipsis has received a great deal of attention in theoretical and computational linguistics #otherefr; #refr; lappin and mccord,"}
{"pre": "[CLS] the translation quality is measured by bleu #refr. [SEP] score #refr with the bleu score of the translation quality.", "cit": "[CLS] #refr develop the machine translation performance predictor #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] others have introduced alternative discriminative training methods #otherefr; #refr, in which a recurring challenge is scalability: to train many"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] recent work uses word-based translation #otherefr; #refr to project source language sentences into the target language for retrieval. [SEP]"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from the sentence pairs in the input sentence by", "cit": "[CLS] so far algorithms for cognate recognition have been focussing predominantly on the detection of cognate words in a text,"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best results for the best", "cit": "[CLS] beginning with #refr, numerous authors have used the pointwise mutual information between pairs of words to analyze word co-locations"}
{"pre": "[CLS] #refr proposed a method for identifying topic boundaries in a document by a topic boundaries in a topic signature", "cit": "[CLS] our reliance on seed sets is orthogonal to existing approaches that use external knowledge, which operate at the level"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the input is to be", "cit": "[CLS] these operations are not domain-specific and are similar to those of previous aggregation components #otherefr; #refr, although the various"}
{"pre": "[CLS] in the context of machine translation, itg has been explored for a variety of nlp tasks such as semantic", "cit": "[CLS] using parallel corpora: comparable and parallel corpora, including news streams and multiple translations of the same story, have been"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a text with a text that are semantically", "cit": "[CLS] lin et al?s features: following lin et al #otherefr, we extract the following three types of features: (1) pairs"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? ? ? ?", "cit": "[CLS] some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g. #refr, #otherefr), but most do"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] in most cases this amounts to an improvement of about 1.5 bleu points #refr and 1.5 meteor points #otherefr."}
{"pre": "[CLS] the similarity between two words was computed using the wordnet similarity measure proposed by #refr. [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] automatic detection of such argument alternations is important to acquisition of verb lexical semantics #otherefr, and moreover, may play"}
{"pre": "[CLS] in this paper, we propose a discriminative model for learning from random field (crf) and perceptron learning #refr. [SEP]", "cit": "[CLS] discriminative taggers and chunkers have been the state-of-the-art for more than a decade #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] sentences with virtual dependency nodes were omitted, as they are not annotated in the constituent treebank and their treatment"}
{"pre": "[CLS] the system is based on the inprotk toolkit for the best performing system described in #refr. [SEP] system, which", "cit": "[CLS] both the eurotra appr~ch to time (cf. #refr and the nigel approach #otherefr grew out of a critical appraisal"}
{"pre": "[CLS] the semantic relation between nominals in the verb and the verb frame acquisition system has been used in the", "cit": "[CLS] as stated before, propbank #otherefr treats negation superficially and framenet #refr regrettably disregards negation. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the second international chinese word segmentation bakeoff #otherefr; #refr and the best results are reported in the best", "cit": "[CLS] second, we limit the number of tags considered for each token by a pruning method that refines #refr tag"}
{"pre": "[CLS] the translation quality is measured by bleu #refr. [SEP] score #refr on the test set. [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] as for the smt system, we use a standard log-linear pb-smt model #otherefr: giza++ implementation of ibm word alignment"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] dependency parsers have been tested on parsing sentences in english #otherefr as well as many other languages #refra). [SEP]"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with a probability distribution", "cit": "[CLS] #refr present a systematic analysis of these ensemble methods and find several non-obvious facts: ? the diversity of base"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] many existing systems tbr smt #refr; niefien et al, 1.#otherefr: 'l?he correspondence b tween the words in the source"}
{"pre": "[CLS] the system used in this paper is based on the c&c parser #otherefr and the c&c parser #refr. [SEP]", "cit": "[CLS] in the domain of the air traveler information system, #refr apply statistical methods to compute the probability of a"}
{"pre": "[CLS] #refr use a supervised sentiment classifier to detect the polarity of words in a target words and their polarity", "cit": "[CLS] although the task of identifying the overall sentiment polarity of a document has been well studied, most of the"}
{"pre": "[CLS] the translation quality is evaluated by case-insensitive bleu-4 #refr. [SEP] quality measured by bleu score #refr with the bleu", "cit": "[CLS] radically different statistical models have also been proposed. #refr investigated maximum entropy models as an alternative to the so-called"}
{"pre": "[CLS] the first is the task of identifying the semantic relations between nominals of noun phrases and the verb classes", "cit": "[CLS] given a weight matrix, a, for each adjective in the phrase, we apply the functions in sequence recursively as"}
{"pre": "[CLS] the second approach is to use a maximum entropy model #otherefr; #refr. [SEP] model for the probability distribution over", "cit": "[CLS] modeling morphosyntactic relations explicitly has been shown to improve statistical parsing models #otherefr; #refr, but the codependency between morphology"}
{"pre": "[CLS] the first is the task of identifying the polarity of a sentence, a sentence, a word is analyzed by", "cit": "[CLS] some researchers #otherefr; #refr have applied statistical methods to identify the strongest semantic associations. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the annotation tool which was the annotation tool for the annotation tool #refr. [SEP] project in", "cit": "[CLS] #refr address this by assigning weights to hedging cues. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] many solutions to the reordering problem have been proposed, e.g. syntax-based models #otherefr, and tree-to-string methods #refr. [SEP] [PAD]"}
{"pre": "[CLS] the first is the case for the automatic evaluation of translation quality measured by bleu #refr, which is the", "cit": "[CLS] we set al weights by optimizing bleu #refr using minimum error rate training #otherefr to translate 2,000 test sentences"}
{"pre": "[CLS] topic models have been successfully applied to a wide range of nlp tasks including word sense disambiguation #otherefr, and", "cit": "[CLS] the measure pk, proposed by #refr, uses a probe window equal to half the average length of a segment;"}
{"pre": "[CLS] in the context of machine translation, itg has been explored for mt #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] if the source and target are reordered so that one side more closely matches the other, or one side"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] equation 6 is a log-linear model used in common nlp tasks such as tagging, chunking and named entity recognition,"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] dwls were first introduced by #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking approaches #otherefr; #refr to learning for dependency", "cit": "[CLS] in #refr, pcfg parsing of ne- gra is improved by using sister-head dependencies, which outperforms standard head lexicalization as"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] coverage precision grammar of english #refr and a lexical functional grammar parser #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a similar to the one used in the sense of the", "cit": "[CLS] some applications that would benefit from knowing this distinction are machine translation #otherefr, (multilingual) information retrieval #refra), etc. [SEP]"}
{"pre": "[CLS] #refr use a bootstrapping method to identify opinion holders and extract opinion expressions from the polarity of opinion expressions", "cit": "[CLS] while early attempts focused on classifying overall document opinion #otherefr; #refr, more recent approaches identify opinions expressed about individual"}
{"pre": "[CLS] the grammar we are extracted from the parallel corpus using the berkeley parser #refr and the berkeley parser #otherefr.", "cit": "[CLS] #refr propose a theoremproving lnodel that integrates yntactic onstraints with variable-cost abductive semantic and pragmatic a~sumptions. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] in the context of spoken dialogue systems #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ?", "cit": "[CLS] training in order to obtain the weighted productions of grst , we use an existing state-of-the-art discourse parser3 #otherefr"}
{"pre": "[CLS] the second approach is to use a small number of seed examples for example outputs #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] most works in domain adaptation have focused on learning a common representation across training and test domains #otherefr; #refr."}
{"pre": "[CLS] the most popular approach to smt is the one proposed by #refr, who use a large set of features", "cit": "[CLS] for example, the candide system #refr was trained on ten years? worth of canadian parliament proceedings, which consists of"}
{"pre": "[CLS] the translation quality is measured by bleu #refr. [SEP] [SEP] = 4. [SEP] = 4. [SEP] = ? =", "cit": "[CLS] we extend the glm framework by parallel perceptron training #refr and dynamic learning with adaptive weight updates in the"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model #refr to learn a translation probabilities from", "cit": "[CLS] for our experiments we use the phrase-based machine translation techniques described in #otherefr, integrating our models within a log-linear"}
{"pre": "[CLS] the generation of referring expressions is a subject of research in natural language generation (nlg) in natural language generation", "cit": "[CLS] related work from generation icludes the correction of misconceptions i the work of #refr and the explicit representation f"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] sentence and thus a", "cit": "[CLS] hyponymy relations can play a crucial role in various nlp systems, and there have been many attempts to develop"}
{"pre": "[CLS] the system uses a combination of the stanford parser #refr to extract dependency relations from the parse trees for", "cit": "[CLS] this high demand has resulted in many nlp research papers on the topic, a synthesis series book #otherefr and"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? 2008. [SEP] ?", "cit": "[CLS] recent developments aim to lift these limitations, either by reducing the amount of supervision #otherefr; artzi and #refr or"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] our phrase-based systems are tuned with k-best mira #refr on development set. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of meaning that are not", "cit": "[CLS] statistics-based method often requires large-scale corpora #otherefr; #refr, sense-tagging or not, monolingual or aligned bilingual, as training data to"}
{"pre": "[CLS] the most common approach to semantic role labeling is to use syntactic information #otherefr; #refr. [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] for the above reasons, our sample selection procedure was informed by two existing resources, the english framenet and salsa,"}
{"pre": "[CLS] the first is the process of annotating the discourse structure in text with the text with the text with", "cit": "[CLS] the core generator has a pipeline architecture which is similar to many existing systems #refr: an incoming request is"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] #refr #otherefr) showed how to enhance chinese-english verb alignments by exploring predicate-argument structure alignment using parallel propbanks. [SEP] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] words in a text", "cit": "[CLS] firstly, as semantic metadata useful for tasks such as summarization #refr, but later recognizing the impact that good keyphrases"}
{"pre": "[CLS] #refr proposed a method for learning a bootstrapping method for bootstrapping a sequence of topic modeling the sequence of", "cit": "[CLS] #refr proposed a generative approach to use extended lda to model selectional preferences. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] is a growing number of different", "cit": "[CLS] we use the transducer model of #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a text that is a", "cit": "[CLS] world knowledge axioms can also be easily derived by processing the gloss #otherefr. a.1 semant ic and logic t"}
{"pre": "[CLS] the second approach is to use a word lattice as a sequence of a sequence of a sequence of", "cit": "[CLS] #refr took a similar approach but used second order co-occurrence vectors and report improved performance. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for each sentence with dependency trees for", "cit": "[CLS] early examples of this work include #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] id institution balagur yandex school of data analysis #otherefr commercial-1,2,3 anonymized commercial systems online-a,b,g anonymized commercial systems online-a,b,g anonymized", "cit": "[CLS] a english-chinese translation environment described by #refr presents students with l1 sentences to translate into l2 speech. [SEP] [PAD]"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking approaches #otherefr; #refr to learning for dependency", "cit": "[CLS] we report experiments on twelve languages from the conll-x shared task #refr.5 all experiments are evaluated using the labeled"}
{"pre": "[CLS] the task of segmenting utterances into words, for example, johnson and colleagues have been shown to be useful in", "cit": "[CLS] in nlp problems such as text classification #otherefr, statistical parsing #refr, information extraction #otherefr, pool-based active learning has produced"}
{"pre": "[CLS] the task of identifying the semantic relations between nominals of noun phrases and their semantic relations has been used", "cit": "[CLS] recent work on offline acquisition of finegrained, labeled classes of instances applies manually-created #otherefr; #refr or automatically-learned #otherefr extraction"}
{"pre": "[CLS] the first is the task of identifying the event descriptions of coreference resolution of events #refr. [SEP] event mentions", "cit": "[CLS] one reason could be that these difficult pronouns do not appear frequently in standard evaluation corpora such as muc,"}
{"pre": "[CLS] the first is the case for the case of the syntactic mt system described in #refr. [SEP] model is", "cit": "[CLS] translation rules have been central to hierarchical phrase-based and syntactic statistical machine translation #otherefr; #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the word alignments for each word in a sentence. [SEP] [SEP]", "cit": "[CLS] each system is optimized using mert #otherefr with bleu #refr as an evaluation measure. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for each sentence in the training corpus.", "cit": "[CLS] in fact, much recent work has demonstrated that learning cross-lingual correspondences from corpus data greatly reduces the ambiguity inherent"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best derivation of the best", "cit": "[CLS] treebank-based probabilistic parsing has been the subject of intensive research over the past few years, resulting in parsing models"}
{"pre": "[CLS] the semantic relation between nominals in the verb frame acquisition and the information extraction task is to be extracted", "cit": "[CLS] spatial role labeling at semeval-2013 is the second iteration of the task, which was initially introduced at semeval-2012 #refra)."}
{"pre": "[CLS] the first is the same as #refr, which we will use the same as the same as the same", "cit": "[CLS] first, there are the studies that focused attention on compound clauses #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr proposed a method for learning a word segmentation based on a word clustering words from a target language", "cit": "[CLS] #refr proposed an unsupervised algorithm for word segmentation. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford named entity recognizer #refr to extract named entities from the text corpus. [SEP] [SEP] [SEP]", "cit": "[CLS] the third (i.e. the grammatical) approach promises a way to achieve this, and has been instantiated in various ways"}
{"pre": "[CLS] the first is the case for the identification of the language of the language model is a language that", "cit": "[CLS] the japanese noun phrase a no b roughly corresponds to the english noun phrase b of a, lint it"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking approaches #otherefr; #refr to learning for dependency", "cit": "[CLS] in one case, we blindly ran each learner through 40 steps of inside-outside re-estimation, ignoring any convergence criteria; in"}
{"pre": "[CLS] the first is the case for the syntactic parsing of the english lexical substitution task #refr. [SEP] test set", "cit": "[CLS] approaches to probabilistic lustering similar to ours were presented recently in #refr and hofmann and puzicha #otherefr. [SEP] [PAD]"}
{"pre": "[CLS] the task of identifying the overall sentence pairs in text is a text #refr. [SEP] of the same event", "cit": "[CLS] wherever applicable, we explore different syntactic and semantic representations of the textual content, e.g., extracting the dependency-based representation of"}
{"pre": "[CLS] we use the minimum error rate training #refr to tune the feature weights for the feature weights of the", "cit": "[CLS] os? #refra) is a recent approach that stresses a unified view between the two types of inference, optimisation and"}
{"pre": "[CLS] in the last decade, there has been a lot of work on learning semantic parsers for nlp #otherefr; #refr.", "cit": "[CLS] one exception to this work is #refr: they use a manually-constructed lexicon for hebrew in order to learn an"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] model with respect to the entire candidate set.", "cit": "[CLS] language understanding has been well studied in the context of question/answering #otherefr; #refr, query understanding #otherefr, etc. [SEP] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a text that are semantically similar to the", "cit": "[CLS] the results of using coreferential similarity are evaluated on a dataset of manually segmented chapters from a novel #otherefr"}
{"pre": "[CLS] the translation quality is evaluated by case-insensitive bleu-4 #refr. [SEP] quality measured by bleu score #refr with the bleu", "cit": "[CLS] #refr) and translation crowdsourcing. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second approach is to use a word clusters as described in #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] this assumption is based on the success of the naive bayes model when applied to supervised word-sense disambiguation #otherefr,"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for dependency parsing #refr. [SEP] grammars", "cit": "[CLS] this paper focuses on dependency parsing, which has become widely used in relation extraction #refr, machine translation #otherefr, and"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] this is applied to maximize coverage, which is similar as the ?final? in #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] event and semantic role", "cit": "[CLS] similarly, but with a different focus, open ie, #refr, deals with a large number of relations which are not"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? 2008. [SEP] ?", "cit": "[CLS] regarding the design of features for predicateargument pairs, we can use the attribute-values defined in #otherefr or tree structures"}
{"pre": "[CLS] in the context of sentiment analysis and opinion mining has been used to extract opinion expressions #otherefr; #refr. [SEP]", "cit": "[CLS] y p(y|x(u), ?) log p(y|x(u), ?) to minimize the conditional entropy of the model?s predictions on unlabeled data #refr."}
{"pre": "[CLS] the lingo grammar matrix #refr is a formalism for which is a grammar that is a grammar that is", "cit": "[CLS] eisenberg #otherefr points out that in conjoined german subordinate clauses, the verb in all the non-final clauses can be"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? 2008. [SEP] ?", "cit": "[CLS] this chaining of entailments across edits can be compared to the method presented in #refr; however, that approach assigns"}
{"pre": "[CLS] topic models have been successfully applied to a wide range of nlp tasks including word sense disambiguation #otherefr, and", "cit": "[CLS] formality average formality score, using a lexicon of formality #refr built using latent semantic analysis #otherefr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] we used charniak?s maximum entropy inspired parser and their reranker #refr for target grammar parsing, called a generative parser"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] in this work we draw on recent advances in bayesian modelling of grammar induction #otherefr; #refr to propose a"}
{"pre": "[CLS] the second approach is to use a small number of seed examples for example outputs and unsupervised learning of", "cit": "[CLS] thus glossy complements the efforts of yarowsky and other bootstrapping techniques for wsd #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] the system uses a 4-gram word-based language model trained on all monolingual data and an additional language model trained"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] a popular method of large-margin optimization is the margin-infused relaxed algorithm #otherefr, which has been shown to perform well"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to learn a translation models from a", "cit": "[CLS] our approach employs a sophisticated bayesian non-parametric prior, namely the hierarchical pitman-yor process #refr to represent backoff from larger"}
{"pre": "[CLS] #refr use a supervised sentiment classifier to detect the polarity of words and their polarity of words in a", "cit": "[CLS] supervised techniques have been proved promising and widely used in sentiment classification #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the system is based on the inprotk project at the xerox implementation of the xerox implementation of the xerox", "cit": "[CLS] the 80 is have seen renewed computational interest in coordination that has brought new efforts #otherefr\\], \\[#refr\\], \\[lesmo and"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for learning and beam-search decoding #otherefr;", "cit": "[CLS] the texts were first automatically segmented and tokenized10 and then they were part-of-speech tagged by tnt tagger #refr, which"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set.", "cit": "[CLS] #refr perform inference over latent synchronous derivation trees under a nonparametric bayesian model with a gibbs sampler. [SEP] [PAD]"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] the co-occurrence r lation can also be based on distance in a bitext space, which is a more general"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] in particular, we show that the reranking parser described in #refr improves performance of the parser on brown to"}
{"pre": "[CLS] the first is the case for the automatic acquisition of subcategorization frames from corpora #refr, which is the web", "cit": "[CLS] #refr, but the difficulty of acquisition means that the validity of utilizing lexical probabilities of the type assumed here"}
{"pre": "[CLS] in the second international chinese word segmentation bakeoff #otherefr; #refr and the same task as the best results are", "cit": "[CLS] a very different, but interesting, approach is taken in #refr who use methods from unsupervised word alignment for unsupervised"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for parsing #refr. [SEP] grammars with", "cit": "[CLS] notable exceptions are plaehn #otherefr, where discontinuous phrase structure grammar parsing is explored, and #refrb), where nonprojective dependency structures"}
{"pre": "[CLS] the grammar is a grammar that is a grammar that is a grammar that is a given a grammar", "cit": "[CLS] nearly all existing unification grammars of this kind use either term unification #otherefr and #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr proposed a method for learning subjective language pairs from the target language and used to predict the target", "cit": "[CLS] topic models built on the foundations of lda are appealing for sentiment analysis because the learned topics can cluster"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? ? ?", "cit": "[CLS] nevertheless, many such imperfect rules can be learned and combined to yield useful kb completions, as demonstrated in particular"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with a probability distribution", "cit": "[CLS] it is a useful measure to assess how certain the parser is about the best analysis, e.g. to measure"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? 2008. [SEP] ?", "cit": "[CLS] while linguists have studied apposition in detail #otherefr, most apposition extraction has been within other tasks, such as coreference"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a task of identifying the degree of semantic relations between words", "cit": "[CLS] recently, the idea of using fine-grained paraphrasing verbs for nc semantics has been gaining popularity #otherefr #refr. [SEP] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders and predict the polarity of subjective words in their polarity", "cit": "[CLS] for process #otherefr; #refr or multipoint scale categories #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approaches to sentiment analysis rely on reviews with the sentiment analysis on the sentiment analysis on", "cit": "[CLS] for our task, we use an implementation of tree kernels for syntactic parse trees #refr that is built on"}
{"pre": "[CLS] the first is the annotation tool which was the annotation tool for the annotation tool #refr. [SEP] project in", "cit": "[CLS] in #refr it is proposed an algorithm for anaphor resolution which is a modified and extended version of that"}
{"pre": "[CLS] the grammar is a grammar that is a parsing model that of #refr and a probability distribution over a", "cit": "[CLS] #refr and 1994. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second approach is to use a small number of manually annotated corpora such as the same data set", "cit": "[CLS] the second approach takes the universal rules of #refr but rather than estimating a probabilistic model with these rules,"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] algorithm with the best derivation of the best derivation", "cit": "[CLS] for example, probabilities were defined over grammar rules in probabilistic cfg #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] semi-supervised learning has been applied to polarity lexicon induction #refr, and sentiment classification at the sentence level #otherefr. [SEP]"}
{"pre": "[CLS] we use the berkeley parser #refr to obtain the syntactic parse trees for each sentence and the english and", "cit": "[CLS] these alignments are employed by standard syntactic rule extraction algorithms, for example, the ghkm algorithm of #refr. [SEP] [PAD]"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with the one of", "cit": "[CLS] although several systems were trained on the bultreebank treebank during the conll-x 2006 shared task #refr and after it,"}
{"pre": "[CLS] the model is trained using the moses decoder #refr. [SEP] score #refr with the final translation quality measured by", "cit": "[CLS] this concept of alignment has been also used for tasks like authomatic vocabulary derivation and corpus alignment #refr. [SEP]"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] #refr make use of three different sentiment detection websites to label twitter data, while davidov et al#otherefr use twitter"}
{"pre": "[CLS] the system is based on the moses toolkit #otherefr and the moses toolkit #refr. [SEP] test set for the", "cit": "[CLS] in the latter case, the rescoring incorporates the same translation features, except for a better target language model #otherefr;"}
{"pre": "[CLS] the first is the automatic evaluation of discourse structure of discourse relations #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] even for relatively frequent ncs that occur ten or more times in the bnc, static english dictionaries give only"}
{"pre": "[CLS] the second approach is to use a small number of approximate inference rules #otherefr; #refr. [SEP] model to handle", "cit": "[CLS] pointwise mutual information #otherefr; #refrb) and it is a measure of the mutual dependence of two strings and reflects"}
{"pre": "[CLS] the second approach is to use a small number of seed examples for example outputs and automatic acquisition of", "cit": "[CLS] here, we simply use the vsm and cosine metric ?#refr? to obtain the similarity. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the polarity of a sentence, a sentence, a document into a document", "cit": "[CLS] these include semantic parsing #otherefr; #refr question answering #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to learn a translation probabilities from", "cit": "[CLS] another related field is clause identification #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr proposed a method for identifying word pairs of words in a target word in a target word in", "cit": "[CLS] for this purpose, we store exact counts of all the words in a hash table and use count-min #otherefr"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with a probability distribution", "cit": "[CLS] grammar transformation techniques such as linguistically inspired non-terminal annotations #otherefr; #refrb) and latent variable grammars #otherefr have increased the"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? 2008. [SEP] ?", "cit": "[CLS] vn was also substantially extended #otherefr using the levin classes extension proposed in #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the automatic evaluation of discourse structure of discourse structure of discourse structure of discourse relations #refr.", "cit": "[CLS] therefore pinyin to character conversion is highly ambigurous and is a active research topic #otherefr, #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a bootstrapping method to identify opinion holders and extract opinion holders and extract opinion expressions from the", "cit": "[CLS] the closest works are riloff et al #refr and pang et al #otherefr?s work. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr proposed a method for learning a word segmentation based on a word clustering words from a target language", "cit": "[CLS] #refr study the task of unsupervised morphological segmentation of multiple languages. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the system is based on the inprotk toolkit for the paradise system described in #refr. [SEP] system, which is", "cit": "[CLS] it can be recovered in the future by including the domain specific knowledge in the dynamic configuration of the"}
{"pre": "[CLS] the first is the case for the automatic acquisition of subcategorization frames from the penn treebank #otherefr; #refr. [SEP]", "cit": "[CLS] the knowledge-rich approach needs mechanisms to take into account errors within a rigid system of rules, and thus different"}
{"pre": "[CLS] the most popular approach to word alignment is the hmm alignment model #refr. [SEP] model is based on the", "cit": "[CLS] there exists a large body of literature in transliteration, especially in the bilingual setting, well summarized by #refr. [SEP]"}
{"pre": "[CLS] #refr proposed a method for extracting parallel subsentential fragments from comparable corpora. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] 5000 film reviews #refr to create a multilingual film review corpus.6 the results for predicting sentiment in german documents"}
{"pre": "[CLS] #refr proposed a method for learning a probabilistic model that learns latent dirichlet allocation (lda) as a latent variable", "cit": "[CLS] dual decomposition #otherefr and lagrangian relaxation in general are often used for solving joint inference problems which are decomposable"}
{"pre": "[CLS] in the literature, supervised learning methods have been applied to the task of learning a small number of learning", "cit": "[CLS] many machine learning methods have been proposed to address this problem, e.g., supervised learning algorithms #otherefr, and unsupervised learning"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] #refr successfully apply decipherment to learn a domain specific translation lexicon from monolingual data to improve out-ofdomain machine translation."}
{"pre": "[CLS] the first is the automatic evaluation of the automatic metric of the automatic evaluation of the automatic evaluation of", "cit": "[CLS] earlier studies on the detection of light verb constructions generally take syntactic information as a starting point #otherefr; #refr,"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] chinese zero anaphora identification and resolution have been studied in a machine learning frameworking in #refr and #otherefr. [SEP]"}
{"pre": "[CLS] we use the stanford chinese word segmenter #refr to tokenize the english pos tagger #otherefr for chinese and chinese", "cit": "[CLS] we also compare neural lms to ngram using pairwise bootstrap #refr: ?+? means statistically significant improvement and ??? means"}
{"pre": "[CLS] the most popular approach to translation lexicons from the source language side #otherefr; #refr is to target language side", "cit": "[CLS] 1ldc2002e18 #otherefrt08 hk hansards (500,000 sentences) 2http://www.statmt.org/wmt07/shared-task.html for both the tasks, the word alignment were trained by giza++ in"}
{"pre": "[CLS] the system is based on the stanford parser #refr and the stanford parser #otherefr. [SEP] system #otherefr. [SEP] system", "cit": "[CLS] but following ng and #refr?s best link approach. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the process of annotating the word vectors of words in a window around the words in", "cit": "[CLS] we cluster this graph using the chinese whispers graph clustering algorithm #refr, which finds the number of clusters automatically,"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] of a text in", "cit": "[CLS] kruijff-korbayova? and kruijff #otherefr describe a method where a rich discourselevel annotation is used to investigate information structure, while"}
{"pre": "[CLS] the second approach is to use a word sense disambiguation algorithm to find the most probable word as described", "cit": "[CLS] due to the recent availability of large text corpora, various tatistical approaches have been tried including using 1) parallel"}
{"pre": "[CLS] the first is the stanford parser #refr and then applied to obtain the syntactic parse trees of the parse", "cit": "[CLS] up to now, wordnet has been one of the most frequently used sources of semantic knowledge for the coreference"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] instead of assigning head and deprel in a single step, some systems use a two-stage approach for attaching and"}
{"pre": "[CLS] the task of identifying the semantic textual similarity of a text is a document #otherefr; #refr. [SEP] of a", "cit": "[CLS] #refr propose structured multilabel classification as opposed to sequential labelling for dealing with nested, discontinuous, and overlapping nes. [SEP]"}
{"pre": "[CLS] #refr use a probabilistic model to model the syntactic structure of the syntactic structure of the sentence in a", "cit": "[CLS] discourse structure has been shown to have an important role in many natural language applications, such as text summarization"}
{"pre": "[CLS] the grammar is a grammar that is a parsing system for the grammar that is a grammar that is", "cit": "[CLS] an efficient, broad-coverage morphology was also available for japanese #refr and was integrated into the grammar. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to this problem is to use a small number of seed examples #otherefr; #refr. [SEP]", "cit": "[CLS] in recent work, a number of researchers have cast this problem as a tagging problem and have applied various"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] statistical significance in bleu score differences was tested by paired bootstrap re-sampling #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the process of annotating the sentence planning problem of the user to be a given by", "cit": "[CLS] the experiment was designed to make it possible to apply the paradise evaluation framework #otherefr, which integrates and unifies"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with a probability distribution", "cit": "[CLS] indeed, syntactic features have long been an extremely useful source of information for relation extraction systems #refr. [SEP] [PAD]"}
{"pre": "[CLS] in the context of machine translation, itg has been explored for a variety of nlp tasks such as sentiment", "cit": "[CLS] previous work using reinforcement learning for natural language processing tasks #otherefr; #refr inspired us to use a similar approach"}
{"pre": "[CLS] the most common approach to this problem is to use a word similarity model #otherefr; #refr. [SEP] model to", "cit": "[CLS] in terms of the learning framework, s2net is closely related to several neural network based approaches, including autoencoders #otherefr;"}
{"pre": "[CLS] the first is the case for the generation of referring expressions from the generation of referring expressions that we", "cit": "[CLS] the proof builds on #refr construction showing that lfg generation produces contextfree languages. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the moses toolkit #refr to train the weights for the feature weights for the log-linear model with", "cit": "[CLS] #refr proposed a shift-reduce algorithm to add btg constraints to phrase-based models. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the paraphrase collection has been shown to be effective in various natural language processing tasks such as paraphrase detection", "cit": "[CLS] these include inference rule discovery for question-answering and information retrieval #otherefr and identification #refr, machine translation evaluation #otherefr, textual"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] #refr improved smt performance by online adaptation of scaling factors #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of identifying the overall event descriptions of discourse structure in text is a text #refr. [SEP] [SEP]", "cit": "[CLS] other related work has made use of discourse connectives or discourse taggers #otherefr; #refr, but we do not because"}
{"pre": "[CLS] #refr proposed a method for identifying word polarity of subjective words and their polarity in a target words in", "cit": "[CLS] there are also research work on automatically classifying movie or product reviews as positive or negative #otherefr; #refr. [SEP]"}
{"pre": "[CLS] in the context of sentiment analysis and opinion mining has been used to improve sentiment classification of sentiment analysis", "cit": "[CLS] we augment each labeled target instance xj with the label assigned by the source domain classifier #refr. [SEP] [PAD]"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] approaches to combination generally either select one of the hypotheses produced by the different systems combined #otherefr; #refr. [SEP]"}
{"pre": "[CLS] the second approach is to use a small number of approximate inference rules #otherefr; #refr. [SEP] model to handle", "cit": "[CLS] the learning of morphological paradigms is not novel as there has already been existing work in this area such"}
{"pre": "[CLS] topic models have been successfully applied to a wide range of nlp tasks including word sense disambiguation #otherefr, and", "cit": "[CLS] both of these algorithms are applied to text following some preprocessing including tokenization, conversion to lowercase and the application"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the input is to be", "cit": "[CLS] 1993; chang et al, 1992; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the case of dependency parsing, previous work has shown that it is possible to improve the performance of", "cit": "[CLS] unsupervised learning of natural language has received a lot of attention in the last years, e.g., klein and manning"}
{"pre": "[CLS] the approach of #refr is based on the assumption that the assumption of the assumption that the assumption of", "cit": "[CLS] a capability for describing a larger domain of locality (schabes, abeill~, and #refr . [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the semantic relations between nominals of noun phrases and the semantic relations", "cit": "[CLS] for instance, entailment relations are crucial in information extraction and qa #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the maximum entropy model #otherefr, which is based on the conditional probability of a", "cit": "[CLS] although we do not know of any work on active learning for nlg, previous work has used active learning"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] this is similar to #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking approaches #otherefr; #refr to learning for dependency", "cit": "[CLS] previous studies #refr show that the accuracies of complete trees are about 40% for english and about 35% for"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for learning and beam-search decoding #otherefr;", "cit": "[CLS] a typical approach in graph-based dependency parsing has been to assume a factorized model, where local features are used"}
{"pre": "[CLS] #refr use a similar approach to identify the polarity of words in a sentence and their polarity in a", "cit": "[CLS] recently, #refr experimented with automatic response generation in social media. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] whereas we have concentrated on determinism in this paper, one can also introduce a limited degree of nondeterminism and"}
{"pre": "[CLS] #refr proposed a method for learning a word sense disambiguation which uses a word segmentation and a word segmentation", "cit": "[CLS] for example, to automatically generate subtitles for television programs; the transcripts cannot usually be used verbatim due to the"}
{"pre": "[CLS] the first is the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] this may lead to a concern about getting the actual semantic chunks back, but #refr have shown that it"}
{"pre": "[CLS] the task of identifying the semantic relations between nominals of noun phrases and their arguments are also key to", "cit": "[CLS] #refr analyze how component subevents of the domain are linked together and discuss the factors which contribute to the"}
{"pre": "[CLS] the bionlp shared task (bionlp-st, hereafter) series has been instrumental in the bionlp 2009 shared task (st) events from", "cit": "[CLS] #refr worked on news and introduced an annotation scheme for temporal expressions, and a method for using explicit tempo-"}
{"pre": "[CLS] id institution balagur yandex school of data analysis #otherefr commercial-1,2,3 anonymized commercial systems online-a,b,g anonymized commercial systems online-a,b,g anonymized", "cit": "[CLS] mt quality in the medical translation task is evaluated using automatic evaluation metrics: bleu #otherefr, and cder #refr. [SEP]"}
{"pre": "[CLS] the first is the case for the case of the grammar development of the language model is a translation", "cit": "[CLS] and research into methods for time normalization has been growing since the ace1 and tempeval #otherefr; #refr challenges began"}
{"pre": "[CLS] the first is the task of identifying the event extraction of event descriptions of event descriptions of event descriptions", "cit": "[CLS] for event coreference, we follow the approach to entity coreference detailed in #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] our individual models are two state-of-the-art systems: a hiero model #otherefr; mi and #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] an unsupervised setup also exists; methods for the unsupervised problem typically rely on language models and linguistic/discourse constraints #refr."}
{"pre": "[CLS] the translation quality is measured by bleu #refr. [SEP] [SEP] [SEP] = 4. [SEP] = 4. [SEP] = ?", "cit": "[CLS] translation hypotheses are scored according to the following features: ? 4 phrase-table scores: phrasal translation probabilites with kneser-ney smoothing"}
{"pre": "[CLS] the first is the polarity of a word is usually analyzed by using the polarity of words and then", "cit": "[CLS] for instance, a tweet that contains a sad face likely contains a negative polarity #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of identifying the overall sentiment in text has been the focus of the focus of the focus", "cit": "[CLS] here the systems investigated the syntax path between the event trigger and a cue word (which came from a"}
{"pre": "[CLS] #refr proposed a method for learning a word sense disambiguation which is a word sense disambiguation method to extract", "cit": "[CLS] while data sparsity is a common problem of many nlp tasks, it is much more severe for sentence compression,"}
{"pre": "[CLS] the most common approach to word sense disambiguation is to use a word similarity measure which is to automatically", "cit": "[CLS] in addition, indirect relations have also been used in other tasks, such as paraphrase acquisition from bilingual parallel corpora"}
{"pre": "[CLS] the feature weights were tuned using mert #refr on the development set. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] we word-aligned the training data using giza++ followed by link deletion #refr, and then parsed the chinese sentences using"}
{"pre": "[CLS] #refr proposed a method for extracting phrases from a parallel corpus of 4,000 pairs of sentences from the same", "cit": "[CLS] e&p #refr introduce a structured vector space model which uses syntactic dependencies to model the selectional preferences of words."}
{"pre": "[CLS] the most common approach to paraphrase generation is to use a lexicon to automatically extracted from corpora #otherefr; #refr.", "cit": "[CLS] a related approach has been proposed by #refr, who integrate selectional preferences into the compositional picture. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be np-hard #refr. [SEP] s", "cit": "[CLS] our team employed variants of directl+ in the previous editions of the shared task on transliteration #refr. [SEP] [PAD]"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] shared", "cit": "[CLS] at the heart of our system1 is an off-the-shelf 1nlp.stanford.edu/software/eventparser.shtml dependency parser, mstparser2 #otherefr; #refr, extended with event extraction-specific"}
{"pre": "[CLS] in this paper, we propose a new approach to use multiple translation of the same sentence as in #refr.", "cit": "[CLS] asterisked results are significantly better than the baseline (p ? 0.05) using 1,000 iterations of paired bootstrap re-sampling #refr."}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within the meaning representation of the meaning of the", "cit": "[CLS] #refr learn lexical simplification rules from the edit histories of wikipedia simple articles. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr proposed a method for identifying the polarity of a word in a word in a target word in", "cit": "[CLS] the stateof-the-art stanford pos tagger #refr improves on the baseline, obtaining an accuracy of 0.8. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] default parameters were used for all experiments except for the number of iterations for giza++ #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the translation quality is measured by bleu #refr. [SEP] score #refr with the bleu score of the translation quality.", "cit": "[CLS] this particular scenario has been explained in #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the sentence in", "cit": "[CLS] in the context of multiple-document summarization, heuristics have also been used to remove parenthetical information #otherefr; #refr. [SEP] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a text that is a", "cit": "[CLS] in this-paper, we examine the ~ssue of raterannotator agreement by comparing the agreement rate of human annotators on a"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr. [SEP]", "cit": "[CLS] current systems attain approximately 94% precision and recall on the chunking task #otherefr; #refr, so the 3collins independently reports"}
{"pre": "[CLS] #refr use a similar approach to compute the polarity of subjective words in the polarity of words in a", "cit": "[CLS] for automated extraction of patterns, we followed the pattern definitions given in #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second dataset is the same as the use of the same as the same as the same as", "cit": "[CLS] while wikipedia has recently been considered a valid alternative #refr, it is mainly focused on covering named entities and,"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to learn a translation probabilities from", "cit": "[CLS] these n-best lists are then rescored using a log-linear combination of feature functions #refr: e? ? argmaxe pr(e) ?1"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr.", "cit": "[CLS] because general-purpose ilp solvers are proprietary and do not fully exploit the structure of the problem, we turn to"}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders and predict the polarity of words in a sentence and", "cit": "[CLS] we also obtain significant higher pyramid f1 score on the blog task as compared to the system of #refr."}
{"pre": "[CLS] #refr used a similar approach to compute the similarity between two words in a sentence. [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] a user study #refr showed that humans can utilize linguistic knowledge at various levels to improve the sr output"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with a probability distribution", "cit": "[CLS] we extend carreras #otherefr graphbased model with factors involving three edges similar to that of #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the second approach is to use a maximum entropy model #otherefr; #refr. [SEP] model for the probability distribution over", "cit": "[CLS] recently, #refr, proposed a simple perceptron-based classifier applied from left to right but augmented with a lookahead mechanism that"}
{"pre": "[CLS] the first is the one of the most popular methods for building thesauri for word sense disambiguation which have", "cit": "[CLS] knowledge-based measures are based on thesauri, semantic networks, taxonomies, or other knowledge sources #otherefr; #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a text that are semantically", "cit": "[CLS] se?#refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the sentence planning problem that the user is a text from a", "cit": "[CLS] in recent years, there has been a growing interest among researchers in methods for incremental natural language understanding #otherefr;"}
{"pre": "[CLS] the most common approach to coreference resolution is to use a conditional random field #otherefr; #refr. [SEP] model to", "cit": "[CLS] for the named entity features, we used a fairly standard feature set, similar to those described in #refr. [SEP]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the training data is to", "cit": "[CLS] following #refr, the bky algorithm uses em to estimate probabilities on symbols that are automatically augmented with latent annotations,"}
{"pre": "[CLS] the most common approach to paraphrase generation is to use a lexicon to paraphrase database into a target language", "cit": "[CLS] we used treetagger and mstparser #refr for english, juman #otherefr for chinese. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for each sentence in the training corpus.", "cit": "[CLS] this is different from our situation, as there exist high performance parsers for czech #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the mentions of a text with a text that is a text", "cit": "[CLS] previous systems have turned either to ontologies #refr or distributional semantics #otherefr to help solve these errors. [SEP] [PAD]"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking approaches #otherefr; #refr to learning for dependency", "cit": "[CLS] at this point, we have many different parsing models that reach and even surpass 90% dependency or constituency accuracy"}
{"pre": "[CLS] #refr proposed a method for learning a word alignment. [SEP] model that learns to predict the probability of a", "cit": "[CLS] sentence compression is valuable in many applications, for example when displaying texts on small screens #otherefr, in subtitle generation"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a text with a text corpus and then", "cit": "[CLS] so we have also used the stanford pos tagger #refr to tag these transcripts before calculating the discriminative tfidf"}
{"pre": "[CLS] the first is the task of word sense disambiguation task in the semeval 2007 task #refr. [SEP] task is", "cit": "[CLS] our motivation for the systems entered in the sts task #refr was to model the contribu- . [SEP] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the event descriptions of the bionlp?09 shared task on event extraction #refr.", "cit": "[CLS] this paper describes the extensions and adaptations needed for applying our anaphora resolution system (mu?#refr to pronoun resolution in"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] others have formulated the problem as an integer linear program #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best results for the best", "cit": "[CLS] despite considerable research effort, progress in fully unsupervised pos induction has been slow and modern systems barely improve over"}
{"pre": "[CLS] we use the moses toolkit #refr to train the weights for the feature weights for each model with the", "cit": "[CLS] dp beam search for phrase-based smt was described by #refr, extending earlier work on word-based smt #otherefr. [SEP] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders and extract opinion expressions in opinion expressions in opinion holders", "cit": "[CLS] a similar approach is taken by #refr: a classifier is learnt that distinguishes between polar and neutral sentences, based"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] this explicitly avoids the degenerate solutions of maximum likelihood estimation #refr, without resort to the heuristic estimator of koehn"}
{"pre": "[CLS] #refr use a similar approach to compute sentence importance. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] semantic word vector spaces are at the core of many useful natural language applications such as search query expansions"}
{"pre": "[CLS] the first is the task of identifying the event extraction of event descriptions of event descriptions of event extraction", "cit": "[CLS] we draw inspiration from this idea in our preemptive subtree generation approach; however, while we extract all possible subtrees"}
{"pre": "[CLS] the generation of referring expressions is a useful way of measuring the performance of a generation of referring to", "cit": "[CLS] strategies that have been introduced to reduce the search space in integrated systems include greedy/incremental search algorithms #otherefr, constructing"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr. [SEP]", "cit": "[CLS] although joint modeling has shown to be effective in various nlp and computer vision applications #otherefr; #refr, our choice"}
{"pre": "[CLS] the first is the task of word sense disambiguation which is based on the idea of a word sense", "cit": "[CLS] #refr use the hownet resource within the split-merge pcfg framework #otherefr for chinese parsing: they use the firstsense heuristic"}
{"pre": "[CLS] the translation quality is measured by bleu #refr. [SEP] score #refr with the bleu score of the translation quality.", "cit": "[CLS] we also report on applying factored translation models #refr for english-to-arabic translation. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] a number of alignment techniques have been proposed, varying from statistical methods #refr to lexical methods #otherefr. [SEP] [PAD]"}
{"pre": "[CLS] the grammar we are extracted from the treebank using the stanford parser #refr. [SEP] parser #refr with the self-trained", "cit": "[CLS] that is, an assumption of full statistical dependence #refr, rather than the more common full independence, is made3 when"}
{"pre": "[CLS] the most common approach to word sense disambiguation is to use a word as a word sense disambiguation task,", "cit": "[CLS] we model the task as a word sense disambiguation problem applying the personalized pagerank algorithm proposed by #refr as"}
{"pre": "[CLS] the second approach is to use a discriminative model #otherefr; #refr. [SEP] model to predict the probability of the", "cit": "[CLS] system p r f #otherefra) - - 97.85 #refr 97.46 98.29 97.87 #otherefr - - 98.17 ours (w/o transition"}
{"pre": "[CLS] in this paper, we propose a method for extracting parallel bilingual news articles from monolingual corpora #refr. [SEP] [SEP]", "cit": "[CLS] an approach of this kind has been proposed for sentence paraphrasing #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in recent years, there has been a growing interest in the task of generating referring expressions, reflected in the", "cit": "[CLS] although machine learning approaches have achieved success in many areas of natural language processing, researchers have only recently begun"}
{"pre": "[CLS] the most common approach to parsing is to use a log-linear parsing model and maximise the conditional random field", "cit": "[CLS] it is a maximum-spanning-tree5 parser (mcdonald et al #refr that searches for the best-scoring tree using a chart-based dynamic"}
{"pre": "[CLS] the first is the automatic metric of text with a word is analyzed by #refr. [SEP] quality estimation method", "cit": "[CLS] for example, in english part-of-speech tagging, the accuracy of the stanford tagger #otherefr falls from 97% on wall street"}
{"pre": "[CLS] the translation model is a phrase-based decoder that a hierarchical phrase-based translation model #otherefr and a hierarchical phrase-based model", "cit": "[CLS] works that apply the ttt model include #refr and zhang et al #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford ner tagger #refr to train the word segmenter #otherefr to obtain the word alignments. [SEP]", "cit": "[CLS] however, we assume that there are enough parallel data available to perform meta-parameter tuning by minimum error rate training"}
{"pre": "[CLS] in the last years, the availability of monolingual parallel corpora has emerged as a comparable corpus to the same", "cit": "[CLS] in our work, we train the mstparser4 #refr on the penn treebank wall street journal (wsj) corpus, and use"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] to test the effect of improved alignment accuracy, we use the discriminative alignment method of #refr as implemented in"}
{"pre": "[CLS] the task of identifying the semantic relations between nominals of noun phrases and their arguments are also key to", "cit": "[CLS] #refr defines an informationtheoretic measure of the association between a verb and nominal wordnet classes: selectional association. [SEP] [PAD]"}
{"pre": "[CLS] the system is based on the standard approach of #refr and the statistical machine translation system #otherefr. [SEP] set", "cit": "[CLS] #refr present langid.py, an off-the-shelf langid system that utilizes the ld feature set. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the context of machine learning techniques have been used for this task #otherefr; #refr. [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] unfortunately, this technique cannot be applied to our problem due to the lack of discourse redundancy within item descriptions."}
{"pre": "[CLS] the first is the task of identifying the event descriptions of the bionlp?09 shared task on event extraction #refr.", "cit": "[CLS] but the most common approach to coreference resolution #otherefr; ng and #refr, etc.) is to use a single classifier"}
{"pre": "[CLS] the second approach is to use a small number of seed examples for example outputs #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] computational morphology #otherefr has begun to play a prominent role in machine translation and speech recognition for morphologically rich"}
{"pre": "[CLS] the system is based on the inprotk toolkit for the best performing system described in #refr. [SEP] system, which", "cit": "[CLS] nlp methods are applicable to such data #refr, and we can hope that ultimately, researchers working on archived bilingual"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of semantic similarity to be", "cit": "[CLS] in addition, new research on the topic has explored the translation of sentences into many languages #refr, as well"}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders and predict the polarity of words in a sentence. [SEP]", "cit": "[CLS] we obtained the prior subjectivity and polarity information from subjectivity lexicon of about 8,000 words used in #refr2. [SEP]"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a text with a text corpus consisting of", "cit": "[CLS] in order to generate quabs automatically, documents identified from ferret?s automatic q/a system are first submitted to a topic"}
{"pre": "[CLS] #refr use a similar approach to identify the polarity of words in a sentence. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] bridging the gap between metaphor identification and interpretation, #refr proposed an unsupervised system to learn sourcetarget domain mappings. [SEP]"}
{"pre": "[CLS] #refr proposed a method for learning a word sense disambiguation which is a word sense disambiguation method to identify", "cit": "[CLS] iii, 2011a), finding frequent items (like n-grams) #refr, building streaming language models #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with a probability distribution", "cit": "[CLS] a rich literature on dop has emerged since, yielding stateof-the-art results on the penn treebank benchmark test #otherefr; #refr"}
{"pre": "[CLS] in the literature, supervised learning methods have been applied to unsupervised learning of natural language processing #otherefr; #refr. [SEP]", "cit": "[CLS] several graph-based learning techniques have recently been developed and applied to nlp problems: minimum cuts #otherefr, random walks #refr,"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] #refr introduced a system that performs both tasks in a single run without any document level pre-filtering. [SEP] [PAD]"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] in recent years, there have been an increasing number of studies #otherefr; #refr using crowdsourcing for data annotation. [SEP]"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be np-hard #refr. [SEP] [SEP]", "cit": "[CLS] although the resulting set of links may still be relatively accurate, we can symmetrize by combining it with the"}
{"pre": "[CLS] the first group (asr features) includes a variety of applications, such as speech recognition #otherefr, and question answering #refr,", "cit": "[CLS] such action may include verifying the user is input, reprompting for fresh input, or, in cases where many errors"}
{"pre": "[CLS] the first is the task of coreference resolution #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] ? ? ? ?", "cit": "[CLS] kernel functions make it possible to capture the similarity between structures without explicitly enumerating all the substructures, and have"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of meaning that are not", "cit": "[CLS] a lot of work has been done in the nlp community on clustering words according to their meaning in"}
{"pre": "[CLS] the second approach is to use a small number of seed examples for example outputs #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] for example, #refr, collins #otherefr proposed statistical parsing models which incorporated lexical/semantic information. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best results for the best", "cit": "[CLS] the semi-supervised approach by kohonen et al. #otherefr was based on morfessor baseline, the simplest of the morfessor methods"}
{"pre": "[CLS] the first is the task of identifying the semantic relations between nominals of noun phrases and semantic relations #otherefr;", "cit": "[CLS] an emerging line of parsing research capitalizes on the advances of compositional distributional semantics #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] some recent works tried to model the sentiment in tweets #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] for other methods using non-parallel corpora, see also #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] several recent works #otherefr; #refr explore using an external world context as a supervision signal for semantic interpretation. [SEP]"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a task of identifying the degree of semantic relations between words", "cit": "[CLS] names of authors are listed alphabetically. prepositional phrase attachment #otherefr; #refr, co-reference resolution #otherefr, semantic information is a necessary"}
{"pre": "[CLS] the most common approach to paraphrase generation is to paraphrase generation from comparable corpora #otherefr; #refr. [SEP] tables by", "cit": "[CLS] for ppdb, we formulate our paraphrase collection as a weighted synchronous context-free grammar #otherefr; #refr 1freely available at http://paraphrase.org."}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the sentence in", "cit": "[CLS] today, mt evaluation is an active field of research, and modern metrics perform analysis at various levels, e.g., lexical"}
{"pre": "[CLS] we use the averaged perceptron algorithm #refr to train the weights for the weights for each training and the", "cit": "[CLS] note that generative hybrids are the norm in smt, where translation scores are provided by a discriminative combination of"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] sentence pair of a", "cit": "[CLS] in 2004, conroy #otherefr tested maximal marginal relevance #refr as well as qr decomposition. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a text that is a", "cit": "[CLS] over the last several of years, mechanical turk, introduced by amazon as ?artificial artificial intelligence?, has been used successfully"}
{"pre": "[CLS] the system is based on the inprotk toolkit for the best performing system described in #refr. [SEP] system, which", "cit": "[CLS] in this paper we discuss how we allply discourse predictions along with non context-based predictions to the problem of"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] others were derived from corpora, including subjective nouns learned from unannotated data using bootstrapping #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in this paper, we propose a method for learning semantic similarity based on the similarity of the similarity between", "cit": "[CLS] unlike ulc, #refr; lo and wu #otherefra) approach the weight estimation problem by maximum correlation training which directly optimize"}
{"pre": "[CLS] the first is the case for the generation of referring expressions from the dialogue acts of the incremental processing", "cit": "[CLS] such a system is reminiscent of earlier work in rulebased generation that implements an interactive or revision-based feedback between"}
{"pre": "[CLS] the task of identifying the overall sentence pairs in text is a text #refr. [SEP] of the same event", "cit": "[CLS] for this we have used the c&c named entity recogniser #refr, which is run on pos-tagged and chunked documents"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for each sentence with dependency trees for", "cit": "[CLS] to achieve this objective, for each sentence in the training data we extract the minimal set of synchronous rules"}
{"pre": "[CLS] we use the berkeley parser #refr to obtain the syntactic parse trees for each sentence and the english and", "cit": "[CLS] permutation parsers have been used to implement hierarchical re-ordering models #otherefr and to enforce inversion transduction grammar (itg) constraints"}
{"pre": "[CLS] we use the averaged perceptron algorithm #refr to train the parameters of the averaged perceptron algorithm #otherefr. [SEP] [SEP]", "cit": "[CLS] online structured learning algorithms such as the structured perceptron #otherefr and k-best mira #refr have become more and more"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] we trained the n-gram language model used by the last four systems on the simple side of the training"}
{"pre": "[CLS] the second approach is to use a small number of approximate inference rules #otherefr; #refr. [SEP] model to handle", "cit": "[CLS] the popular method is to regard word segmentation as a sequence labeling problems #otherefr; #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr. [SEP]", "cit": "[CLS] supervised methods early work on normalization focused on labeled sms datasets, using approaches such as noisy-channel modeling #otherefr; #refr."}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a task of identifying the degree of semantic relations between words", "cit": "[CLS] the framenet project #refr is an attempt to realize the semantic field. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the system is based on the moses toolkit #otherefr and the standard phrase-based system described in #refr. [SEP] test", "cit": "[CLS] we converted the lan- 3we used 5 iterations of model 1, 4 iterations of hmm #refr and 4 iterations"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for learning and beam-search decoding #otherefr;", "cit": "[CLS] the graphbased method views the problem as finding an optimal tree from a fully-connected directed graph #refr, while the"}
{"pre": "[CLS] the parser is trained on the penn treebank corpus and tested on the penn treebank corpus #refr. [SEP] test", "cit": "[CLS] 1998), word sense disambiguation #otherefr and recently in parsing #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the generation system is based on the inprotk project #refr. [SEP] score for the number of the generation of", "cit": "[CLS] we follow #refr, who also use tag, in their commitment to flat semantics. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford ner tagger #refr to train the training data with the training data and test data", "cit": "[CLS] this approach bears some similarity to the adaptation methods standard for the setting where labelled data is available for"}
{"pre": "[CLS] the language grounding problem has assumed several guises in the literature such as speech recognition and machine translation #otherefr,", "cit": "[CLS] in a number of works, #refr describes a finite-state morphological analyzer of modern standard arabic which handles both inflectional"}
{"pre": "[CLS] we use the berkeley parser #refr to obtain the syntactic parse trees for parse trees for english and the", "cit": "[CLS] indeed, researchers have shown that gigantic language models are key to state-ofthe-art performance #refr, and the ability of phrase-based"}
{"pre": "[CLS] the most common approach to paraphrase generation is to use a lexicon to paraphrase generation #otherefr; #refr. [SEP] words", "cit": "[CLS] the approach is the same as the method reported by #refr except for term weights and similarities. [SEP] [PAD]"}
{"pre": "[CLS] in the context of machine translation, itg has been explored for a variety of nlp tasks, including pos tagging", "cit": "[CLS] the minimum error rate training (mert) #refr procedure is used for tuning the model parameters of the translation system."}
{"pre": "[CLS] #refr use a model to identify stances in a coherent discourse. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] for unsupervised methods, maximal marginal relevance #otherefr and #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr proposed a method for learning a word segmentation and a word segmentation model that uses a word segmentation", "cit": "[CLS] one problem with such a model is its tendency to distribute probabilities equally and the resulting inability to model"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to learn a translation hypotheses and a", "cit": "[CLS] the representations are able to carry similarity and task dependent information, e.g. sentiment, paraphrase or dialogue labels, significantly beyond"}
{"pre": "[CLS] the first is the task of coreference resolution #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] ? the typed-similarity [SEP]", "cit": "[CLS] one of the earliest methods for using predicate-argument frequencies in pronoun resolution is that of #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the training data is to", "cit": "[CLS] resolving this dilemma has been the topic of several studies in pitch accent placement #otherefr; #refr and in prosodic"}
{"pre": "[CLS] the most popular approach to word alignment is the ibm models #otherefr; #refr. [SEP] model 1 and the hmm", "cit": "[CLS] we evaluate our approach by comparing translation quality, as evaluated by the ibm-bleu #refr metric on the nist chinese-to-english"}
{"pre": "[CLS] #refr proposed a method for learning a semantic composition of distributional similarity to identify the similarity between words in", "cit": "[CLS] functions for simple phrases directly map distributional vectors of words to distributional vectors for the phrases #otherefr; #refr. [SEP]"}
{"pre": "[CLS] the task of identifying positive and negative sentiments and negative sentiments from a passage, #otherefr; #refr. [SEP] [SEP] [SEP]", "cit": "[CLS] to identify these subcategories, we run latent dirichlet analysis #otherefr; brody and #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in this paper, we propose a discriminative model for training using a discriminative model based on the averaged perceptron", "cit": "[CLS] structural learning and latent structural learning has been studied a lot in nlp in recent years#otherefr; #refr, and has"}
{"pre": "[CLS] the first is the task of identifying the event descriptions of the bionlp?09 shared task on event extraction #refr.", "cit": "[CLS] #refr developed a heuristic of ranking transition pairs by cost to evaluate different cfranking schemes. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for learning and beam-search decoding #otherefr;", "cit": "[CLS] in nlp literature, discriminative reranking has been well explored for parsing #otherefr; #refr and statistical machine translation #otherefr. [SEP]"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? ? ? ?", "cit": "[CLS] markov logic makes it 1the only exception that we are aware of is ge and mooney #otherefr. possible to"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr.", "cit": "[CLS] unsupervised systems specify the assumption of same-head coreference in several ways: by as- 1gold mention detection means something slightly"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] our results add to a growing body of evidence #otherefr; #refr that mira is preferable to mert across languages"}
{"pre": "[CLS] the most common approach to this problem is to use a stochastic tree kernel #otherefr; #refr. [SEP] model is", "cit": "[CLS] system networks are like the type hierarchies used in hps#otherefr: #refr; henschel, 1995; teich, in press)): the grammatical types"}
{"pre": "[CLS] the most popular approach to translation models #otherefr; #refr rely on the source side and the target side #otherefr.", "cit": "[CLS] adaptations to the algorithms in the presence of ngram lms are discussed in #otherefr; #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] we tune the system using k-best batch mira #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to translation lexicons from parallel corpora #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ?", "cit": "[CLS] prior work shows that a variety of signals, including distributional, temporal, topic, and string similarity, may inform bilingual lexicon"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? 2008. [SEP]", "cit": "[CLS] the reader is referred to #refr for more details of our mention detection and mention chaining modules. [SEP] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] while these approaches have been shown to improve machine translation performance #refr they usually combine chart parsing with the"}
{"pre": "[CLS] the task of identifying positive and negative sentiments and negative sentiments from a passage, #otherefr; #refr. [SEP] of the", "cit": "[CLS] recent work on classifying temporal relations within the timebank corpus built 6-way relation classifiers over 6 of the corpus?"}
{"pre": "[CLS] #refr proposed a method for learning a word sense disambiguation which is a word sense disambiguation method to extract", "cit": "[CLS] unsupervised approaches proposed so far have involved a number of techniques, including language modeling #otherefr and clustering #refr. [SEP]"}
{"pre": "[CLS] the first is the stanford parser #refr and then applied to obtain the syntactic parse trees of the sentence", "cit": "[CLS] the polarity rules are based on an independent polarity lexicon #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the automatic evaluation of discourse structure of discourse relations #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] a pilot study by hota et al. #otherefr on automatic gender identification in shakespeare?s texts, as well as a"}
{"pre": "[CLS] in recent years, supervised learning methods have been applied to dependency parsing #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] previously, unlabeled data is explored to derive useful local-context features such as word clusters #otherefr, subtree frequencies #refr, and"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? ? ? ?", "cit": "[CLS] we use the coarse-grain classes described in (li and #refr: six non-overlapping classes: abbreviations (abbr), descriptions (desc, e.g. definitions"}
{"pre": "[CLS] #refr use a similar approach to compute sentence importance. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] similarly, the query-focused summarization (e.g., (daume? and #refr) is also related but it focuses on sentence extraction rather than"}
{"pre": "[CLS] the chinese word segmentation method is used to extract the word segmentation and pos tags for pos tags for", "cit": "[CLS] we evaluated performance by measuring wer (word error rate), per (position-independent word error rate), bleu #refr and ter #otherefr"}
{"pre": "[CLS] in the context of machine translation, itg has been explored for paraphrase generation #otherefr; #refr. [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] pr systems that can be broadly categorized as ir-based include #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] recent work couples span modeling tightly with reordering decisions, either by adding an additional feature for each hierarchical phrase"}
{"pre": "[CLS] the first step is to use a stochastic model that predicts a user interface between the output of a", "cit": "[CLS] most recently, chen et al utilized fergus (bangalore and #refr and attempted to make it more domain independent in"}
{"pre": "[CLS] the first is the case for the semantic role labeling task of the framenet project #refr. [SEP] task is", "cit": "[CLS] just like previously reported case studies #otherefr; #refr aiming at the annotation of lexical chains, we found that the"}
{"pre": "[CLS] the first is the case for the automatic acquisition of subcategorization frames from corpora #otherefr, #refr, and many others.", "cit": "[CLS] 13note that japanese is a head final language. f5b: pos of the word which the rightmost constituent word of"}
{"pre": "[CLS] the data set contains 14,619 items and is available from the conll shared task data sets #refr. [SEP] test", "cit": "[CLS] it provides a form of ?distant supervision? #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second approach is to use a maximum entropy model #otherefr; #refr. [SEP] model to predict the probability of", "cit": "[CLS] there are two approaches to solve this problem: to increase the coverage of the dictionary #otherefr; #refr and to"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a lexicon to measure translation #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] there are several sentiment analysis approaches that make use of manually annotated review datasets #otherefr and #refr constructed a"}
{"pre": "[CLS] the system used in the conll shared task #refr was the shared task on event extraction and the genia", "cit": "[CLS] two more systems used them in combination with other techniques #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the evaluation metric is the bleu score #refr on the test set and the test set for the test", "cit": "[CLS] fbk [cross lingual, compositional & multiclass] #refra) uses cross-lingual matching features extracted from lexical phrase tables, semantic phrase tables,"}
{"pre": "[CLS] the learning algorithm used is a variation of the winnow update rule incorporated in snow #otherefr; #refr, a multi-class", "cit": "[CLS] prominent examples are the works by deschacht and moens #otherefr and #refr who learned and applied hidden markov models"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] more work has been done in the area of translating between closely related languages and dealing with data sparsity"}
{"pre": "[CLS] the system used in the conll shared task #refr was the shared task on dependency parsing #otherefr. [SEP] shared", "cit": "[CLS] indeed, a classical parsing scenario is to pregroup mwes using gold mwe annotation #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to translation models #otherefr; #refr rely on parallel data #otherefr. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] a growing body of machine translation research aims to exploit lexical patterns (e.g., ngrams and phrase pairs) with gaps"}
{"pre": "[CLS] we use the stanford corenlp suite to lemmatize and partof-speech tag each word #otherefr; #refr, and parse the input", "cit": "[CLS] similarly, #refr analyzed the resolution of unexpressed event roles as a special case of cr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the source sentence", "cit": "[CLS] phrase-based translation models #otherefr; #refr, which go beyond the original ibm translation models #otherefr 1 by modeling translations of"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for each sentence in the sentence with", "cit": "[CLS] a more sophisticated approach #refr might lead to better results. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford named entity recognizer #refr to extract named entities from the text and extract named entities", "cit": "[CLS] this is represented by the idea behind the pyramid evaluation framework #refr, where different levels of the pyramid represent"}
{"pre": "[CLS] #refr use a similar approach to compute sentence importance. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] the end goal of our work differs in that we aim to revise original image captions into 5we take"}
{"pre": "[CLS] the work of #refr is similar to ours in that it is possible to use a more complex model", "cit": "[CLS] while there has been some work exploring the use of machine leaning techniques for discourse and dialogue #refr, to"}
{"pre": "[CLS] #refr use a similar approach to identify the polarity of words in a sentence. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] many words that are endogenous to social media have been linked with specific geographical regions #refr and demographic groups"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] a great deal of researches have been conducted on this topic with promising progress #otherefr; #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of converting natural language processing #otherefr; #refr. [SEP] [SEP] [SEP] to be a given", "cit": "[CLS] the parser induction algorithm used in all of the experiments in this paper was a distribution of collins is"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] shared", "cit": "[CLS] so far this has either been done through postprocessing heuristics (bj?#refr, or through a local classifier at the end"}
{"pre": "[CLS] topic models have been successfully applied to a wide range of nlp tasks including word sense disambiguation #otherefr, and", "cit": "[CLS] #refr specify a list of cue phrases by hand; the cue phrases are used as a feature in a"}
{"pre": "[CLS] #refr proposed a method for learning a word alignment. [SEP] model that learns bilingual corpora. [SEP] pairs from the", "cit": "[CLS] recently, ta?#refr presented a technique for coupling token constraints derived from projected cross-lingual information and type constraints derived from"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] is a growing number of", "cit": "[CLS] systems for thesaurus extension #refr, information extraction #otherefr each partially address this problem in different ways. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the event and event extraction of event descriptions of event descriptions of", "cit": "[CLS] an event is described by the first tweet that we find discussing it and is defined as something that"}
{"pre": "[CLS] the model is trained with a maximum entropy model #otherefr, which is trained on the penn treebank wall street", "cit": "[CLS] set test set english-wsj sections section 22 section 23#otherefr 1-18,609 18,610-19,609 19,609-20,610 german sentences sentences sentences #refr 1-18,602 18,603-19,602"}
{"pre": "[CLS] in the literature, supervised learning methods have been applied to unsupervised learning of natural language processing #otherefr; #refr. [SEP]", "cit": "[CLS] since the different pos corpora use different tag sets, we map all of them corpora onto the universal pos"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] table 1: features for afwater evaluated by maximising the pseudo-likelihood on a training corpus #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the context of sentiment analysis and opinion mining has been used to determine if the word is to", "cit": "[CLS] the second is subjectivity classification of sentences, clauses, phrases, or word instances in the context of a particular text"}
{"pre": "[CLS] #refr use a supervised sentiment classifier to detect the polarity of words in a target words and their polarity", "cit": "[CLS] #refr deployed the framework of latent structural svms#otherefr for multilevel sentiment classification. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the last decade, there has been a lot of work on joint inference in dependency parsing #otherefr; #refr.", "cit": "[CLS] table 10 shows the performance of the previous systems that were compared, where mcdonald06 refers to the second-order parser"}
{"pre": "[CLS] the task of identifying the semantic relations between nominals of noun phrases and their arguments are also key to", "cit": "[CLS] many algorithms have been developed to automatically acquire semantic class members using a variety of techniques, including co-occurrence statistics"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to translate the words and the", "cit": "[CLS] since the space of different combinations is too large to be searched exhaustively, we use a guided search procedure"}
{"pre": "[CLS] we use the stanford parser #refr to extract dependency relations from the parse trees for each sentence with dependency", "cit": "[CLS] recent work has shown that these entity-level properties allow systems to correct coreference errors made from myopic pairwise decisions"}
{"pre": "[CLS] the grammar we are extracted from the parallel corpus using the berkeley parser #refr and the berkeley parser #otherefr.", "cit": "[CLS] recently, there have been advancement made in the parsing techniques for large-scale lexicalized grammars #otherefr; #refr, and it have"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? 2008. [SEP]", "cit": "[CLS] another important group of related work is on using syntactic dependency features in a vector-space model for measuring word"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a text that is a", "cit": "[CLS] starting from schwarm and ostendorf #otherefr, #refr, kate et al. #otherefr). [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to word alignment is the hmm model #refr. [SEP] model is based on the hmm", "cit": "[CLS] since no large corpus with multiple gold standards exists to our knowledge, systems could instead report the quality of"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] for a definition and further discussion of ill-nestedness, we refer to #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is not to be useful for the purpose", "cit": "[CLS] for some time, multipoint performance degradations caused by switching to automatically induced word categories have been interpreted as indications"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for parsing #refr. [SEP] grammars with", "cit": "[CLS] splitting techniques have also been exploited to speed up parsing time for other lexicalized formalisms, such as bilexical context-free"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a word in a word", "cit": "[CLS] on the other hand, according to the data-driven approach, a frequency-based language model is acquired from corpora and has"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best results for the best", "cit": "[CLS] this part of the model is parametric, operating over a fixed number of tags t , and is identical"}
{"pre": "[CLS] the most common approach to selectional preference acquisition is to use a lexicon to use a lexicon to identify", "cit": "[CLS] #refr developed an lsa approach incorporating a thesaurus, to distinguish the same two relations. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the averaged perceptron algorithm #refr to train the parameters of the averaged perceptron algorithm with the averaged", "cit": "[CLS] on the other hand, #refr proposed a cascaded linear model which has a two layer structure, the inside-layer model"}
{"pre": "[CLS] the first is the task of converting the system of ge task #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] in this paper we follow the standard learning approach to coreference developed by soon et al #otherefr and also"}
{"pre": "[CLS] the feature weights were tuned using mert #refr on the development set. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] taking och?s mert procedure as a baseline, #refr experiment with different training criteria for smt and obtain the best"}
{"pre": "[CLS] in the second international chinese word segmentation bakeoff #otherefr; #refr and the same task as the best results are", "cit": "[CLS] moreover, #refr show how a tree kernel can be applied to dop1 is all-subtrees representation. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] the feature function ? is a second-order edgefactored representation #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the word alignment of word alignment of word alignment of word alignment using", "cit": "[CLS] in phrase-based machine translation #refr, a distortion limit is used to constrain the position of output phrases. [SEP] [PAD]"}
{"pre": "[CLS] the system is based on the inprotk toolkit for the best performing system described in #refr. [SEP] system, which", "cit": "[CLS] one recent l)opular sul)task is the learning of non-re#otherefr, #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] in phrase-based models #otherefr; #refr, phrase is introduced to serve as the fundamental translation element and deal with local"}
{"pre": "[CLS] the second approach is to use a small number of manually annotated corpora such as the same data set", "cit": "[CLS] in the meantime, #refra) #otherefrb) proposed a method to acquire contcxt-dcpendent pos disambiguation rules and created an accurate tagger,"}
{"pre": "[CLS] the most common approach to this task is to use a lexicon to identify the polarity classification task #refr.", "cit": "[CLS] it is therefore important to deploy topic recognition #refr and/or topic clustering #otherefr to identify and group relevant pieces"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best derivation of the best", "cit": "[CLS] it has been shown that more sophisticated probability models #otherefr and learning regimes #refr, as well as the incorporation"}
{"pre": "[CLS] in the case of dependency parsing, previous work has shown that it is possible to improve the performance of", "cit": "[CLS] mcclosky et al#otherefr they later show that the extra information brought by the discriminative reranking phase is a factor"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is not to be useful for the number", "cit": "[CLS] more recent algorithms in the same tradition, including the refined mbdp-1 of venkataraman #otherefr, the wordends algorithm of #refr,"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the sentences and the syntactic parse trees of the sentences in", "cit": "[CLS] systems for scientific stylometry would give sociologists new tools for analyzing academic communities, and new ways to resolve the"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] algorithm with the best derivation = exp ( m?", "cit": "[CLS] they show that standard lexicalized models fail to outperform an unlexicalized baseline (a vanilla pcfg) on negra, a german"}
{"pre": "[CLS] the work of #refr is similar to ours in that it is possible to use a more complex linguistic", "cit": "[CLS] other previous studies have used citing sentences in various applications such as: scientific paper summarization #otherefr; #refr; abu-jbara and"}
{"pre": "[CLS] the first is the case for the syntactic parsing of the english lexical substitution task #refr. [SEP] grammars in", "cit": "[CLS] accordingly, much research in this area has been focused on automatic lexical acquisition #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] all these results seem to suggest that adding punctuation in speech transcription is of little help to statistical parsers"}
{"pre": "[CLS] the most popular approach to translation models #otherefr; #refr rely on the source side and the target side #otherefr.", "cit": "[CLS] for example, the cdec decoder #refr supports the context-free-reordering/finitestate-translation framework described by dyer and resnik #otherefr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] in the context of machine translation, itg has been explored for statistical word alignment in both unsupervised #otherefr; #refr", "cit": "[CLS] other prior work has mined supplemental parallel data from comparable corpora #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best derivation of the best", "cit": "[CLS] secondly, we generalise the nonparametric bayesian learning framework of adaptor grammars #otherefr. evant to other applications of probabilistic srcgs,"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking approaches #otherefr; #refr to learning for dependency", "cit": "[CLS] in some cases with complex combinatorial constraints, this simple mbr scheme has proved more effective than exact decoding over"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a text with a text that are semantically", "cit": "[CLS] further work attempted to resolve the unnaturally long expressions which could be generated by this approach #refr. [SEP] [PAD]"}
{"pre": "[CLS] in the last years, the field of automatic mt evaluation metrics have been proposed in the field of mt", "cit": "[CLS] the grammar feature covers errors such as sentence fragments, verb form errors and pronoun errors #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to paraphrase generation has been the source sentences into a target language syntax based on", "cit": "[CLS] accuracy is the proportion of the source words with at least one correct #otherefr, or 2,000 most frequent nouns"}
{"pre": "[CLS] the first is the same as #refr, which was used for the same as the same task as the", "cit": "[CLS] one of the reasons are conll shared tasks for syntactic dependency parsing in the years 2006, 2007 #otherefr, cf."}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] for translation experiments, we used cdec #otherefr, a fast implementation of hierarchical phrase-based translation models #refr, which represents a"}
{"pre": "[CLS] the second approach is to use mismatches in which the same lexical and syntactic features as the context #refr.", "cit": "[CLS] #refr also report comparable results for minipar features and simple word based proximity features. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal (wsj) section of the penn treebank #otherefr; #refr.", "cit": "[CLS] this is true of the widely used link grammar parser for english #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the case of the penn treebank #otherefr, which we use the penn treebank", "cit": "[CLS] while some previous works on subcategorization fi'alne acquisition assumed very little prior knowledge concerning the classification of subcategorization frames"}
{"pre": "[CLS] the first is the automatic evaluation of discourse coherence models in the natural language processing (nlp) tasks such as", "cit": "[CLS] since then, based on #refr, an algorithm for detecting the thematic hierarchy of a text using only lexical cohesion"}
{"pre": "[CLS] the first is the case for the automatic acquisition of subcategorization frames from the penn treebank #otherefr; #refr, and", "cit": "[CLS] another approach is based on the markov random field (mrf) theory #refr, whose korean version will be added to"}
{"pre": "[CLS] the second approach is to use a word lattice as a word segmentation and its context #otherefr; #refr. [SEP]", "cit": "[CLS] for japanese, #refr proposed a method of computing an arbitrary length character n-gram, and showed that the character n-gram"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for each sentence in the sentence with", "cit": "[CLS] we take an approach to deriving a grammar from manually annotated corpora by training probabilistic models like current statistical"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal (wsj) section of the penn treebank #otherefr; #refr.", "cit": "[CLS] this has led to the development of various data-driven dependency parsers, such as those by yamada and matsumoto #otherefr,"}
{"pre": "[CLS] in the second international chinese word segmentation bakeoff #otherefr; #refr and the same task as the best results are", "cit": "[CLS] 6.3.1 model the model is a hidden markov model #otherefr; #refr.6 we use a bigram model and a locally"}
{"pre": "[CLS] the grammar is a grammar that is a grammar that is a grammar that is a grammar that is", "cit": "[CLS] within natural-language processing, tomita #otherefr and #refr have reported parsers which operate on-line, but, incidentally, not incrementally in the"}
{"pre": "[CLS] the parser is trained on the penn treebank corpus and tested on the penn treebank wall street journal, parser", "cit": "[CLS] this includes both the parsers that attach probabilities to parser moves #otherefr, but also those of the lexicalized pcfg"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to learn a hierarchical phrase translation probabilities", "cit": "[CLS] #refr describe a joint-probability phrase-based model for alignment, but the approach is limited due to excessive complexity as viterbi"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? ? ?", "cit": "[CLS] the research most closely related to ours is an ensemble-based method for automatic thesaurus construction #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a task of identifying the degree of semantic relations between words", "cit": "[CLS] the statistic-based approaches view the semantic interpretation as a multi-class classification problem. #otherefr; #refr use supervised methods and explore"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] in this paper we evaluate a ccg parser #otherefrb) on the briscoe and carroll version of depbank #refr. [SEP]"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] #refr worked on phrase-level semantic orientations. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the berkeley parser #refr to obtain the syntactic parse trees for each sentence and the english and", "cit": "[CLS] for 1-best search, we use the cube pruning technique #otherefr; #refr which approximately intersects the translation forest with the"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? ? ? ?", "cit": "[CLS] a method of latent embedding of relation instances for sentence-level relation extraction was shown in #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to compute sentence importance. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] the qg formalism has been previously applied to parser adaptation and projection #otherefr, paraphrase identification #refr, and question answering"}
{"pre": "[CLS] the system is based on the standard approach of #refr and the standard log-linear pb-smt model: #otherefr. [SEP] evaluation", "cit": "[CLS] for one, both addicter and hjerson report mt errors related more to adequacy than fluency, although it was shown"}
{"pre": "[CLS] the system is based on the c&c parser #refr and the collins parser #otherefr. [SEP] shared task on dependency", "cit": "[CLS] the spmrl 2013 shared task #refr was an interesting opportunity to develop a simple #otherefr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to word alignment is the ibm models #otherefr; #refr. [SEP] model 1 and the hmm", "cit": "[CLS] the map alignment for each source position j ? {1, 2, ..., j}, is then computed as amap (j)"}
{"pre": "[CLS] #refr used a similar method to detect sentiment in the polarity of words in the polarity of words in", "cit": "[CLS] the task of sentiment analysis has evolved from document level analysis #otherefr; #refr) to sentence level analysis #otherefr). [SEP]"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] of a sentence, and", "cit": "[CLS] the task of relation classification is to predict semantic relations between pairs of nominals and can be defined as"}
{"pre": "[CLS] the work of #refr is similar to ours in that it is possible to use a discourse representation of", "cit": "[CLS] whereas several studies have been concerned with causal markers and their interactions with other linguistic means, for instance, vander"}
{"pre": "[CLS] the second approach is to use a maximum entropy model #otherefr; #refr. [SEP] model for the probability distribution over", "cit": "[CLS] the later result is confirmed by many others #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to paraphrase acquisition is the source sentences #otherefr; #refr. [SEP] input to be extracted from", "cit": "[CLS] 2we use in this phase the juman morphological analyzing system #otherefr for tagging japanese texts and brill is transformation-based"}
{"pre": "[CLS] the grammar is a grammar that is the grammar of the grammar of the grammar of the grammar of", "cit": "[CLS] the tsnlp project (lehmann and #refr and its successor diet #otherefr, which built large multilingual testsuites, likewise fall into"}
{"pre": "[CLS] #refr use a bootstrapping method to identify opinion holders and extract opinion holders and extract opinion expressions from documents", "cit": "[CLS] in this paper, the pre-defined process, must-link, and cannot-link are derived from #refr?s work, all must-links and cannot-links are"}
{"pre": "[CLS] the most common approach to machine translation is to use a log-linear combination of statistical machine translation #otherefr; #refr.", "cit": "[CLS] and researchers in multi-document text summarization #refr, information extraction #otherefr have focused on identifying and exploiting paraphrases in the"}
{"pre": "[CLS] the first is the task of identifying the polarity of a word is a word sense disambiguation of a", "cit": "[CLS] there is also work on grouping senses of other inventories using information in the inventory #refr along with information"}
{"pre": "[CLS] in the context of sentiment analysis and opinion mining has been used to build a wide variety of text", "cit": "[CLS] finding such aspects is a challenging research problem that has been addressed in a number of ways #otherefr; #refr;"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of meaning that are not", "cit": "[CLS] arguably the most widely used is the mutual information #otherefr; #refr; d. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to learn a translation models from a", "cit": "[CLS] to measure the translation quality, we use the bleu score #refr and the nist score #otherefr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] more recently, #refr applied their beam filling algorithm to phrase-based decoding. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of event extraction was first introduced by #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] others, such as ritter et al. #otherefr and #refr, use freebase as knowledge base and evaluate their classifier on"}
{"pre": "[CLS] the system used in this paper is based on the c&c parser #otherefr and the stanford parser #refr. [SEP]", "cit": "[CLS] the documents are then parsed using eugene charniak?s maximum entropy inspired parser #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford corenlp suite to lemmatize and partof-speech tag each word #otherefr; #refr, and extract the text", "cit": "[CLS] features being explored include the clause entities, organized into a grid #otherefr, syntactic features #refr, and others. [SEP] [PAD]"}
{"pre": "[CLS] the task of event extraction has received a lot of attention in recent years #otherefr; #refr. [SEP] [SEP] [SEP]", "cit": "[CLS] information extraction #otherefr) or classifiers (e.g., #refr, freitag and mccallum #otherefr) to extract role fillers for events. [SEP] [PAD]"}
{"pre": "[CLS] the lingo grammar matrix #refr is a formalism for which is a natural language generation system that is a", "cit": "[CLS] for extensions of this multi-tape approach see #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] as far as we can see, zhu et al#otherefr were the first to use english/simple english wikipedia data for"}
{"pre": "[CLS] the work of #refr is similar to ours in that it is possible to use a more complex linguistic", "cit": "[CLS] a substantial body of work has been done on determining the affect #otherefr; yessenalina and #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] in the context of machine translation, itg has been explored for applications in machine translation #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] we use a similarity function proposed in #refr which is based on longest common subsequence ratio #otherefr. [SEP] [PAD]"}
{"pre": "[CLS] the grammar we are extracted from the treebank using the stanford parser #refr. [SEP] parser #refr with the parser", "cit": "[CLS] since 2002, the grammar has been extended and modified by berthold crysmann #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr proposed a method for identifying the polarity of a document by a document into a document to determine", "cit": "[CLS] we use a topic coherence #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking approaches #otherefr; #refr to learning for dependency", "cit": "[CLS] figure 1: word alignment with dependency parses for an english-hindi parallel sentence to train the traditional dependency parsers #otherefr,"}
{"pre": "[CLS] the first is the automatic evaluation of discourse structure of discourse structure of discourse relations #refr. [SEP] [SEP] [SEP]", "cit": "[CLS] ozg?ur and #refr and morante and daelemans #otherefr) with promising results. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr proposed a method for learning a word segmentation and a word segmentation model that uses a word segmentation", "cit": "[CLS] in parsing, #refr proposed an extended inside-outside algorithm that infers the parameters of a stochastic cfg from a partially"}
{"pre": "[CLS] the first is the task of coreference resolution #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] ? the typed-similarity [SEP] ?", "cit": "[CLS] previous investigations of coreference errors have focused on quantifying the importance of subtasks such as named entity recognition and"}
{"pre": "[CLS] the first is the task of identifying the mentions of a text with a text at the sentence level", "cit": "[CLS] e.g., b3/muc f1 are 75.6/72.4 on ace-2 and 69.4/70.6 on muc-6 compared to 78.3/66.0 on ace-2 and 70.9/68.5 on"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the entire sequence of the entire", "cit": "[CLS] bp has been effectively used on other nlp tasks #refr, and is effective in cases such as this where"}
{"pre": "[CLS] the pos tagging task is to be able to efficiently perform well on the basis of the training data", "cit": "[CLS] #refr described a new approach to the disambiguation of capitalized words in mandatory positions. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second approach is to use a word lattice as a sequence of a sequence of a sequence of", "cit": "[CLS] bleu #refr is the geometric mean of the n-gram precisions in the output with respect to a set of"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] the standard features for genre classification models include words, part-of-speech (pos) tags, and punctuation #refr, but constituent-based syntactic categories"}
{"pre": "[CLS] the first is the automatic evaluation of discourse structure of discourse relations #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] in particular, we use wikipedia as an important source of world knowledge which is capable of providing information about"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set.", "cit": "[CLS] the paper contains the following original contributions: 1) the dp-based decoding algorithm in #refr is extended in a formal"}
{"pre": "[CLS] the first is the automatic evaluation of the automatic metric of the automatic evaluation of the automatic evaluation of", "cit": "[CLS] some applications that would benefit from knowing this distinction are machine translation #otherefr, (multilingual) information retrieval #refra), etc. [SEP]"}
{"pre": "[CLS] the system is based on the inprotk project #refr. [SEP] score for the number of the number of the", "cit": "[CLS] recently, #refrb) developed an annotated corpus of student responses (henceforth, the sra corpus) with the goal of facilitating dynamic"}
{"pre": "[CLS] the first is the task of identifying the event descriptions of the bionlp?09 shared task on event extraction #refr.", "cit": "[CLS] information extraction (ie) makes this feasible by considering only events of a specified type, such as personnel succession or"}
{"pre": "[CLS] #refr proposed a method for identifying word polarity of subjective words and their polarity in a target words in", "cit": "[CLS] for words that do not appear in wordnet, we use distributional similarity #refr as a proxy for word relatedness."}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] bleu score #refr and", "cit": "[CLS] in this framework, the source language, let?s say english, is assumed to be generated by a noisy probabilistic source.1"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] we proved 4there are also other lists of semantic relations used by the research community (e.g., #refr), but empirically"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for each sentence with dependency trees for", "cit": "[CLS] word-by-word alignments for the sentence pairs are produced by first running giza++ #otherefr in both directions and then combining"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to learn a translation probabilities from", "cit": "[CLS] for the hierarchical phrase-based approach, #refr present a discriminative rule model and show the difference between using only the"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] input: x , a sentence annotated with named entities #otherefr, and part-ofspeech tags and basic dependencies from the stanford"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within the input is to be a given by", "cit": "[CLS] the negra corpus #refr provides pred-arg information for german, similar in granularity to glarf. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the sense inventory is a very similar to the one used in the one used in the most popular", "cit": "[CLS] wsd was then performed using the isr-wn network in combination with the algorithm of gutie?rrez #otherefr, which is an"}
{"pre": "[CLS] the most common approach to word alignment is to use a word alignment model #otherefr; #refr. [SEP] model to", "cit": "[CLS] this includes, but is not limited to, applying prior distributions (mermer and #refr or smoothing techniques #otherefr. [SEP] [PAD]"}
{"pre": "[CLS] the model weights of the log-linear model are optimized with minimum error rate training (mert) #refr. [SEP] model weights", "cit": "[CLS] there are word reordering constraint methods using itg #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to learn a translation probabilities from", "cit": "[CLS] also related to our work, #refr proposes to parameterize a hierarchical reordering model with sparse features that are conditioned"}
{"pre": "[CLS] the similarity between two sentences is computed using the wordnet #otherefr and the wordnet similarity measure proposed by #refr.", "cit": "[CLS] we did method source spearman #otherefr web snippets 0.69 [0.42, 0.84] 0.69 [0.42, 0.85] (wu and #refr wordnet 0.78"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] nielsen #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the parsing of the english lexical substitution task #refr. [SEP] test set of", "cit": "[CLS] the computational performances of the application reported in #refra) indicate that the parser only spends 10% of its time"}
{"pre": "[CLS] in the literature, supervised learning methods have been applied to natural language processing tasks such as part-of-speech tagging #otherefr,", "cit": "[CLS] therefore, an increasing attention has been recently given to semi-supervised learning, where large amounts of unlabeled data are used"}
{"pre": "[CLS] the first is the case for semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] we focus our experimental study on the semantic role labeling problem #otherefr, fast statistical parsers such as #refr still"}
{"pre": "[CLS] the task of coreference resolution is a well studied problem #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] entity coreference resolution is a well studied problem with many successful techniques for identifying mention clusters #otherefr; #refr; raghunathan"}
{"pre": "[CLS] the first is the task of identifying the overall semantic textual similarity of a text of a text in", "cit": "[CLS] #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] recognizing good translations falls in the scope of research on automatic mt quality estimation #otherefr; #refr; bach et al"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a task of identifying the degree of semantic relations between words", "cit": "[CLS] therefore, as was done in previous work #otherefr; #refr, we pick out our semantic classes c from wordnet and"}
{"pre": "[CLS] the first is the case for the case of the grammar development of the grammar development environment the grammar", "cit": "[CLS] in this way, the parser is head-driven #refr; the head determines the course of f~irther parsing. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the translation model is a log-linear model #refr. [SEP] model weights ? = ? ? ? ? ? ?", "cit": "[CLS] other toolkits appeared including joshua #otherefr and the first version of our package, phrasal #refr, a java-based, open source"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be np-hard #refr. [SEP] s", "cit": "[CLS] #refr show that the performance of an smt system does not suffer if lm parameters are quantized into 256"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to translate the phrases as the", "cit": "[CLS] tion of bleu #refr and ter #otherefr as optimization criterion, ?? := argmax? {(2 . [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr proposed a method for learning a word sense disambiguation which is a word sense disambiguation method to identify", "cit": "[CLS] the utility of the transfer or annotation projection by means of bilingual lexicons obtained from the clss models has"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] parsing technologies have improved considerably in the past few years, and high-performance syntactic parsers are no longer limited to"}
{"pre": "[CLS] the grammar we are extracted from the parallel corpus using the berkeley parser #refr and the berkeley parser #otherefr.", "cit": "[CLS] one way to solve the above problems has been referred to as pseudo-projectivity, i.e. a parser produces a projective"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] the first part of our system, the syntactic analysis itself, generates surface dependency structures in a sequential fashion #refrb;"}
{"pre": "[CLS] #refr use a bootstrapping method to identify opinion holders and extract opinion expressions from documents in a target words", "cit": "[CLS] in the case of sentiment analysis, while people are able to achieve 87.5% accuracy #refr on a movie review"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of meaning that are not", "cit": "[CLS] a number of previous approaches to name discrimination have employed ideas related to context vectors. #refr proposed a method"}
{"pre": "[CLS] the second approach is to use a word lattice as a sequence of a sequence of a sequence of", "cit": "[CLS] these studies attempt to minimize lexicon complexity #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford named entity recognizer #refr to extract named entities from the set of the set of", "cit": "[CLS] most of our relation extraction features are based on the previous work of #otherefr and #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the system is based on the inprotk toolkit for the best performing system described in #refr. [SEP] score for", "cit": "[CLS] there have been several efforts to apply machine learning techniques to the same task #otherefr #refr #otherefr. [SEP] [PAD]"}
{"pre": "[CLS] the first is the task of converting the extraction task of a sentence, from the bionlp?09 shared task on", "cit": "[CLS] current state-of-the-art event extractors follow the same architectural blueprint and divide the extraction process into a pipeline of three"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for parsing #refr. [SEP] grammars with", "cit": "[CLS] these features perform binary tests on the state directly, unlike #refr which works with an intermediate representation of the"}
{"pre": "[CLS] we use the minimum error rate training #refr to tune the feature weights for the feature weights of the", "cit": "[CLS] mert #otherefr; #refr, pro #otherefr and so on, which iteratively optimize a weight such that, after re-ranking a k-best"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] model with the average number of the average", "cit": "[CLS] recently, modeling of semantic compositionality #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? ? ?", "cit": "[CLS] substantial research exists on the learning of hyperonymy relations #otherefr; gi#refr and selectional preferences #otherefr; o. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with a probability distribution", "cit": "[CLS] combining the outputs of several systems has been shown in the past to improve parsing performance significantly, including integrating"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for dependency parsing #refr. [SEP] [SEP]", "cit": "[CLS] supervised dependency parsing achieves the stateof-the-art in recent years #otherefra; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to coreference resolution is to use a seed lexicon to generate a set of seed", "cit": "[CLS] to address the first weakness, researchers have attempted to train a mention-ranking model for determining which candidate antecedent is"}
{"pre": "[CLS] #refr proposed a method for learning a word clustering words from a target language model to target language to", "cit": "[CLS] to enable this, we first take class labeled data (doesn?t need to be multi-class labeled data unlike #refr) and"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr. [SEP]", "cit": "[CLS] we only describe these models briefly since full details are presented elsewhere#otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the grammar is a grammar that is a grammar that is a grammar that is a grammar that is", "cit": "[CLS] a more systematic evaluation of the grammar is in preparation, using the test suite for natural language processing #refr."}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr.", "cit": "[CLS] comparing active dual supervision using matrix factorization with grads on sentiment analysis the sentiment analysis experiment is conducted on"}
{"pre": "[CLS] in this paper, we propose a method for learning the context of transliteration pairs from the bilingual lexicon from", "cit": "[CLS] our technical approach follows a large body of work developed over the last few years, following #otherefr; #refr. [SEP]"}
{"pre": "[CLS] the task of identifying the event descriptions of event extraction and event descriptions has been studied in the goal", "cit": "[CLS] for nearest neighbors edges, we take neural network embeddings learned in #refr corresponding to each vocabulary entry. [SEP] [PAD]"}
{"pre": "[CLS] the task of relation extraction has received much attention in recent years #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] this is typically carried out by applying supervised learning, e.g. #otherefr; #refr by using a handlabeled corpus. [SEP] [PAD]"}
{"pre": "[CLS] the generation of referring expressions is a subject of research in natural language generation (nlg) in natural language generation", "cit": "[CLS] stochastic methods for nlg may provide such automaticity, but most previous work #otherefr, #refr concentrate on the specifics of"}
{"pre": "[CLS] the first is the automatic evaluation of discourse structure of discourse relations #refr. [SEP] discourse markers are the discourse", "cit": "[CLS] #refr studied repetitions in task-oriented conversations. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to word alignment is the ibm models #otherefr; #refr. [SEP] model 4 alignments #otherefr; #refr.", "cit": "[CLS] in the machine translation area, most research on confidence measure focus on the confidence of mt output: how accurate"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] the parsing schema for the tag cyk-based algorithm #refr is a function that maps such a grammar g to"}
{"pre": "[CLS] the most common approach to paraphrase generation is to use a lexicon to paraphrase generation #otherefr; #refr. [SEP] words", "cit": "[CLS] much work on paraphrase generation has focussed on lexical variation and syntactic transformation within individual sentences #otherefr; #refr. [SEP]"}
{"pre": "[CLS] the model is trained with the perceptron algorithm of #refr. [SEP] score entire sequence models and the entire sequence", "cit": "[CLS] several of the state-of-the-art approaches learn a scoring function defined over mention pair, cluster-mention or cluster-cluster pair to guide"}
{"pre": "[CLS] the first is the case for the case of the grammar formalism used by #refr, which is the most", "cit": "[CLS] it amounts to a kind of proof-search that has been described by merenciano and morrill #otherefr and by #refr."}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking approaches #otherefr; #refr to learning for dependency", "cit": "[CLS] we also include the results of the unsupervised dependency parsing model with non-parallel multilingual guidance #otherefr8, and ?pr? which"}
{"pre": "[CLS] the task of identifying the overall sentence pairs in text is a text #refr. [SEP] of the same event", "cit": "[CLS] performance was evaluated using the bleu score #refr: en-fr 0.23, en- cz 0.14, fr-en 0.26 and cz-en 0.22. [SEP]"}
{"pre": "[CLS] the first is the task of identifying the event descriptions of the goal is to classify the system in", "cit": "[CLS] to date, rl has been used mainly for learning dialogue policies for slot-filling applications such as restaurant recommendations #otherefr;"}
{"pre": "[CLS] the most common approach to parsing is to use a linear model and maximise the structure to parse tree", "cit": "[CLS] a breakthrough has come in the form of research by #refra; 2006b) who show that self-training can be used"}
{"pre": "[CLS] in the literature, discriminative reranking has been widely used in nlp tasks such as pos tagging #otherefr, and tagging", "cit": "[CLS] discriminative models do not only have theoretical advantages over generative models, as we discuss in section 2, but they"}
{"pre": "[CLS] the first is the task of converting natural language processing #otherefr; #refr. [SEP] [SEP] [SEP] to be a given", "cit": "[CLS] it is worth noting that this algorithm is used in many modern unification grammar-based systems, e.g., the lkb system"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr.", "cit": "[CLS] vector-space models of lexical semantics have been a popular and effective approach to learning representations of word meaning #refr;"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal (wsj) treebank #otherefr; #refr. [SEP] trees and the", "cit": "[CLS] researches in statistical cfg parsing #refr and psycholinguistics #otherefr showed that this strategy is desirable for nlp. [SEP] [PAD]"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to learn a translation models from a", "cit": "[CLS] the performance of the rctm probabilities joined with a single word penalty feature matches the performance of the state-of-the-art"}
{"pre": "[CLS] #refr proposed a method for learning semantic relatedness between words from a target words with a target words with", "cit": "[CLS] #refr, gaussier #otherefr). [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the event descriptions of coreference resolution of events #refr. [SEP] event mentions", "cit": "[CLS] for example, pleonastic 2 pronouns are identified by looking for modal adjectives #otherefr, #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford named entity recognizer #refr to extract named entities from the text corpus. [SEP] [SEP] [SEP]", "cit": "[CLS] the domain dependency of words that how strongly a word features a given set of data (documents) contributes to"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the entire sequence of the entire", "cit": "[CLS] similar models have been used to learn subcategorization information #refr or properties of verb argument slots #otherefr. [SEP] [PAD]"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with the one of", "cit": "[CLS] our choice is based on the effectiveness of information transfer rather than convenience of annotation #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the task of event extraction has received a lot of attention in recent years #otherefr; #refr. [SEP] [SEP] [SEP]", "cit": "[CLS] most other work on relation extraction focuses only on binary relations #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford ner tagger #refr to identify named entities and named entities in the text and extract", "cit": "[CLS] otherwise, it is measured by wordnet similarity package #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the generation of referring expressions is a subject of research in natural language generation (nlg) in natural language generation", "cit": "[CLS] clearly, negotiation is a type of problemsolving (di #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second dataset is the same as the use of the same as the same as the same as", "cit": "[CLS] the proposed partition can then be compared with a gold-standard partition using many existing clustering comparison methods, such as"}
{"pre": "[CLS] the model is trained using the giza++ implementation of ibm model 4 alignments #refr. [SEP] model 4 alignments #otherefr", "cit": "[CLS] the first is to relax or update the independence assumptions based on more information, usually syntactic, from the language"}
{"pre": "[CLS] the work of #refr is similar to ours in that it is possible to use a more complex linguistic", "cit": "[CLS] other theories like polanyi?s linguistic discourse model #otherefr explicitly adopt a syntactic point of view, and rst with strongly"}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders and predict the polarity of words in a sentence and", "cit": "[CLS] #refr described an unsupervised learning method for obtaining positively and negatively oriented adjectives with accuracy over 90%, and demonstrated"}
{"pre": "[CLS] the system uses a log-linear combination of #refr. [SEP] model to predict the best derivation of the best derivation", "cit": "[CLS] the efficacy of this approach has been well-established in many areas, including automated evaluation of machine translation systems #otherefr,"}
{"pre": "[CLS] #refr use a similar approach to compute similarities between pairs of sentences in a source and target languages by", "cit": "[CLS] transliteration is used for finding the translations of ne in the source language from texts in the target language"}
{"pre": "[CLS] the task of segmenting utterances into words, for example, johnson and colleagues have been shown to be useful in", "cit": "[CLS] for example: mccarthy and carroll #otherefr use their mdl approach as part of a system for syntactic and semantic"}
{"pre": "[CLS] the evaluation metric is bleu #refr, which is based on the reference translations of the reference translations of the", "cit": "[CLS] a number of other metrics have been proposed to address these limitations, for example, by allowing for the matching"}
{"pre": "[CLS] the second approach is to use a word segmentation and pos tagger to tag dictionary to tag each word", "cit": "[CLS] statistical chinese word segmentation gains high accuracies on newswire #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second approach is to use a small number of seed examples to score the output of the output", "cit": "[CLS] other applications include cross-lingual information retrieval #otherefr, detection of confusable drug names #refr, and lexicography #otherefr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the parsing of the english lexical substitution task #refr. [SEP] test set of", "cit": "[CLS] the first such method is now known as ?dop1? #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] #refr and kurohashi and nagao #otherefr). [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] to evaluate the effectiveness of unary constraints, we apply our technique to four parsers: an exhaustive cky chart parser"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a task of identifying the degree of semantic relations between words", "cit": "[CLS] to implement this method, we first use the stanford named entity recognizer #refr to identify the set of person"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking approaches #otherefr; #refr to learning for dependency", "cit": "[CLS] #refr apply entropy regularization to dependency parsing. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] we extracted scfg rules from the parallel corpus using the standard heuristics #otherefr and filtering strategies #refr. [SEP] [PAD]"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal, parser #refr and tested on the penn treebank", "cit": "[CLS] 1981, jensen et al 1983, weischedel/sondheimer 1983, lesmo/torasso 1984, #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the berkeley parser #refr to obtain the syntactic parse trees for parse trees for each sentence and", "cit": "[CLS] results on nist mt 03, 05, and 08 are statistically significant with p < 0.05, using bootstrap re-sampling with"}
{"pre": "[CLS] the system is based on the moses toolkit #otherefr and the moses toolkit #refr. [SEP] decoder with the best", "cit": "[CLS] the model weights of all systems have been tuned with standard minimum error rate training #refr on a concatenation"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal, parser #refr and tested on the penn treebank", "cit": "[CLS] moreover, as p-dop is formulated as an enrichment of the treebank probabilistic context-free grammar #otherefr; #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to paraphrase generation is to use a lexicon to paraphrase generation #otherefr; #refr. [SEP] words", "cit": "[CLS] previous work on transfer-based machine translation #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the future, we plan to investigate the use of dialogue acts and dialogue act classifier from #refr. [SEP]", "cit": "[CLS] recently, algorithms have also been developed to the identification of sets of objects rather than individuals #refr, stone 2000,"}
{"pre": "[CLS] the translation quality is evaluated by case-insensitive bleu-4 #refr. [SEP] quality measured by bleu score #refr with the bleu", "cit": "[CLS] #refr applied discriminative training to english-name-to-japanese- katakana transliteration. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders and predict the polarity of words in a sentence and", "cit": "[CLS] we use mxterminator #refr to split sentences in the captions #otherefr to tag words, then include adjectives, adverbs, verbs,"}
{"pre": "[CLS] the task of identifying the semantic relations between nominals of noun phrases and their arguments are also key to", "cit": "[CLS] many methods for extracting causality or scriptlike knowledge between events exist #otherefr; #refr, but none uses a notion similar"}
{"pre": "[CLS] the system is based on the inprotk project #refr. [SEP] score for the number of the development set of", "cit": "[CLS] this 'weighing' can be done during finite-state parsing: the formalism employs a mechanism for imposing penalties on regular expressions,"}
{"pre": "[CLS] the first is the task of coreference resolution #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] ? ? ? ?", "cit": "[CLS] this is a high accuracy tagging task often performed using a sequence classifier #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of coreference resolution is a well studied problem #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] #refr use statistical analysis of web ngram features including lexical relations. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of identifying the semantic relations between nominals of noun phrases and their arguments are also key to", "cit": "[CLS] many methods have been proposed to deal with this task, including supervised learning algorithms #otherefr; #refr, semi-supervised learning algorithms"}
{"pre": "[CLS] in the context of machine translation, itg has been explored for a variety of nlp tasks such as semantic", "cit": "[CLS] the task is somewhat simplified by grefenstette #otherefr and #refr who do not produce a translation of the entire"}
{"pre": "[CLS] the first is the case for the language model is a word in which the context of the target", "cit": "[CLS] #refr\\] presents a scheme for inducing phonological rules from surface data, mainly in the context of studying certain aspects"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a task of identifying the degree of semantic relations between words", "cit": "[CLS] #refr also use syntactically motivated features to cluster together verbs from the biomedical domain and in more recent work"}
{"pre": "[CLS] the system is based on the moses toolkit #otherefr and the moses toolkit #refr. [SEP] decoder with the best", "cit": "[CLS] the alignment method is either model-based #refr in which a statistical word aligner is used to compute hypothesis alignment,"}
{"pre": "[CLS] the first is the task of coreference resolution #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? the typed-similarity", "cit": "[CLS] for example, pleonastic it has been identified using heuristic approaches #otherefr, #refr), supervised approaches #otherefr). [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr proposed a method for learning a word segmentation and a word segmentation model that uses a word segmentation", "cit": "[CLS] among the various models for tagging, there are maximum entropy models #otherefr, transformation based learning #refr, and other succesful"}
{"pre": "[CLS] the first is the case for the parsing of the english lexical substitution task #refr. [SEP] test set of", "cit": "[CLS] however, #refr argues that there is a strong interaction between syntax and morphological segmentation, and that the two tasks"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] this amounts to the automatic induction of fine-grained semantic similarity from corpus data, a notoriously difficult problem #refr. [SEP]"}
{"pre": "[CLS] we use the stanford named entity recognizer #refr to extract named entities from the stanford named entities from the", "cit": "[CLS] selectional preferences (sps) are also useful in measuring similarity #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] we thus explore annotation error correction and its feasibility for dependency annotation, a form of annotation that provides argument"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] after this linguistic preprocessing, the token segmentation of the stanford tools is removed and overwritten by the arktweet tagger"}
{"pre": "[CLS] the task of identifying the semantic relations between nominals of a text is also relevant to the target language", "cit": "[CLS] to reduce the impact of misparsing, we plan to use a version of c&c that can produce the top-n"}
{"pre": "[CLS] the task of identifying the taxonomy of identifying the degree of semantic relations between nominals is crucial for text", "cit": "[CLS] there has recently been work on the automatic extraction of binary relations that scale to a web corpus, for"}
{"pre": "[CLS] the first step is to use a stochastic model that predicts a user interface between the output of a", "cit": "[CLS] most recently, chen et al utilized fergus #otherefr and attempted to make it more domain independent in #refr. [SEP]"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a similar to the similarity of the similarity of two sentences", "cit": "[CLS] a larger number of semantic classes have received smaller amounts of attention, e.g. the classes in the genia ontology"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? ? ? ?", "cit": "[CLS] we follow the inference approach in #refr and formalize this process as an integer linear program (ilp). [SEP] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] indeed, #refr applied selftraining to pos-tagging with poor results, and #otherefr applied it to a generative statistical pcfg parser"}
{"pre": "[CLS] the first is the task of learning a semantic parser that is a semantic role labeling task in the", "cit": "[CLS] it does so by adapting deep learning methods from related work in sentiment analysis #refr, document classification #otherefra), inter"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] the second is a phrase-based decoder implementing the model of #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr proposed a method for learning a word clustering based on a probabilistic model that learns vector space representations", "cit": "[CLS] #refr focus their work syntactically, analyzing temporal links between main and subordinate clauses. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the generation of referring expressions is a subject of research in natural language generation (nlg) in natural language generation", "cit": "[CLS] inprotk realises the iu-model of incremental processing #refr, where incremental systems are conceptualised as consisting of a network of"}
{"pre": "[CLS] in the literature, slight variations of this problem are also referred to as unsupervised learning #otherefr, decision lists #refr,", "cit": "[CLS] additional examples include congressional floor debate records (e.g. political party, speaker, bill) #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for each sentence in the training corpus.", "cit": "[CLS] most commonly, one observes parallel or comparable corpora in two languages, and must reconstruct a matching from one language?s"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the training data is to", "cit": "[CLS] the difference in accuracy between a svm model applied to rrr dataset #otherefr 86.0 rrr wordnet boosting #refr 84.4"}
{"pre": "[CLS] in the context of sentiment analysis and opinion mining has been used to determine if the polarity of words", "cit": "[CLS] since their feature types cover the best performing feature types in earlier works e.g. #otherefr; #refr we re-implemented and"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] we used a morpheme model and a dependency model identical to those proposed by uchimoto et al #refr; uchimoto"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] the underlying formalisms used has been quite broad and include simple formalisms such as itgs #otherefr and #refr, synchronous"}
{"pre": "[CLS] #refr proposed a method for extracting parallel subsentential fragments from comparable corpora. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] #refr show that synonymy extraction for lexical semantic resources using distributional similarity produces continuing gains in accuracy as the"}
{"pre": "[CLS] the second approach is to use a maximum entropy model #otherefr; #refr. [SEP] model for the probability distribution over", "cit": "[CLS] for example, the introduction of global training or complicated features #otherefr; #refr, and the semisupervised and unsupervised technologies utilizing"}
{"pre": "[CLS] the work of #refr is similar to ours in that it is possible to use a more complex linguistic", "cit": "[CLS] this view is, we believe, consistent with the more detailed formal interfaces to discourse semantics/pragmatics presented in #otherefr; #refr"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a text with a text that are semantically", "cit": "[CLS] following the release of the pdtb, smaller corpora annotated with discourse relations have been developed for hindi #otherefr, and"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best derivation of the best", "cit": "[CLS] 3http://nclt.computing.dcu.ie/~jfoster/resources/ corpus method f1 cfg 54.08 bnc btsg 67.73 btsg + insertion 69.06 cfg 64.99 btsg 77.19 wsj btsg"}
{"pre": "[CLS] the approach of #refr is based on the assumption that the assumption of the syntactic structure of the source", "cit": "[CLS] compared to earlier word-based methods such as ibm models #otherefr, phrasebased methods such as pharaoh are much more effective"}
{"pre": "[CLS] #refr use a similar approach to identify the polarity of words in a sentence. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] gender prediction has been studied across blogs #otherefr, and twitter #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from the same text with a text corpus #refr,", "cit": "[CLS] while many approaches have adapted nlp systems to specific domains #otherefr; #refr; blitzer et al, 2007; daume. [SEP] [PAD]"}
{"pre": "[CLS] the most common approach to this problem is to use a lexicon to model that of a word or", "cit": "[CLS] there has been recent interest in determining anaphoricity before performing anaphora resolution #otherefr; #refr, but results have not been"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be a given by using", "cit": "[CLS] such a glosser may be used as a tool for second-language improvement #refr, and thus provide an educational alternative"}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders and opinion holders in opinion holders in opinion holders in", "cit": "[CLS] #refr and francisco and gerva?s #otherefr worked on fairy tales. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the evaluation metric is the bleu score #refr on the test set and the test set for the test", "cit": "[CLS] #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? 2008. [SEP] ?", "cit": "[CLS] 2sometimes called frames. semantic role of each dependent #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best results for the best", "cit": "[CLS] in the context of unsupervised pos tagging models, modeling this distinction greatly improves results #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the case of the grammar formalism described in #refr. [SEP] system, which is", "cit": "[CLS] grammar writing systems (gws) used in natural language processing (nlp) work have been compared to expert systems #refr by"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr.", "cit": "[CLS] there is a growing body of work on statistical learning for different versions of the semantic parsing problem #otherefr;"}
{"pre": "[CLS] the grammar is a grammar that is a grammar that is a grammar that is a grammar that is", "cit": "[CLS] the complete construction of lr-style parser for tags can be found in #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of coreference resolution #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] ? the typed-similarity [SEP]", "cit": "[CLS] #refr present an early approach to unsupervised coreference resolution based on a straightforward clustering approach. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to identify the relations in which noun phrases and their relations in a noun", "cit": "[CLS] we use the c&c parser #otherefr, using the refined version of #refr which brings the syntax closer to the"}
{"pre": "[CLS] the translation quality is measured by bleu #refr. [SEP] bleu score #refr on the test sets. [SEP] [SEP] [SEP]", "cit": "[CLS] for instance, statistical systems are heavily optimized to their training data, and do not perform as well on out-of-domain"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from the english text with a text using the", "cit": "[CLS] as learning technique, we use margin infused relaxed algorithm #otherefr and applied to dependency parsing by #refr. [SEP] [PAD]"}
{"pre": "[CLS] the most popular approach to word alignment is the one proposed by #refr. [SEP] model is to use a", "cit": "[CLS] therefore, we conducted a first experiment using an in-house sentence alignment program called japa that we developed within the"}
{"pre": "[CLS] #refr proposed a method for learning a word sense disambiguation which is a word sense disambiguation method to identify", "cit": "[CLS] typos, ad hoc abbreviations, phonetic substitutions, ungrammatical structures and emoticons abound in short text messages, causing grief for text"}
{"pre": "[CLS] the grammar is a synchronous grammar formalism which is a synchronous grammar (scfg) used to express the synchronous grammar", "cit": "[CLS] furthermore, it is known [see, for example, #refr] that for every weighted string automaton a with states s, we"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a similar to the similarity of words in the similarity of", "cit": "[CLS] we consider two strategies to perform the wsd: (1) compare all senses of two words and select the maximum"}
{"pre": "[CLS] the system is based on the moses toolkit #otherefr and the moses toolkit #refr. [SEP] system, pharaoh decoder with", "cit": "[CLS] the features include: a maximum sentence length of 80, growdiag-final-and symmetrization of giza ++ alignments, an interpolated kneser-ney smoothed"}
{"pre": "[CLS] the first is the process of annotating the discourse structure in text with discourse structure in text with discourse", "cit": "[CLS] these units may be motivated topically #otherefr #refr, or visually #otherefr, depending on the domain and task. [SEP] [PAD]"}
{"pre": "[CLS] the parser is trained on the penn treebank corpus and tested on the penn treebank wall street journal, parser", "cit": "[CLS] several parsers have been implemented in various grammar formalisms and empirical evaluation has been reported: lfg #refr, ltag #otherefr."}
{"pre": "[CLS] in this paper, we present a state-of-the-art dependency parsing algorithm for dependency parsing #refr. [SEP] parsing and klein and", "cit": "[CLS] in fact, #refr found a similar technique to be effective?though only in a model with a large feature space"}
{"pre": "[CLS] the first is the automatic evaluation of the automatic metric of the automatic evaluation of the automatic evaluation of", "cit": "[CLS] #refr extracted the 10 most frequent non-identical sound correspondences from the aligned word transcriptions. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to paraphrase generation is to use a lexicon to paraphrase database into a target language", "cit": "[CLS] on the other hand, corpus-based distributional measures of semantic distance, such as cosine and ?-skew divergence #otherefr, rely on"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best results for the best", "cit": "[CLS] #refr proposed correcting the class priors for domain adaptation purposes in a word sense disambiguation task. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking approaches #otherefr; #refr to learning for dependency", "cit": "[CLS] research in the field of unsupervised and weakly supervised parsing ranges from various forms of em training #otherefr; #refr"}
{"pre": "[CLS] the wsd system used for wsd in the experiments was based on the senseval-3 english all-words wsd task #refr.", "cit": "[CLS] the supervised evaluation method of previous semeval wsi tasks #otherefr is applied to map induced senses to wordnet 3.1"}
{"pre": "[CLS] the generation of referring expressions is a subject of research in natural language generation #otherefr; #refr. [SEP] [SEP] [SEP]", "cit": "[CLS] over the last decade, #refr, and others 2 have contributed to this issue #otherefr). [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the process of annotating the sentence pairs in a sentence pair into a sentence and then", "cit": "[CLS] although state of the art computational models of creativity often produce remarkable results, e.g., manurung et al #otherefr, #refr,"}
{"pre": "[CLS] the system is based on the inprotk project #refr. [SEP] score for the number of the user is not", "cit": "[CLS] applications where the data is available in a database include report generators #otherefr], plandoc [#refr], multimeteo [coch, 1998], fog"}
{"pre": "[CLS] we use the stanford named entity recognizer #refr to extract named entities from the text and extract named entities", "cit": "[CLS] to extract events from raw text, an open information extraction software - reverb #refr is used. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be a difficult problem #otherefr;", "cit": "[CLS] other systems extract and use examples that are represented as linear patterns of varying complexity #otherefr; #refr. [SEP] [PAD]"}
{"pre": "[CLS] the second approach is to use a discriminative model #otherefr; #refr. [SEP] model to predict the probability of the", "cit": "[CLS] #refr, hakkani-tu?r et al#otherefr). [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be np-hard #refr. [SEP] s", "cit": "[CLS] itg has been extensively explored in unsupervised statistical word alignment #refr and machine translation decoding #otherefr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the task of identifying the semantic relations between nominals of noun phrases and their semantic relations has been used", "cit": "[CLS] existing methods for extracting classes of instances acquire sets of instances that are each either unlabeled #otherefr; #refr. [SEP]"}
{"pre": "[CLS] #refr use a similar approach to compute similarity between two words in a sentence. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] li et al2006) utilise distributional similarity #refr to correct misspelled search queries. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the semantic parser used in this paper is the conll shared task on semantic role labeling tasks #refr. [SEP]", "cit": "[CLS] framenet #refr, building on fillmore?s theory of frame semantics, provides definitions of frames and their semantic roles, a lexical"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to learn a translation models from a", "cit": "[CLS] many others have proposed more explicit reordering models #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a semantic role labelling task", "cit": "[CLS] we will follow #refr and use a combination of the precision and recall rates: f~=i = (2\" precision*recall) /"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] our mos concept is also closely related to hierarchical reordering model #refr in phrase-based decoding, which computes o of"}
{"pre": "[CLS] the task of identifying the semantic relations between nominals of noun phrases and their semantic relations has been shown", "cit": "[CLS] because of this, question answering over linked data #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to word sense disambiguation is to use a word as a word sense disambiguation task,", "cit": "[CLS] a parallel strand of research seeks to represent the meaning of larger compositional structures using matrix and tensor algebra"}
{"pre": "[CLS] the first is the process of annotating the word segmentation and then the word segmentation model of a word", "cit": "[CLS] #refr conducted a pilot study in which subjects found it ?difficult and time-consuming? to identify hierarchical relations in discourse."}
{"pre": "[CLS] the first is the case for the word sense disambiguation task of the disambiguation of the disambiguation of the", "cit": "[CLS] tool in a number of recent projects we have explored the use of machine learning techniques for named entity"}
{"pre": "[CLS] #refr proposed a method for learning a word sense disambiguation which uses a word to identify the same word", "cit": "[CLS] several attempts have been made to tackle this problem by using unsupervised learning techniques, which make vast amount of"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking approaches #otherefr; #refr to learning for dependency", "cit": "[CLS] take chinese for example; there have been several attempts to develop accurate parsers for chinese #otherefr; #refr, but the"}
{"pre": "[CLS] the first is the annotation tool which was the annotation tool #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] we developed an english hpsg parser, enju 1 #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a text that is a", "cit": "[CLS] in addition, a number of researchers have built systems to take reading comprehension examinations designed to evaluate children?s reading"}
{"pre": "[CLS] in this paper, we propose a discriminative model for training using a discriminative word alignment model based on the", "cit": "[CLS] in most cases, the accuracy of parsers degrades when run on out-of-domain data #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the grammar we are extracted from the treebank using the stanford parser #refr. [SEP] parser #refr with the parser", "cit": "[CLS] relevant, in principle, to our discussion here, are also the results obtained with treebank grammars for german: #refr have"}
{"pre": "[CLS] the idea of using cross-lingual information extraction was proposed by #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] a cvm learns semantic representations of larger syntactic units given the semantic representations of their constituents #otherefr; #refr; socher"}
{"pre": "[CLS] the second approach is to use a small number of seed examples for example outputs #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] it also contributes to word sense disambiguation #otherefr, part-of-speech tagging #refr and machine translation #otherefr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the system is based on the inprotk project #refr. [SEP] score for the number of the number of the", "cit": "[CLS] tempora l st ructure the input used to compute the temporal structure of a situation consists of the grammatical"}
{"pre": "[CLS] the first is the one of the most popular approach to use a word in a word in a", "cit": "[CLS] in recent years, a new approach to nlg has emerged, which hopes to build on the success of the"}
{"pre": "[CLS] the model is trained using the averaged perceptron algorithm #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] = ? ?", "cit": "[CLS] in unsupervised evaluation, the system outputs are compared by using metrics v-measure #otherefr and paired f-score #refr. [SEP] [PAD]"}
{"pre": "[CLS] the task of identifying paraphrases of semantic relations between nominals (gi#refr, the two words in a target language and", "cit": "[CLS] in id 152, we would like the hypothesis to align with the first part of the text, to 1this"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best results for the best", "cit": "[CLS] additionally, many discriminative models use a generative model as a base model and add discriminative features with reranking #otherefr;"}
{"pre": "[CLS] the second approach is to use a maximum entropy classifier trained on the largest publicly available data set of", "cit": "[CLS] examples include anaphora resolution #refr and gene normalization #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with a probability distribution", "cit": "[CLS] we then parse with each refinement, in sequence, much along the lines of #refr, except with much more complex"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the longrange reordering model is to use of the", "cit": "[CLS] ccg, and parsing with ccg, has been described in detail elsewhere #otherefr; #refr; here we provide only a short"}
{"pre": "[CLS] the model is trained using the giza++ implementation of ibm model 4 alignments #refr. [SEP] model 4 alignments #otherefr", "cit": "[CLS] studies reveal that statistical alignment models outperform the simple dice coefficient #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best derivation of the best", "cit": "[CLS] #refr describe a sentence planner for german that annotates the propositional input with discourse-related features in order to determine"}
{"pre": "[CLS] the most common approach to machine translation is to use syntactic information to enhance the source sentences #refr. [SEP]", "cit": "[CLS] mt outputs were tagged by morc?e tagger #otherefr and then parsed with mst parser #refr that was trained on"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] in this section, alignment model is trained by mgiza++ #refr with grow-diag-final-and heuristic function. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr.", "cit": "[CLS] the idea of augmenting a model with additional latent variables to increase its expressiveness is known as hidden or"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] for example, on the wmt 2006 shared task evaluation, the french to english translation bleu scores dropped from about"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to learn a translation probabilities from", "cit": "[CLS] several methods have been proposed to use syntactic information to handle the reordering problem, e.g. #otherefr; #refr. [SEP] [PAD]"}
{"pre": "[CLS] the tempeval community focused on the classification of the temporal relations between events and their arguments and their arguments", "cit": "[CLS] in our earlier work #refr, we showed that current technologies for factoid question answering (qa) are not adequate for"}
{"pre": "[CLS] the model is trained using the giza++ implementation of ibm model 4 alignments #refr. [SEP] model 4 alignments #otherefr", "cit": "[CLS] following #refr, we automatically decide the threshold for word frequency to discriminate between content words and function words. [SEP]"}
{"pre": "[CLS] the most popular approach to phrase-based translation models #otherefr; #refr rely on the source side and the target side", "cit": "[CLS] in #otherefr and #refr, the authors modify a certain dynamic programming technique used for tsp for use with an"}
{"pre": "[CLS] the work of #refr is similar to ours in that it is possible to use a more complex model", "cit": "[CLS] besides, ferna#refr find certain dialogue acts are important cues for detecting decisions in multi-party dialogue. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the grammar is a grammar that is the grammar of the grammar of the grammar of the grammar of", "cit": "[CLS] we briefly describe a calculus to represent the structures used by this system, extending on the work of rounds,"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the words is to be represented by a window", "cit": "[CLS] there has already been work on detecting corrections in sentence revisions #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of segmenting utterances into words, for example, johnson and colleagues have been used in a variety of", "cit": "[CLS] active learning #otherefr; #refr). [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the automatic evaluation of the automatic metric of the automatic evaluation of the automatic evaluation of", "cit": "[CLS] to capture this intuition, we turn, following #refr, to \"mutual information\" #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to identify the polarity of words in a sentence. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] in the document classification approach, researchers have been exploring techniques for classifying documents according to semantic/sentiment orientation such as"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] the sentiment detection task was modeled after a well-known document analysis setup for sentiment classification, introduced by #refr. [SEP]"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the penn treebank #otherefr. [SEP] s", "cit": "[CLS] with regard to lexical features, the situation is more complex in that there are a number of studies questioning"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] of a text in", "cit": "[CLS] furthermore, bootstrapping from the simile seed yields higher cluster accuracy on the 402- noun data-set than #refr themselves achieve"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking approaches #otherefr; #refr to learning for the", "cit": "[CLS] for centuries seafarers expertly combined both sails to traverse extensive distances, greatly increasing the reach of medieval navigation.1 likelihoods"}
{"pre": "[CLS] #refr proposed a method for learning a probabilistic model that learns latent dirichlet allocation (lda) as a latent variable", "cit": "[CLS] recent years have seen ilp applied to many structured nlp applications including dependency parsing #refr, text alignment #otherefr. [SEP]"}
{"pre": "[CLS] the second approach is to use mismatches and is the one of the most popular choice of the most", "cit": "[CLS] recently, graph-based ranking methods have been proposed for sentence ranking and scoring, such as lexrank #otherefr and textrank #refr."}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] the most popular seed expansion methods discussed in the literature are based on wordnet #otherefr; #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? 2008. [SEP]", "cit": "[CLS] framenet #refr provides semantic frames and annotated example sentences for 4,186 verbs. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best derivation of the best", "cit": "[CLS] methods of selecting spelling and correction pairs with maximum entropy model #otherefr or similarity functions #refr have been developed."}
{"pre": "[CLS] the task of identifying the overall sentence pairs in text is a text #refr. [SEP] of the same event", "cit": "[CLS] in our experiments, we applied the sentence splitter and lemmatizer implementation of the morphadorner package6 and the stanford tokenizer"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for parsing #refr. [SEP] grammars with", "cit": "[CLS] most previous dependency parsing models have focused on projective trees, including the work of eisner #otherefr, #refr, and mcdonald"}
{"pre": "[CLS] the most common approach to word alignment is to use a word alignment model #otherefr; #refr. [SEP] model to", "cit": "[CLS] previous work in this branch mostly involves applying syntactic constraints #otherefr; #refr and syntactic features #otherefr into the models."}
{"pre": "[CLS] the system used in the conll shared task #refr is based on the genia corpus #otherefr. [SEP] shared task", "cit": "[CLS] supervised models rely on these data in order to learn preferences and constraints #otherefr, while unsupervised models apply clustering"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] ger #refr2 to recognize named entities. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the context of machine translation, itg has been explored for a variety of nlp tasks such as semantic", "cit": "[CLS] instead, we opt to use a more tractable perceptron learning approach #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford ner tagger #refr to train the training data with the feature weights. [SEP] [SEP] [SEP]", "cit": "[CLS] for each dependency from word w to head 5as pointed out by daume iii and #refr and krahmer et"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the word alignments for each word and then extract the word", "cit": "[CLS] an alternative way of accounting for phrase size is presented by #refr, who introduce structural distortion features into a"}
{"pre": "[CLS] #refr used a similar method to detect sentiment in the polarity of words in the polarity of words in", "cit": "[CLS] previous work has proposed methods for identifying subjective text that expresses opinion and distinguishing it from objective text that"}
{"pre": "[CLS] #refr use a similar approach to identify stances in which a coherent discourse. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] recently, #refr presented an approach for fully automatic, model-free evaluation of machine-generated summaries. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] the fact that it is even possible to improve the performance of a phrase-based direct translation system by tossing"}
{"pre": "[CLS] the second approach is to use a word lattice as a sequence of a sequence of a sequence of", "cit": "[CLS] the task of language modeling is to predict the next word in a sequence of words #refr. [SEP] [PAD]"}
{"pre": "[CLS] the most common approach to parsing is to use a log-linear parsing model and maximise the conditional random field", "cit": "[CLS] in the experiments, we applied both of our approaches to an hpsg parser enju #refr, and then evaluated the"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the longrange reordering model is to use of the", "cit": "[CLS] this work represents a second pilot study (after #refr) for the longer-term ptolemaios project at saarland university4 with the"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] numerous studies have contributed to the development of increasingly accurate l2p systems #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for learning and beam-search decoding #otherefr;", "cit": "[CLS] for example, klein and manning?s #otherefr dependency model with valence (dmv) could be imple- 1unrestricted head-outward automata are strictly"}
{"pre": "[CLS] the most popular approach to word alignment is the ibm models #otherefr; #refr. [SEP] model 1 and the hmm", "cit": "[CLS] this can be accomplished as in #refr and schafer and yarowsky #otherefr by creating bag-of-words context vectors around both"}
{"pre": "[CLS] the second approach is to use a small number of manually annotated corpora such as the same data set", "cit": "[CLS] and ambiguity classes have been shown to be successfully employed, in a variety of ways, to improve pos tagging"}
{"pre": "[CLS] the second approach is to use a maximum entropy model #otherefr; #refr. [SEP] model for the probability distribution over", "cit": "[CLS] this paper presents a more comprehensive analysis of errors in chinese parsing, building on the technique presented in #refr,"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a text with a text that are semantically", "cit": "[CLS] the model brings together aspects we?ve previously looked into separately: grounded semantics in #otherefr; and a more sophisticated approach"}
{"pre": "[CLS] #refr use a similar approach to compute the polarity of words in a sentence. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] opinionfinder is a state-of-the-art publicly available software package for sentiment analysis that can be applied to determining sentence-level subjectivity"}
{"pre": "[CLS] the most common approach to paraphrase generation is to use a lexicon to automatically extracted from corpora #otherefr; #refr.", "cit": "[CLS] first, using linguistic results on the semantics of french nps #otherefr, we identified predicate-argument configurations that cannot be matched"}
{"pre": "[CLS] the first is the task of identifying the polarity of a sentence, a sentence, and a document into a", "cit": "[CLS] while there seems to be no previous work investigating automatic classification, there is related work on classifying ?it?, which"}
{"pre": "[CLS] the most common approach to word sense disambiguation is to use a word or phrase based on bilingual lexicon", "cit": "[CLS] #refr] proposes to collect counts over words occurring in a four word window around the target word. [SEP] [PAD]"}
{"pre": "[CLS] the task of identifying the overall sentence pairs in text is a text #refr. [SEP] of the same event", "cit": "[CLS] predicates such as ?to present? and ?to include? have the tendency of appearing towards the very beginning or the"}
{"pre": "[CLS] the second approach is to use a word segmentation and pos tagger to tag dictionary to tag each word", "cit": "[CLS] japanese morphological analysis and lexical acquisition word segmentation for japanese is usually solved as the joint task of segmentation"}
{"pre": "[CLS] #refr proposed a method for identifying the polarity of a document by a document into a document to determine", "cit": "[CLS] several recent techniques for hypernym detection have made use of distributional semantics #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to learn a translation probabilities from", "cit": "[CLS] imposing a source tree on itg (ist-itg) constraints #refr is an extension of itg constraints and a hybrid of"}
{"pre": "[CLS] the most common approach to coreference resolution is to use a search #otherefr; #refr. [SEP] model to find the", "cit": "[CLS] the importance of coreference resolution has led to it being the subject of recent conll shared tasks #refr. [SEP]"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to learn a translation models from a", "cit": "[CLS] the first is to align the words using a ?standard? word alignement technique, such as the refined method described"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] the moses phrase-based smt system was applied #otherefr, together with giza++ #refr for alignment and mert #otherefr for tuning"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be np-hard #refr. [SEP] s", "cit": "[CLS] following the work of #refr, we implement a linear-chain crf merging system using the following features: stemmed (separated) surface"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a text with a semantic role labeling task,", "cit": "[CLS] we assigned the maximum idf to words not found in the bnc. - svm-light-tk3 #refr which encodes the basic"}
{"pre": "[CLS] the generation of referring expressions is a subject of research in natural language generation (nlg) in natural language generation", "cit": "[CLS] occasions for providing definitions arise most obviously when the user asks a question of the form \"what is ..."}
{"pre": "[CLS] #refr use a similar approach to identify stances in which a coherent discourse. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] they also analyzed us presidential debates, but with the objective of validating a topic segmentation method they proposed earlier"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a task of identifying the degree of semantic relations between words", "cit": "[CLS] there are various possibilities to determine corpus-based features that potentially cover the associations; we decided in favour of feature"}
{"pre": "[CLS] #refr use a similar approach to compute sentence importance. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] in the remainder of this paper, we first present the bayesian hidden markov model (bhmm; #refr) that is used"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] tree-to-string translation rules are generic and applicable to numerous linguistically syntax-based statistical machine translation #otherefr, tree-to-string translation #refr, and"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] although tree-based approaches potentially offer a more accurate model of translation, they are also a lot more complex and"}
{"pre": "[CLS] the first is the case for the syntactic parsing model of the semantic role labeling task of translating natural", "cit": "[CLS] this makes it possible to decompose the treebank trees into elementary trees as proposed by #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to translation lexicons from parallel corpora #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] this setting has been considered before, most notably in koehn and knight #otherefr and #refr, but the current paper"}
{"pre": "[CLS] the most common approach to this problem is to use a log-linear model #otherefr; #refr. [SEP] model to handle", "cit": "[CLS] the model we use is similar to #otherefr; #refr, which regard answer extraction as a ranking problem instead of"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be a difficult problem #otherefr;", "cit": "[CLS] our approach is built on top of the factor-based smt model proposed by #refr, as an extension of the"}
{"pre": "[CLS] the first is the case for the case of the grammar development of the translation model presented by #refr.", "cit": "[CLS] practically, a timeout mechanism and a process for recovery from unsuccessful translation (e.g., applying the idea of fitted parse"}
{"pre": "[CLS] the work of #refr is similar to ours in that the generation of referring expressions is in that the", "cit": "[CLS] although we address these user states in the tutoring domain, spoken dialogue researchers across domains and applications have investigated"}
{"pre": "[CLS] in the literature, supervised learning methods have been applied to the task of learning a small number of unsupervised", "cit": "[CLS] we add some features in #refr: trigger words (the words on the dependency path except stop words) and entity"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] due to the relatively small size of the evaluation set and instability inherent in minimum error rate training #otherefr;"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking approaches #otherefr; #refr to learning for dependency", "cit": "[CLS] approaches to unsupervised grammar induction, both for phrase-structure and dependency grammars, have typically used very simplistic models #otherefr; #refr."}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] nivre?s parser has been tested for swedish #otherefr, czech #refr, bulgarian #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for english and the english sentences. [SEP]", "cit": "[CLS] therefore, studies have recently resorted to other resources for the enhancement of parsing models, such as large-scale unlabeled data"}
{"pre": "[CLS] the most common approach to this problem is to use a large set of features #otherefr; #refr. [SEP] features", "cit": "[CLS] specifically, we use labeled dependency trees following the ?basic? variant of the stanforddependencies scheme (de marneffe and #refrb; de"}
{"pre": "[CLS] the parser is trained using the c&c parser #refr. [SEP] parser is trained on the wall street journal (wsj)", "cit": "[CLS] tile rule notation employed, including conventions for expressing abbreviations, is based on that described in #refr\\]. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with a beam search", "cit": "[CLS] iii and marcu, 2005), parsing #otherefr and word alignment #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] for instance, yang and xue #otherefr attempted to predict the existence of an ec before a word; #refr predicted"}
{"pre": "[CLS] the most common approach to content selection, is to use a topic models to identify the topics #otherefr; #refr.", "cit": "[CLS] early studies used text similarity measures such as cosine similarity #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? 2008.", "cit": "[CLS] the system was tuned in #refr improving slightly its performance. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to learn a translation models from a", "cit": "[CLS] these word alignments provide the input for later approaches that construct phrase-level translation rules which may #otherefr; #refr or"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the sentence in", "cit": "[CLS] a particularly popular model for the evaluation of text coherence is the entity-based local coherence model of #refr, which"}
{"pre": "[CLS] #refr proposed a method for learning a word clustering words from a target language model to a target language", "cit": "[CLS] for evaluation, we use p-ip score and b-cubed score #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of segmenting utterances into words, for example, johnson and colleagues have been shown to be effective in", "cit": "[CLS] in #refr, it was further reported that pure character-based models can even outperform word-based models with character combination features."}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] quasi-synchronous grammar makes no restrictions on the form of the target monolingual grammar, though dependency grammars have been used"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] #refr proposed a ?hierarchical? lexicalized rm in which the orientation (m, s, or d) is determined not by individual"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] previous works have attempted to handle morphology, decompounding and regularization through lemmatization, morphological analysis, or unsupervised techniques #otherefr; #refr."}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr.", "cit": "[CLS] closer to our use of semantic compatibility features for pronouns are the approaches of #refr and yang et al"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with a probability distribution", "cit": "[CLS] recent studies on parsing indicate that the use of unlabeled data by self-training can help parsing on the wsj"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking approaches #otherefr; #refr to learning for dependency", "cit": "[CLS] the berkeley parser has been applied to the tu?bad/z corpus in the constituent parsing shared task of the acl-2008"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with the search algorithm", "cit": "[CLS] #refr applied online learning by scoring edges in a connected graph and finding the maximum spanning tree (mst) of"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] #refr introduced minimum error rate training #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the tagging task of converting natural language processing #otherefr; #refr. [SEP] [SEP] [SEP]", "cit": "[CLS] the more similar conditions reported in previous work are those experiments performed on the wsj corpus: #refr reports 3-4%"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be np-hard #refr. [SEP] grammars", "cit": "[CLS] recently, specific probabilistic tree-based models have been proposed not only for machine translation #otherefr; #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] he 1not to mention earlier non-pcfg lexicalized statistical parsers, notably #refr for the penn treebank. also modified the treebank"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr. [SEP]", "cit": "[CLS] a tbl chunker trained on wall street journal corpus #refr maps each word to an associated chunk tag, encoding"}
{"pre": "[CLS] in the context of machine learning techniques have been applied to many natural language processing tasks with promising results,", "cit": "[CLS] based on these observations, we study a new application of alternating structure optimization (aso) #refra; ando and zhang, 2005b)"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] individual pdt systems were tuned on the gale dev10 web tune set using online-pro #refr to the positive diversity"}
{"pre": "[CLS] the feature weights were tuned using minimum error rate training (mert) #refr on the development set using minimum error", "cit": "[CLS] the second alternative used berkeleyaligner #refr, which shares information between the two alignment directions to improve alignment quality. [SEP]"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] model with the average number of the average", "cit": "[CLS] #refr introduced a treebased model of persons? eye movements in reading. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the training data is to", "cit": "[CLS] to address this concern, we chose the python #otherefr programming language and the natural language toolkit #refr, written entirely"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a text with a semantic role labeling task", "cit": "[CLS] statistical approaches such as #refr, on the other hand, use text corpora to generate personalized texts. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] polarity of a positive", "cit": "[CLS] textrank and its variants #otherefr; #refr are graph-based text ranking models, which are derived from google?s pagerank algorithm #otherefr."}
{"pre": "[CLS] the feature weights were tuned using minimum error rate training (mert) #refr on the development set using minimum error", "cit": "[CLS] parameter settings we tune our system toward approximate sentence-level bleu #otherefr,3 and the decoder is configured to use cube"}
{"pre": "[CLS] #refr proposed a method for identifying word pairs of sentences in a word in a word in a sentence", "cit": "[CLS] also developed in parallel, #refr build a pos tagger for tweets using 20 coarse-grained tags. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the translation quality is evaluated by case-insensitive bleu-4 #refr. [SEP] quality measured by bleu score #refr with the bleu", "cit": "[CLS] labeling to address the problem of limited resources, we tried to expand the training corpus by applying the corpus"}
{"pre": "[CLS] the model is trained using the averaged perceptron algorithm #refr. [SEP] score #refr with the best derivation forest, [SEP]", "cit": "[CLS] table 2: perplexities for baseline backoff lms, flms, nlms, and lm interpolation scribed in #refr (model 6). [SEP] [PAD]"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a similar to the similarity of two words in the similarity", "cit": "[CLS] it assumes that nearby words provide strong and consistent clues to the sense of a target word, see #refr."}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to learn a translation models from a", "cit": "[CLS] syntax-based preprocessing approaches that have relied on hand-written rules to restructure source trees for particular translation tasks have been"}
{"pre": "[CLS] the most common approach to this problem is to use a dictionary to model the entire document #otherefr; #refr.", "cit": "[CLS] for example, #refr has empirically reported that such global approaches achieve performance better than the ones based on incrementally"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the longrange reordering problem as a sequence of words", "cit": "[CLS] the compression rules learnt are typically syntactic tree-to-tree transformations #otherefr; #refr of some variety. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the sentence planning problem that the user is a text generation of", "cit": "[CLS] if successful, b acknowledges u (directly, gesturally or implicitly) and responds to the content of u. (b) if unsuccessful,"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of possible antecedents for the", "cit": "[CLS] there is currently support for word- net #otherefr, and uby #refr, which provides access to wordnet, wikipedia, wiktionary, germanet,"}
{"pre": "[CLS] #refr proposed a method for learning a word alignment of bilingual lexicon from monolingual data. [SEP] pairs of sentences", "cit": "[CLS] the bilda model we use is a natural extension of the standard lda model and, along with the definition"}
{"pre": "[CLS] the system is based on the moses toolkit #otherefr and the standard phrase-based smt system described in #refr. [SEP]", "cit": "[CLS] several approaches to reference-free automatic mt quality assessment have been proposed, using classification #otherefr, regression #refr, and ranking #otherefr."}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] its success stories range from parsing #refr to machine translation #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to smt is the one proposed by #refr, who use a large set of features", "cit": "[CLS] pivoting has also been used for paraphrasing and lexical adaptation #refr. #otherefr investigate pivot languages for resource-poor languages (but"}
{"pre": "[CLS] the translation system is evaluated by bleu score #refr on the nist #otherefr. [SEP] quality measured by the translation", "cit": "[CLS] translation scores are reported using caseinsensitive bleu #refr with a single reference translation. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the averaged perceptron algorithm #refr. [SEP] score #refr with the best results for the", "cit": "[CLS] examples of such efforts include work on the induction of synchronous grammars (wu and #refr and learning multilingual lexical"}
{"pre": "[CLS] #refr proposed a method for learning a word sense disambiguation which is a word sense disambiguation method to extract", "cit": "[CLS] to address the shortcoming of ds, riedel et al. #otherefr and #refr cast the relaxed ds assumption as multi-instance"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] #refr used transformation-based l arning, an error-driven learning technique introduced by eric bn11#otherefr, to locate chunks in the tagged"}
{"pre": "[CLS] #refr proposed a method for identifying paraphrases based on a bilingual parallel corpus of news stories from a target", "cit": "[CLS] most efforts approach the problem of modelling phrase meaning through vector composition using linear algebraic vector operations #otherefr; #refr,"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the longrange reordering model is to use of the", "cit": "[CLS] the model is intended to combine the lexical sensitivity of n-gram models #otherefr without he computational overhead of statistical"}
{"pre": "[CLS] the first is the task of identifying the polarity of a word is a word in a word in", "cit": "[CLS] #refr call it an understudied language analysis problem. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to learn a translation models from a", "cit": "[CLS] in particular, we integrate the models into a phrase-based system which uses bracketing transduction grammars #otherefr for phrasal translation"}
{"pre": "[CLS] the most common approach to paraphrase generation is to use a lexicon to paraphrase generation #otherefr; #refr. [SEP] words", "cit": "[CLS] sentence compression has been considered before in contexts outside of summarization, such as headline, title, and subtitle generation #otherefr;"}
{"pre": "[CLS] the first is the automatic evaluation of the automatic metric of the automatic evaluation of the automatic evaluation of", "cit": "[CLS] foma #refr is a freely available2 toolkit that allows to both build and parse fs automata and transducers. [SEP]"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of meaning that are not", "cit": "[CLS] abney, schapire, and #refr used the dataset from collins and brooks #otherefr with a boosting algorithm and achieved 85.4%"}
{"pre": "[CLS] the most common approach to paraphrase generation is to use a lexicon to automatically extracted from corpora #otherefr; #refr.", "cit": "[CLS] jacquemin #otherefr and #refr identify phraselevel paraphrases, while lin and pantel #otherefr acquire structural paraphrases encoded as templates. [SEP]"}
{"pre": "[CLS] the most common approach to parsing is to use a log-linear parsing model and maximise the conditional random field", "cit": "[CLS] #refr augment the baseline model for arabic with nine morphological features. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second approach is to use a maximum entropy model #otherefr; #refr. [SEP] model for the probability distribution over", "cit": "[CLS] for completeness, we give these results in ?4. and minimum bayes risk decoding (mbr; #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the process of annotating the word in a language #refr. [SEP] corpus by the corpus of", "cit": "[CLS] note that #refra) generalizes the approach and considers a multitude of different clustering metrics and methods, introducing a pair"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the input is to be", "cit": "[CLS] as syntactic information has been widely used in machine translation systems #otherefr; #refr, an interesting question is to predict"}
{"pre": "[CLS] the generation of referring expressions is a subject of research in natural language generation (nlg) in natural language generation", "cit": "[CLS] because a quantified expression refers to multiple entities in a domain, our work can be categorized as referring expression"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] it differs from the many approaches where #otherefr; #refr and from transfer-based systems defined by context-free grammars #otherefr. [SEP]"}
{"pre": "[CLS] the features used in the experiments are the first in the experiments reported in this paper are the experiments", "cit": "[CLS] in this paper we present our contribution to the conll 2012 shared task #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr. [SEP]", "cit": "[CLS] this strategy has been successful and commonly used in coreference resolution #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the literature, slight variations of this problem are also referred to as unsupervised learning #otherefr, and semi-supervised learning", "cit": "[CLS] examples of methods that explore multi-view constraints include self-training #otherefr, multiview learning #refr, and discriminative and generative model combination"}
{"pre": "[CLS] we use the averaged perceptron algorithm #refr to train the parameters of the averaged perceptron algorithm #otherefr. [SEP] [SEP]", "cit": "[CLS] in this paper, we will follow previous work in deriving the tag dictionary from a labeled corpus #refr; this"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr. [SEP]", "cit": "[CLS] in tiffs case, the language inodel can be estimated from a labelled corpus #otherefr(weisehedel t al., 1.993) or from"}
{"pre": "[CLS] the model is trained with a maximum entropy model #otherefr, which is trained on the penn treebank grammar #refr.", "cit": "[CLS] for the latter, since nlp is often a pipeline of several modules, where the 1-best solution from one module"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] here we will focus on some of the most widely used, namely bleu #refr, nist #otherefr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to word alignment is the hmm alignment model #refr. [SEP] model is based on the", "cit": "[CLS] the english corpus nhtsa was pos-tagged and stemmed with stepp tagger #otherefr and dependency parsed using the mst parser"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of semantic representations #otherefr. [SEP]", "cit": "[CLS] see #refr for an application of grammatical dependencies for word sense disambiguation. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? 2008. [SEP]", "cit": "[CLS] #refr and rooth et al #otherefr relied on the expectation- maximisation algorithm to induce soft clusters of verbs, based"}
{"pre": "[CLS] we use the stanford chinese segmenter #refr to tokenize the english sentences. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] uncased ibm bleu was used for evaluation #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] conversely, #refr retain the structure of the parse trees and modify the word alignments. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the work of #refr is similar to ours in that it is possible to use a more complex model", "cit": "[CLS] in a previous study #refr, the authors came up with a dozen possible syntactic forms for verb-object pairs (based"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? 2008. [SEP] ?", "cit": "[CLS] #refr showed that perfect knowledge of the allowable syntactic frames for a verb allows 98% accuracy in type assignment"}
{"pre": "[CLS] the first strategy is to use incremental sigmoid belief networks in a linear model for incremental parsing #refr. [SEP]", "cit": "[CLS] these techniques included unweighted fs morphology, conditional random fields #otherefr,23 and grammar induction #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained on the penn treebank corpus and tested on the penn treebank wall street journal, parser", "cit": "[CLS] #refr show how hlds representations can be built via ccg derivations. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders and opinion holders in opinion holders and opinion holders in", "cit": "[CLS] similarity measures such as cosine, jaccard, dice, etc #refr, can be employed to compute the similarities between the seeds"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] because other compositional lexicons are not freely available, we used the system described in #refr, which induces flippers and"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] they have also been used for shallow parsing #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the second international chinese word segmentation bakeoff #otherefr; #refr and the state-of-the-art accuracies of the state-of-the-art accuracies of", "cit": "[CLS] joint models have been widely studied to enhance multiple tasks in nlp community, including joint word segmentation and pos-tagging"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] we use moses #otherefr and k-best batch mira #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a similar to the one used in the sense of the", "cit": "[CLS] the pattern matcher runs over corpora that are: part-of-speech tagged using a widely used tagger #refr; stemmed by using"}
{"pre": "[CLS] the first is the case for the generation of the generation of referring expressions from the dialogue manager sends", "cit": "[CLS] our starting point is the work done by zettlemoyer and collins on parsing using relaxed ccg grammars #refr (zc07)."}
{"pre": "[CLS] the most popular approach to translation lexicons from the source language into target language side #otherefr; #refr. [SEP] grammars", "cit": "[CLS] motivated by the idea that a translation lexicon induced from non parallel data can be applied to mt, a"}
{"pre": "[CLS] we use the berkeley parser #refr to obtain the syntactic parse trees for each sentence and the english and", "cit": "[CLS] depending on the type of input, the systems are divided into two categories: stringbased systems whose input is a"}
{"pre": "[CLS] the first is the task of identifying the semantic relations between nominals of noun phrases and the semantic relations", "cit": "[CLS] pptt might resemble other semantic lexicons created in the long history of nlp #otherefr; #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the feature weights were tuned using mert #refr on the development set. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] optional additional models used in this evaluation are the hierarchical reordering model #otherefr, and insertion and deletion models (idm)"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with the one of", "cit": "[CLS] multilingual parsers of participants in the conll 2006 shared task #refr can handle japanese sentences. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] propbank #otherefr is an annotation of one million words of the wall street journal portion of the penn treebank"}
{"pre": "[CLS] the most popular approach to translation lexicons from parallel corpora #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] bilingual lexicon induction is the task of learning translations from monolingual texts, and typical approaches compare projected distributional signatures"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] #refr use joint morphological-lexical language models to re-rank the output of english dialectal-arabic mt, and badr et al #otherefr"}
{"pre": "[CLS] #refr use a similar approach to identify opinion polarity lexicons for opinion holders and their polarity in opinion holders", "cit": "[CLS] while it is not directly obvious that there is much information in these truncated substrings, character n-grams have successfully"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a text that are semantically", "cit": "[CLS] furthermore, we calculated the ambiguity factor 3.51 for #refr experiment shows the rmldom baselines cited for the respective experiments,"}
{"pre": "[CLS] the model is trained with a maximum entropy model #otherefr, which is trained on the penn treebank wall street", "cit": "[CLS] latent variable models have recently been of increasing interest in natural language processing, and in parsing in particular #otherefr;"}
{"pre": "[CLS] the most popular approach to translation lexicons from parallel corpora #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] a similar approach that gives weights to different subcorpora was proposed in #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a semantic role labelling task", "cit": "[CLS] in our previous work, we employed three types of features for training the re-ranker: morphosyntactic features (n-grams of morphemes"}
{"pre": "[CLS] we use the stanford chinese segmenter #refr to tokenize the english sentences. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] it is the way we interact with commercial mt services (such as google translate and microsoft translator), and the"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] sentence in a text", "cit": "[CLS] early work on speculative language detection tried to classify a sentence either as speculative or non-speculative (see, for example,"}
{"pre": "[CLS] in the second international chinese word segmentation bakeoff #otherefr; #refr and the best results are achieved by using the", "cit": "[CLS] for example, the maintainers of the stanford tagger #refr recommend tagging with a model whose per tag error rate"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] constituency parsers have advanced considerably in the last two decades #otherefr; #refr boosted by the availability of the penn"}
{"pre": "[CLS] we use the stanford ner tagger #refr to extract named entities from the text and extract named entities from", "cit": "[CLS] ldas have also been employed to reduce feature dimensions in relation detection systems #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] there have also been related approaches to clustering verb types #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] widely used dependency parsers which generate deep dependency representations include minipar #refr, which uses a declarative grammar, and the"}
{"pre": "[CLS] the similarity between two sentences is computed using the wordnet #otherefr and the wordnet similarity measure proposed by #refr.", "cit": "[CLS] latent variable models have also been used to estimate the semantic similarity between words, word usages, and texts #otherefr;"}
{"pre": "[CLS] #refr proposed a method for extracting phrases from a parallel corpus of 4,000 pairs of sentences from the same", "cit": "[CLS] parsing accuracy has been used as a preliminary evaluation of semantic models that produce syntactic structure #otherefr; wu and"}
{"pre": "[CLS] the first is the task of coreference resolution #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? the typed-similarity", "cit": "[CLS] recently, anaphoricity determination has been evaluated in the context of coreference resolution, with results showing that training an anaphoricity"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a similar to the one used in the sense of the", "cit": "[CLS] framenet #refr and propbank #otherefr contain verb syntactic patterns, but they do not have a mapping to wordnet. [SEP]"}
{"pre": "[CLS] the semantic interpretation of the semantic roles has been addressed in the context of a number of natural language", "cit": "[CLS] nitrogen #refr, a natural language generation system developed at isi, also includes a large-scale l xicon to support the"}
{"pre": "[CLS] the most common approach to this problem is to use a stochastic parse tree #otherefr; #refr. [SEP] model to", "cit": "[CLS] dredze et al yielded the second highest score1 in the domain adaptation track #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] linguistically syntaxbased models (e.g., #refr) utilize structures defined over linguistic theory and annotations (e.g., penn treebank) and guide the"}
{"pre": "[CLS] the first is the automatic evaluation of discourse structure of discourse relations #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] ~brm semantic ca, teg'orizat, ion is on several aspects similar to _th#otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to compute sentence importance. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] such a model is easily represented using a factored language model (flm), an idea introduced in #refr, and incorporated"}
{"pre": "[CLS] #refr use a similar approach to compute sentence importance. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] to improve classification accuracy, a variety of learning techniques are developed, including regression models #refr, nearest neighbour classification #otherefr."}
{"pre": "[CLS] in the domain of domain adaptation has been shown to be effective for various nlp tasks such as pos", "cit": "[CLS] additionally, we used a changed version of tnt, which utilises functionality from icemorphy, the morphological analyser of icetagger, and"}
{"pre": "[CLS] the first is the automatic evaluation of the automatic metric of the automatic evaluation of the automatic evaluation of", "cit": "[CLS] #refr applied a maximum entropy approach to the problem. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the system is a phrase-based smt system that uses the moses toolkit #otherefr and the moses toolkit #refr. [SEP]", "cit": "[CLS] for the translation direction de?en, the german source text is reordered based 1described at http://www.statmt.org/wmt11/ baseline.html on syntactic parsing"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a text that are semantically", "cit": "[CLS] there has also been some success incorporating selectional preferences #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the same as #refr, which was used to create a word in the context of a", "cit": "[CLS] unlike previous approaches #otherefr; #refr that have focused on full automation, port- dial adopts a human-in-the-loop approach were a"}
{"pre": "[CLS] the grammar is a grammar that is a given a grammar that is a grammar that of the grammar", "cit": "[CLS] the original grammar matrix consisted of types defining the basic feature geometry, types associated with minimal recursion semantics (e.g.,"}
{"pre": "[CLS] #refr proposed a joint model for unsupervised word segmentation and pos tagging and pos tagging and pos tagging and", "cit": "[CLS] while there has been much prior work on this task #otherefr; #refr, a common thread in many of these"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr.", "cit": "[CLS] the task of aligning each sentence of an abstract to one or more sentences of the body has been"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] lattice parsing originated in the speech unfortunately, not enough information was available to carry out comparison with the method"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr.", "cit": "[CLS] our use of response-based training is similar to work on learning semantic parsers from execution output such as the"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] this work most closely follows the work on semantic parsing using ccg #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the same as #refr, which was used to create a word in the context of a", "cit": "[CLS] in #refr an approach combining constituency and dependency models yields 77.6% f-score. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] score #refr with the entire candidate set. [SEP]", "cit": "[CLS] table 3) indicate that the 9various authors have remarked on the shortcomings of the muc evaluation scheme #otherefr; #refr."}
{"pre": "[CLS] the second approach is to use a maximum entropy model #otherefr; #refr. [SEP] model to predict the probability of", "cit": "[CLS] predicting the risks of publicly listed companies is of great interests not only to the traders and analysts on"}
{"pre": "[CLS] in the literature, supervised approaches have been successfully applied to a wide range of nlp tasks including pos tagging", "cit": "[CLS] #refr use a new pool of unlabeled data tagged by an ensemble of state-of-the-art taggers in every training step"}
{"pre": "[CLS] we use the berkeley parser #refr to obtain the syntactic parse trees for each sentence and the english and", "cit": "[CLS] with pruning threshold pe = 8, forest-based extraction achieves a #otherefr, which is an absolute improvement of 1.0% points"}
{"pre": "[CLS] the most common approach to paraphrase generation is to be very similar to those used in #refr and machine", "cit": "[CLS] #refr examined the performance of six mt metrics (including bleu) in evaluating the output of a lfg-based surface realizer"}
{"pre": "[CLS] the first is the automatic evaluation of discourse structure of discourse structure of discourse structure of discourse relations #refr.", "cit": "[CLS] we applied the noun clustering method of #refr to 2000 most frequent nouns in the bnc to obtain 200"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] to judge antonymous relations, we use an antonym lexicon extracted from a japanese dictionary #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of converting natural language processing #otherefr; #refr. [SEP] [SEP] [SEP] to be a given", "cit": "[CLS] this suggests using a co-training approach #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the automatic metric of text with human annotation of text segments as described in #refr. [SEP]", "cit": "[CLS] what we do assume is that the user is able to determine word boundaries, which is in reality a"}
{"pre": "[CLS] the task of identifying the semantic textual similarity of a text is a text of a text that of", "cit": "[CLS] the semantic text similarity challenge in semeval 2012 #refr explicitly defined different levels of similarity from 5 (semantic equivalence)"}
{"pre": "[CLS] the most common approach to this task is to use a semantic representation of the source sentence to parse", "cit": "[CLS] the approach of relating pairs of input structures by learning predictable syntactic transformations has shown to deliver state-of-the-art results"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be np-hard #refr. [SEP] s", "cit": "[CLS] related work today?s most widely used models for word alignment are still the models ibm 1-5 of brown et"}
{"pre": "[CLS] the system is based on the inprotk project #refr. [SEP] score for the user is not a given input", "cit": "[CLS] dialogue system we used the sammie in-car system developed in the talk project #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the stanford parser #refr and then applied to obtain the syntactic parse trees of the sentence", "cit": "[CLS] #refr take a computationally tractable set of these properties and use them to compute a score to recognize text"}
{"pre": "[CLS] the task of event extraction was first introduced by #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] lex 1 5,435 entries from nomlex #refr, framenet#otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to machine translation is to use a log-linear translation system to generate the synchronous tree", "cit": "[CLS] the other form of hybridization ? a statistical mt model that is based on a deeper analysis of the"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to learn a translation models from a", "cit": "[CLS] to complete our evaluation, we also report the effect of an in-domain lm trained on 50 word classes induced"}
{"pre": "[CLS] the first is the same as #refr, which was used for the same as the same task as the", "cit": "[CLS] comparing two sets of annotation is complicated by the fact that the set of annotations that corrects an input"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] when traversing headlist and wordlist we start with the last word added. #refr describes an optimized version of covington?s"}
{"pre": "[CLS] the generation system is based on the inprotk toolkit for the best system described in #refr. [SEP] system, pharaoh", "cit": "[CLS] the processing is .based ,on a fully lexicatized tree- adjoining grammar derived from the hpsg grammar used in the"}
{"pre": "[CLS] the language model is trained with the perceptron algorithm described in #refr. [SEP] model 2 (dtm2) #refr for the", "cit": "[CLS] at the word level, n-gram modeling techniques have been extensively refined, with stateof-the-art techniques based on smoothed n-gram counts"}
{"pre": "[CLS] we use the stanford named entity recognizer #refr to extract named entities from the twitter text to identify named", "cit": "[CLS] to classify posts? stance in dual-sided debates, previous approaches have used probabilistic #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the second international chinese word segmentation bakeoff #otherefr; #refr and the same task as the best results are", "cit": "[CLS] where van zaanen #otherefr achieved 39.2% unlabeled f-score on atis word strings, #refr reports 42.0% on the same data,"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the entire sequence of the probability", "cit": "[CLS] we apply transform and join paradigms to grammar induction, an important problem of computational linguistics that involves notoriously difficult"}
{"pre": "[CLS] the most popular approach to translation models #otherefr; #refr rely on the source side and the target side #otherefr.", "cit": "[CLS] #refr and och et al #otherefr presented approaches to re-rank the output of the decoder using syntactic information. [SEP]"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be a given by using", "cit": "[CLS] this is manifestly not the case, as is shown by the following list of translation pairs selected from our"}
{"pre": "[CLS] the system is based on the moses toolkit #otherefr and the moses toolkit #refr. [SEP] implementation of the best", "cit": "[CLS] morphologies are written in a variant of two-level morphology #refr, and grammars in a unification-based formalism #otherefr. [SEP] [PAD]"}
{"pre": "[CLS] the most common approach to parsing is to use a linear model trained on a maximum spanning tree (mst)", "cit": "[CLS] the multilingual track of the conll-2007 shared task #refr addressed also the task of dependency parsing of hungarian. [SEP]"}
{"pre": "[CLS] the first is the task of identifying the overall sentence pairs from the target sentence using the system of", "cit": "[CLS] table 1 compares the performance level obtained using all the features with that of loose reimplementations of the systems"}
{"pre": "[CLS] the most popular approach to word alignment is the hmm alignment model #refr. [SEP] model is based on the", "cit": "[CLS] for instance, functional words in one language tend to correspond to functional words in another language #refr, and the"}
{"pre": "[CLS] in the context of sentiment analysis and opinion mining has been used to improve the performance of sentiment analysis", "cit": "[CLS] for example, information retrieval systems can provide affective or informative articles separately #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the system is based on the stanford parser #refr and the stanford parser #otherefr. [SEP] system #otherefr. [SEP] system", "cit": "[CLS] however, the semantic information contained even in a coreference corpus of thousands of documents is insufficient to generalize to"}
{"pre": "[CLS] the dialogue manager (dm) is the one of the most common approach to nlg systems that of the dialogue", "cit": "[CLS] commandtalk #otherefr, circuit fix-it shop #refr and trains-96 #otherefr are spoken language systems but they interface to simulation or"}
{"pre": "[CLS] the system is based on the stanford parser #refr and the stanford parser #otherefr. [SEP] system #otherefr. [SEP] system", "cit": "[CLS] the code and data are available from http://www .delph-in.net/crawler/, for replicability #refr. attributed to imperfect parser coverage. [SEP] [PAD]"}
{"pre": "[CLS] the translation quality is measured by bleu #refr. [SEP] score #refr with the bleu score of the nist #otherefr.", "cit": "[CLS] concerning the former, we trained an unsupervised model with the berkeley aligner4, an implementation of the symmetric word-alignment model"}
{"pre": "[CLS] the model is trained using the moses decoder #refr. [SEP] score #refr with the decoder that the decoder is", "cit": "[CLS] chiang #otherefr distinguishes statistical mt approaches that are ?syntactic? in a formal sense, going beyond the finite-state underpinnings of"}
{"pre": "[CLS] the first is the case for the syntactic analysis of the english lexical substitution task of the penn treebank", "cit": "[CLS] another application of hard clustering methods #otherefr or the atr decision-tree part-of- speech tagger #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] the other used the statistical machine translation that had been created by using the ibm model 4 obtained from"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] for preprocessing, a similar approach to that shown in #refr was employed, and the mada+tokan system for disambiguation and"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] this includes parsing and relation extraction #refr, entity labeling and relation extraction #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the case of dependency parsing, previous work has shown that it is possible to improve the performance of", "cit": "[CLS] for the following ensemble experiments we make use of both #refr and stanford?s #otherefr constituent parsers. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the second approach is to use a small number of seed examples for training data #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] for natural language problems in general, of course, it is widely recognized that significant accuracy gains can often be"}
{"pre": "[CLS] the first is the automatic evaluation of the automatic metric of the automatic evaluation of the automatic evaluation of", "cit": "[CLS] we shall also attempt to learn an unsupervised normalization model with only monolingual data, similar to the work for"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] this means that the approach for sa should be switched from the rather shallow analysis techniques used for text"}
{"pre": "[CLS] we use the stanford ner tagger #refr to train the classifier to extract named entities from the training data.", "cit": "[CLS] this second step is typically achieved using greedy heuristics #otherefr, although more sophisticated clustering approaches have been used, too,"}
{"pre": "[CLS] the second approach to the problem of word segmentation is to use a word lattice #otherefr; #refr. [SEP] is", "cit": "[CLS] it is unclear, however, what kind of metric would be most suitable for the evaluation of string realisations, so"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] pradhan et al, 2005), analyzing the complex input ? syntax trees #refr, exploiting the complicated output ? the predicate-structure"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] unlike previous research on identifying the subjective character and the polarity of phrases and sentences (#refr and many others),"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] in future work, we plan to investigate the unexpected drop in hypertagger performance on our ne-collapsed corpus, which we"}
{"pre": "[CLS] #refr proposed a method for identifying paraphrases based on a bilingual parallel corpus of news stories from a target", "cit": "[CLS] of course, identifying the scope of negation is a hard problem in its own right?see e.g. the *sem 2012"}
{"pre": "[CLS] in the context of text summarization has been addressed by many researchers #otherefr; #refr, and other researchers #otherefr; yu", "cit": "[CLS] #refr studied what edits people use to create summaries from sentences in the source text. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of identifying the semantic textual similarity of a text is a text of a text that of", "cit": "[CLS] even though #refr position carmeltc in the context of essay grading, it may be considered to deal with short"}
{"pre": "[CLS] the task of semantic textual similarity #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? ? ? ? ?", "cit": "[CLS] beltagy et al #otherefr proposed a hybrid approach to sentence similarity: they use a very #refr give an amusing"}
{"pre": "[CLS] we use the berkeley parser #refr to obtain the syntactic parse trees for each sentence and the sentence and", "cit": "[CLS] first, at the root node, we apply rule r1 preserving top-level word-order between english and chinese, (r1) ip(x1:npb x2:vp)?"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] [SEP] is a set of semantic similarity to", "cit": "[CLS] and this is despite the fact that the availability of easy-to-use libraries for efficient information access is known to"}
{"pre": "[CLS] the most common approach to deriving selectional preference acquisition is to use a lexicon to measure the similarity between", "cit": "[CLS] recent computational research on english phrasal verbs have been focused on increasing the coverage and scalability of phrasal verbs"}
{"pre": "[CLS] the first is the automatic metric of discourse structure of text #otherefr; #refr. [SEP] [SEP] is the one of", "cit": "[CLS] currently, publicly available lists of dcs already exist for english #otherefr, german #refr, and french #otherefr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] #refr used a similar method to identify the polarity of words in a sentence. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] researchers have found this at various levels of analysis, including the manual annotation of phrases #otherefr, sentiment tagging of"}
{"pre": "[CLS] #refr use a similar approach to compute sentence importance. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] iii and marcu, 2002; martins and smith, 2009; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders and predict the polarity of words in a sentence and", "cit": "[CLS] several reports have shown that negations and modalities are sentiment-relevant #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] as evaluation measures, we use the standard bleu #refr as well as ribes #otherefra), a reorderingbased metric that has"}
{"pre": "[CLS] the english side of the parallel corpus was parsed using the berkeley parser #refr, which was parsed with the", "cit": "[CLS] unification based transfer our approach ismore transfer oriented than some other unification based approaches to mt (e.g., #refr. [SEP]"}
{"pre": "[CLS] the first is the task of identifying the semantic textual similarity of a text of a text segments of", "cit": "[CLS] the idea of pseudowords was originally proposed by gale et al#otherefr, word sense induction #refr or studies concerning the"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] 12although mate?s performance was not significantly better than berkeley?s in our setting, it has the potential to tap richer"}
{"pre": "[CLS] the model is trained using the averaged perceptron algorithm #refr. [SEP] score #refr with the best results for the", "cit": "[CLS] this is the same set-up used by liang and klein #otherefr, #refr, and johnson and goldwater #otherefr. [SEP] [PAD]"}
{"pre": "[CLS] the unsupervised pos tagging problem has been addressed by #refr and #otherefr for a number of languages and has", "cit": "[CLS] the common practice in the literature is to report mean results over several random initializations of the algorithm #otherefr;"}
{"pre": "[CLS] the translation quality is measured by bleu #refr. [SEP] [SEP] = 4. [SEP] = 4. [SEP] = ? i", "cit": "[CLS] callison-burch et al #otherefr pioneered the exploration of monolingual post-editing within the mt community, an approach extended more recently"}
{"pre": "[CLS] the first step is to use a stochastic model that predicts a probability distribution over a set of possible", "cit": "[CLS] human judgements on information structuring for both interand intra-sentential units are known to have low agreement #otherefr; #refr. [SEP]"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal (wsj) corpus #refr. [SEP] trees and the penn", "cit": "[CLS] fortunately, it comes with much headroom for improving accuracy, including a tunable margin parameter c for the classifier, richer"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with a beam search", "cit": "[CLS] in terms of purning strategies, they can be broadly divided into two categories: the first group #otherefr uses soft"}
{"pre": "[CLS] the most popular approach to word alignment is the hmm alignment model proposed by #refr. [SEP] model is based", "cit": "[CLS] there have been several research papers on using mturk to help natural language processing tasks, #refr used mturk to"}
{"pre": "[CLS] the task of identifying the semantic textual similarity of a text is a text of a text that of", "cit": "[CLS] the combination of similarity scores with a machine-learning algorithm to obtain a unified effect for a particular task is"}
{"pre": "[CLS] the task of identifying the semantic textual similarity of a target of a target word is a given target", "cit": "[CLS] the cross-pair similarity model uses: (1) a tree similarity measure kt (?1, ?2) #refr that counts the subtrees that"}
{"pre": "[CLS] the first is the case for the automatic evaluation of the automatic evaluation of the automatic evaluation of the", "cit": "[CLS] this process is the reverse operation of the noun phrase decomposition described in #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to identify the topics of discourse relations in terms of discourse relations in a", "cit": "[CLS] while several text segmentation systems have been proposed to date, we decided to use a graphbased segmentation algorithm using"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] the factored translation model combines features in a log-linear fashion #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to learn a translation models from a", "cit": "[CLS] #refr introduced ?factors? to phrase-based mt to explicitly capture arbitrary features in the phrase-based model. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the wsd system is based on the standard approach described in #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] the n-gram models were built using the irstlm toolkit #otherefr on the dewac corpus #refr, using the stopword list"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for parsing #otherefr; #refr for parsing", "cit": "[CLS] in terms of speed, our parser outperforms all other transitionbased parsers; it takes about 9 milliseconds per 7#refr and"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] there are number of different existing approaches for identifying semantic roles, varying from traditional parsing approaches, for example using"}
{"pre": "[CLS] the first is the automatic evaluation of discourse coherence evaluation in the discourse structure of the discourse structure of", "cit": "[CLS] the most commonly used paradigm is that of markov decision processes and partially observable markov decision processes, where the"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for parsing #refr. [SEP] grammars with", "cit": "[CLS] #refr applied a k-best maximum spanning tree algorithm to non-projective dependency analysis, and showed that k-best discriminative reranking improves"}
{"pre": "[CLS] the wsd system used for wsd in the experiments was based on the senseval-3 english lexical sample task #refr.", "cit": "[CLS] using mrds for wsd was suggested in #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders and predict the polarity of words in a sentence and", "cit": "[CLS] as #refr and feng et al. #otherefr notice, a dense graph with a large number of nodes is susceptible"}
{"pre": "[CLS] the task of identifying the semantic relations between nominals of noun phrases and their arguments are also key to", "cit": "[CLS] we chose svms as they have been shown to perform well on a variety of ner tasks, for example"}
{"pre": "[CLS] the translation quality is measured by bleu #refr. [SEP] score #refr with the bleu score of the translation quality.", "cit": "[CLS] simard et al #otherefr, #refr) is a popular method 1http://www.statmt.org/wmt13 for improving outputs of a rule-based mt system. [SEP]"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] we lower-cased and tokenized all terms by the following analyzers: juman8 for japanese, the memm-based pos tagger9 for english,"}
{"pre": "[CLS] the most common approach to coreference resolution is to use a conditional random field #otherefr; #refr. [SEP] model to", "cit": "[CLS] previously, domain adaptation learning has been successfully used in other nlp tasks such as relation extraction #otherefr, name entity"}
{"pre": "[CLS] the first is the automatic evaluation of discourse relations #refr. [SEP] quality of the discourse markers in the discourse", "cit": "[CLS] in addition, the influencer?s ideas or language may be adopted by others and others may explicitly recognize influencer?s authority.2"}
{"pre": "[CLS] the feature weights were tuned using minimum error rate training (mert) #refr on the development set using minimum error", "cit": "[CLS] pareto-optimality is a natural way to think about multi-metric optimization and multi-metric optimization (mmo) was recently explored using the"}
{"pre": "[CLS] the system is based on the inprotk project #refr. [SEP] score for the number of the user is not", "cit": "[CLS] our response to these considerations has been to design and implement a software environment called gate - a general"}
{"pre": "[CLS] the first is the stanford parser #refr and then applied to obtain the syntactic parse trees of the sentence", "cit": "[CLS] the text and hypothesis are converted to typed dependency graphs produced by the stanford parser #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] pe , pe0 ) 4this prior is similar to one used by denero et al #otherefr, who used the"}
{"pre": "[CLS] the second approach is to use a small number of seed examples for example outputs #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] recent online learners update both parameters and the estimate of their confidence #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr and toutanova et al #otherefr proposed a method for inducing syntactic parse selection of a bilingual parallel corpus", "cit": "[CLS] the inside probabilities of complete-links (n~(i,j), lt(i,j)) and complete-sequences (sr(i,j), sl(i,j)) are as follows. j -1 /3t~(i,j) = ~"}
{"pre": "[CLS] the first is the task of identifying the semantic relations between nominals of noun phrases and the verb classes", "cit": "[CLS] this seems to be because they worked particularly well with the adjective and pronoun data in the full dataset."}
{"pre": "[CLS] the second approach is to use a semantic similarity measure and is based on the similarity of the similarity", "cit": "[CLS] in order to make different natural language processing tasks be able to help each other, jointly modeling methods become"}
{"pre": "[CLS] the most common approach to semi-supervised learning algorithms use a small set of seed examples #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] the feature combinations play an essential role in obtaining a classifier with state-of-the-art accuracy for several nlp tasks; recent"}
{"pre": "[CLS] the first is the automatic evaluation of discourse coherence models in discourse structure of the most previous work on", "cit": "[CLS] next we tokenize this preprocessed message using the nltk tokenizer #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to learn a translation models from a", "cit": "[CLS] the ? sign denotes the confidence bounds estimated via bootstrap resampling #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the word alignments for each sentence and then extract the word", "cit": "[CLS] table 1: statistics about arabic phrases in the nist- 2004 large data track. translation including the joint probability phrasebased"}
{"pre": "[CLS] the first is the case for semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] in the future, we plan to relax the assumption of fully labeled training data, and to allow for learning"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the longrange reordering problem as a source sentence into", "cit": "[CLS] in our experiments we use a grammar with a start symbol s, a single preterminal c, and two nonterminals"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to learn a phrase pairs of", "cit": "[CLS] a very popular approach is to automatically learn reordering rules based on pos tags or syntactic chunks #otherefr; #refr."}
{"pre": "[CLS] the most common approach to semantic textual similarity is to use a similarity measure which is to automatically extracted", "cit": "[CLS] it has become standard in clustering approaches to distributional semantics to assign types to predicates before clustering, and only"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr. [SEP]", "cit": "[CLS] these strategies have numerous applications, including settling disputes regarding the authorship of old and historically important documents #otherefr, and"}
{"pre": "[CLS] we use the stanford ner tagger #refr to train the training data with the feature weights, and then used", "cit": "[CLS] labeled alignments are also used by #refr to train a crf word alignment model. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for each sentence in the sentence with", "cit": "[CLS] in contrast to similar proposals for cfgs #refr, which impose no formal restrictions on the nonterminals x, y, z"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a task of identifying the degree of semantic relations between words", "cit": "[CLS] lsa #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the approach of #refr is based on the assumption that the assumption of the assumption that the assumption of", "cit": "[CLS] in natural-language processing (nlp), language and translation are often modeled using some kind of grammar, automaton or transducer, such"}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders and predict the polarity of subjective words in subjective words", "cit": "[CLS] #refr present an unsupervised opinion analysis method for debateside classification. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the context of machine translation, itg has been explored for a variety of nlp tasks, including pos tagging", "cit": "[CLS] the actual syntactic information that we transfer consists of purely monolingual lexical attachment statistics computed on an annotated source"}
{"pre": "[CLS] the tempeval community focused on the classification of the temporal relations between events and their arguments and their arguments", "cit": "[CLS] however, in english, not all discourse markers unambiguously identify causality #otherefr - for example, #refr proposed a corpus of"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be np-hard #refr. [SEP] s", "cit": "[CLS] to address the translation of discourse connectives, source-side pre-processing has been used to annotate surface forms either in the"}
{"pre": "[CLS] the first is the case for the generation of referring expressions from the dialogue manager sends and has been", "cit": "[CLS] this topic description along with other parts of the 779' fundamental grammar of japanese was implemented on a unifica{ion-based"}
{"pre": "[CLS] the task of relation extraction has received much attention in recent years #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] while traditional information extraction #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained on the penn treebank corpus and tested on the penn treebank wall street journal, parser", "cit": "[CLS] the most important tree-bank transformation in the literature is lexicalization: each node in a tree is labeled with its"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a word sense disambiguation of", "cit": "[CLS] vecchi et al. have applied the additive and multiplicative models of #refr and adjective-specific linear maps of baroni and"}
{"pre": "[CLS] the model is based on the simple and effective model of #refr. [SEP] score for the task of a", "cit": "[CLS] we used naturalowl #otherefr; #refr, an nlg system for owl ontologies that relies on a pipeline of content selection,"}
{"pre": "[CLS] the second approach is to use a word lattice as a sequence of a sequence of word segmentation and", "cit": "[CLS] the existing approach for correcting the spelling error in the languages that have no word boundary assumes that all"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the sentences and extract dependency trees for each sentence with dependency", "cit": "[CLS] the third resource is emotion-related words from framenet #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr and toutanova et al #otherefr proposed a method for inducing ccgs to use a morphological analyzer for pos", "cit": "[CLS] there has been a great deal of recent interest in the unsupervised discovery of syntactic structure from text, both"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for learning and beam-search decoding #otherefr;", "cit": "[CLS] for example, given a sentence as input: part-of-speech (pos) tagging involves finding the appropriate pos tag sequence #refr; parsing"}
{"pre": "[CLS] the translation quality is measured by bleu #refr. [SEP] score #refr with the bleu score of the translation quality", "cit": "[CLS] the meant family of metrics #otherefr adopt the principle that a good translation is one where a human can"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] perhaps the best way to think about this problem is to contrast our work with that of #refr. [SEP]"}
{"pre": "[CLS] the first is the automatic metric of discourse structure of discourse structure of discourse structure of discourse segments and", "cit": "[CLS] this distinguishes collaborative negotiation from non-collaborative n gotiation such as labor negotiation #otherefr; #refr. others' beliefs in order to"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with a probability distribution", "cit": "[CLS] parsing is also key to the latest advances in machine translation, which translate syntactic phrases #otherefr; #refr. [SEP] [PAD]"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] unsupervised phrasal itgs recently, phrase alignment with itgs #otherefr; #refr and parameter estimation with gibbs sampling #otherefr are popular."}
{"pre": "[CLS] the second approach is to use a maximum entropy model #otherefr; #refr. [SEP] model for the probability mass to", "cit": "[CLS] for brevity, we omit the smoothing details of bbn is model #otherefr for a complete description); we note that"}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders and predict the polarity of words in a sentence and", "cit": "[CLS] multi-aspect sentiment ranking the goal of multi-aspect sentiment classification is to predict a set of numeric ranks that reflects"}
{"pre": "[CLS] we use the stanford ner tagger #refr to train the training data with the feature weights, and then used", "cit": "[CLS] for punctuation prediction, huang and zweig #otherefr used finite state and multi-layer perceptron method; #refr used conditional random fields;"}
{"pre": "[CLS] #refr use a similar approach to identify the text in which the text in which the context of a", "cit": "[CLS] in these experiments, the length of phrases was limited to maximum 7 words. sion, whereas lnc.ntc slightly sacrifices the"}
{"pre": "[CLS] the first is the case for the generation of referring expressions from the linguistic phenomena of referring expressions and", "cit": "[CLS] #refr and phonology #otherefr have been taken. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to paraphrase generation has been the extraction #otherefr; #refr. [SEP] input text is based on", "cit": "[CLS] in contrast to previous attempts to use crowdsourcing to obtain parallel data, in which workers performed translation #refr, our"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for parsing #refr. [SEP] grammars with", "cit": "[CLS] successful discriminative parsers have relied on generative models to reduce training time and raise accuracy above generative baselines #otherefr;"}
{"pre": "[CLS] the model is trained using the averaged perceptron algorithm #refr. [SEP] score #refr with the decoder that is trained", "cit": "[CLS] we train the parameters of the model using averaged perceptron #refr modified for structured outputs, but can easily fit"}
{"pre": "[CLS] the task of identifying the event descriptions of event extraction and event descriptions has been well studied in the", "cit": "[CLS] automatic work on bridging distinguishes between recognition #otherefr; #refr and antecedent selection. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the system used in this paper is based on the c&c parser #otherefr shared task #refr. [SEP] [SEP] [SEP]", "cit": "[CLS] columns 1?3 were predicted using the tagger of #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] [SEP] is a set of meaning that are", "cit": "[CLS] still the alternative of using an automatically clustered hierarchy has other disadvantages, a particular problem being that techniques so"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a text that are semantically", "cit": "[CLS] as different applications require different types of collocations that are often not found in dictionaries, automatic extraction of collocations"}
{"pre": "[CLS] the first is the one of the most popular approach to use a word in which is to use", "cit": "[CLS] another approach, using maximum entropy methods, introduces a parameter for trigger pairs of mutually informative words, so that the"}
{"pre": "[CLS] the task of segmenting utterances into words, for example, johnson and colleagues have been shown to be a number", "cit": "[CLS] fce what we call the fce corpus is a small sample of the first certificate in english portion of"}
{"pre": "[CLS] #refr use a similar approach to identify the polarity of words in a sentence and their polarity in a", "cit": "[CLS] our method for such an extraction is inspired by the previous work by tomokiyo and hurst #refr. [SEP] [PAD]"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] model with the average number of the average", "cit": "[CLS] #refr took an unsupervised approach for semantic parsing based on self-training driven by confidence estimation. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking approaches #otherefr; #refr to learning for dependency", "cit": "[CLS] in addition, in this study we use english as the source resource-rich language, but our methodology can be applied"}
{"pre": "[CLS] in the context of sentiment analysis and opinion mining has been used to improve the performance of a variety", "cit": "[CLS] in semeval-2013?s task 2b on sentiment analysis in twitter, given a twitter message, i.e. a tweet, the goal is"}
{"pre": "[CLS] the first is the automatic evaluation of discourse structure of discourse structure of discourse structure of discourse structure of", "cit": "[CLS] the automatic detection of discourse relations, such as causal, contrast or temporal relations, is useful for many applications such"}
{"pre": "[CLS] the second approach is to use a small number of seed examples for example outputs #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] #refr used hidden markov models (hmms) for csl, where the observations were word sequences and the hidden states were"}
{"pre": "[CLS] the most popular paraphrase collection has been used to paraphrase the source sentences into either from comparable corpora #otherefr;", "cit": "[CLS] #refr showed how one can discover paraphrases that do not share any translation in one language by traversing a"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the sentence in", "cit": "[CLS] #refr model literal vs. nonliteral classification as a word sense disambiguation task and use a clustering algorithm which compares"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be np-hard #refr. [SEP] s", "cit": "[CLS] first, it clarifies the model, in the same way that #otherefr; #refr elucidate other machine translation models in easily-grasped"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] event and semantic role", "cit": "[CLS] for some experiments, relations were considered between noun phrases, labelled using lt chunk #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford named entity recognizer #refr to extract named entities from the stanford named entities from the", "cit": "[CLS] #refr describe an approach to template learning without labeled data. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to translate the phrases as the", "cit": "[CLS] our french?english system (?3) showcased our group?s syntactic system with coarsened nonterminal types #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of possible senses in the", "cit": "[CLS] this makes supervised approaches to wsd a difficult proposition #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the berkeley parser #refr to obtain the syntactic parse trees for each sentence and the english and", "cit": "[CLS] depending on the type of input, these efforts can be divided into two broad categories: the string-based systems whose"}
{"pre": "[CLS] the first is the task of identifying the mentions of a text with a text that is a text", "cit": "[CLS] mu?#refr used co-training for coreference resolution, a semi-supervised method. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] trees are converted to cnf first by binarizing using standard linguistically-motivated techniques #refr; johnson, 1998b). [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] event and semantic role", "cit": "[CLS] each segment represents passages or topics, and provides a coarsegrained study of the linear structure of the text #otherefr;"}
{"pre": "[CLS] the task of identifying the semantic relations between nominals of a text is also relevant to the target language", "cit": "[CLS] early research on ne translation used phonetic similarities, for example, to mine the translation ?mandelson????????[mandeersen] with similar sounds #otherefr;"}
{"pre": "[CLS] the first is the task of coreference resolution #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] ? ? ? ?", "cit": "[CLS] with notable exceptions #otherefr; #refr supervised approaches to coreference resolution are often realized by pairwise classification of anaphor-antecedent candidates."}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] shared", "cit": "[CLS] participants in these shared tasks have introduced dozens of systems for event extraction, and the resulting methods have been"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] under a phrase based translation model #otherefr; #refr, this distinction is important and will be discussed in more detail."}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be np-hard #refr. [SEP] s", "cit": "[CLS] past work has synchronously binarized such rules for efficiency #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to selectional preference acquisition is to use a lexicon to identify the lexical semantics to", "cit": "[CLS] an example of the latter are topic models #otherefr; ?o se?#refr, or word sense disambiguation #otherefr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the features used in our experiments are the coreference resolution system of ng and #refr. [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] we use the dataset from the conll-2011 shared task, ?modeling unrestricted coreference in ontonotes? #refr2. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of coreference resolution #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? the typed-similarity", "cit": "[CLS] these include #otherefr news articles taken from the czech national corpus; (4) the naist text corpus #refrb), which consists"}
{"pre": "[CLS] the first is the case for the syntactic annotation of the penn treebank #otherefr; #refr, which we use the", "cit": "[CLS] some previous research on speculation detection and boundary determination over biomedical data has been done by medlock & briscoe"}
{"pre": "[CLS] the first is the automatic evaluation of the automatic metric of the automatic evaluation of the automatic evaluation of", "cit": "[CLS] while #otherefr concentrate on the description of the task of the isle computational lexicon working group and address the"}
{"pre": "[CLS] the most common approach to word alignment is to use a log-linear model #otherefr; #refr. [SEP] model to handle", "cit": "[CLS] some supervised models receive syntax trees as their input and use them to generate features and to guide the"}
{"pre": "[CLS] in the context of machine translation, the machine learning approach has been applied to a number of tasks in", "cit": "[CLS] we use the strong subjectivity cues from the lexicon of #refr as a measurement of emotion. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the longrange reordering problem as a sequence of words", "cit": "[CLS] alternatively, for a somewhat different but related formulation of the probability model, the sampling method developed by #refr can"}
{"pre": "[CLS] the first is the stanford ner tagger #refr, which is based on the most popular one. [SEP] [SEP] [SEP]", "cit": "[CLS] for preprocessing, the stanford parser #refr and named entity recognizer #otherefr are deployed. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the approach of #refr is based on the assumption that the assumption of the assumption that the assumption of", "cit": "[CLS] finding the best derivation, on the other hand, is possible in polynomial time #otherefr; #refr, and thus, most nlp"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal (wsj) corpus #refr. [SEP] trees and the penn", "cit": "[CLS] however, recent results in non-projective dependency parsing, especially using data-driven methods, indicate that most non-projective structures required for the"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the training data is to", "cit": "[CLS] we start from 2 training instances; results were the same or slightly better with 10 or 100 instances. work"}
{"pre": "[CLS] the second approach is to use a small number of approximate inference rules #otherefr; #refr. [SEP] model to handle", "cit": "[CLS] this work employs conditional markov models (cmms) #refr to diacritize semitic (and other) languages and requires only diacritized texts"}
{"pre": "[CLS] the wsd system used for wsd in the semeval 2013 task #refr was the semeval 2013 task #otherefr. [SEP]", "cit": "[CLS] in our previous work #refr we developed a word-sense induction #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a similar to the one used in the sense of the", "cit": "[CLS] finally, inducing lexical semantics from distributional data #otherefr; #refr) is also a form of surface cueing. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] #refr proposed a method for learning a word clustering based on a probabilistic model that learns vector space representations", "cit": "[CLS] there is a long tradition of nlp research on representations, mostly falling into one of four categories: 1) vector"}
{"pre": "[CLS] the most popular approach to translation lexicons from the source language into target language side #otherefr; #refr. [SEP] grammars", "cit": "[CLS] this has improved performance when applied to phenomena including segmentation, morphological analysis, and more recently source langage word order"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] penn treebank tags using the morce tagger #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with the one of", "cit": "[CLS] this results in cubic-time decoders for arc-factored and sibling second-order models #otherefr; mcdonald and #refr, and quartic-time for grandparent"}
{"pre": "[CLS] the first is the task of converting natural language processing #otherefr; #refr. [SEP] [SEP] [SEP] to be a given", "cit": "[CLS] there have been several efforts to apply machine learning techniques to the same task #otherefr #refr #otherefr. [SEP] [PAD]"}
{"pre": "[CLS] the evaluation metric is the bleu score #refr on the test set and the test set for the test", "cit": "[CLS] umelb [cross-lingual, pivoting, compositional] #refr adopts both pivoting and cross-lingual approaches. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the words is to the", "cit": "[CLS] the dependency labels were provided by the bohnet parser #refr for english and by magyarlanc 2.0 #otherefr for hungarian."}
{"pre": "[CLS] the first is the automatic evaluation of discourse structure of discourse structure of discourse relations #refr. [SEP] [SEP] [SEP]", "cit": "[CLS] there has been some prior effort to annotate anaphora and coreference using games with a purpose as a method"}
{"pre": "[CLS] in the context of machine translation, itg has been explored for a variety of nlp tasks such as semantic", "cit": "[CLS] paraphrasing, the task of finding sets of semantically equivalent surface forms, is crucial to many natural language processing applications,"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] algorithm with the average number of the average number", "cit": "[CLS] there is considerable interest in learning computational grammars.1 while much attention has focused on learning syntactic grammars either in"}
{"pre": "[CLS] in the context of spoken dialogue systems #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] when relations (i.e., two-place properties) are taken into account at all (e.g., #refr, the motivating examples are kept so"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a task of identifying the degree of semantic relations between words", "cit": "[CLS] o s?e#refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to paraphrase generation is to use a lexicon to paraphrase database into a single sentence", "cit": "[CLS] the techniques proposed have a strong relationship to the type of text corpus used 3this verse from apollinaire?s nuit"}
{"pre": "[CLS] the conll shared tasks on dependency parsing in 2006 and 2007 #refr have been shown to be a wide", "cit": "[CLS] previous research dealt with such information using re-ranking #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the automatic evaluation of discourse structure of discourse structure of discourse structure of discourse structure of", "cit": "[CLS] grounding sports commentaries in game events is a specific instance of this problem that has attracted attention #refr, in"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the entire sequence of the probability", "cit": "[CLS] that parsing model has since been extended to make unsupervised learning more feasible #otherefr; #refr; spitkovsky et al 2012b)."}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] #refr used this approach for combining a graph-based and a transition-based parser and achieved excellent results on test sets"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] a small number of efforts has been dedicated to the simultaneous learning of the probabilities of phrase translation pairs"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] while these methods do not yet compete with fully supervised approaches, they can drastically outperform both unsupervised methods #otherefr"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a single sentence is a", "cit": "[CLS] the component models were also used in other senseval-3 tasks: semantic role labeling #otherefr and the lexical sample tasks"}
{"pre": "[CLS] the second approach is to use a small number of seed examples for example outputs #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] transformation-based learning has been used to tackle a wide range of nlp problems, ranging from part-ofspeech tagging #otherefr to"}
{"pre": "[CLS] topic models have been successfully applied to a wide range of tasks including sentence compression #otherefr; #refr, sentence compression", "cit": "[CLS] coherence measured as a log sum of cooccurrence proportions of each topic?s high frequency words across multiple documents #refr."}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] figure 2: a simple lexicalized parse tree. criminative models described in #otherefr and the stochastic lexicalized tag models in"}
{"pre": "[CLS] the most popular approach to translation lexicons from parallel corpora #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] the baseline translation system is the alignment templates system with scaling factors #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be np-hard #refr. [SEP] s", "cit": "[CLS] following popular approaches to learning feature weights in the machine translation community #otherefr; #refr, we use the minimum error"}
{"pre": "[CLS] in this paper, we propose a method for learning a translation model from parallel corpus using the web corpus", "cit": "[CLS] these include biases like the size principle #otherefr; #refr, or innate prespecifications like universal grammar in the principles and"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the sentences and the syntactic parse trees of the sentences and", "cit": "[CLS] this is an advantage of treebased compression technique #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] the extended lexicon model which we apply is motivated by a trigger-based approach #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in this paper, we propose a method for learning a sequence of word alignment model #refr. [SEP] [SEP] [SEP]", "cit": "[CLS] transliteration methods typically fall into two categories: generative approaches #otherefr that try to produce the target transliteration given a"}
{"pre": "[CLS] #refr proposed a method for learning a word clustering based on a probabilistic model that learns vector space representations", "cit": "[CLS] other work uses human-annotated corpora, such as the rst bank #otherefr, or adhoc annotations, used by (gi#refr. [SEP] [PAD]"}
{"pre": "[CLS] we use the moses toolkit #refr to train the weights for the feature weights for the log-linear models. [SEP]", "cit": "[CLS] there exists a variety of different metrics, e.g., word error rate, position-independent word error rate, bleu score #refr, nist"}
{"pre": "[CLS] the first is the task of coreference resolution #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? the typed-similarity", "cit": "[CLS] semantic knowledge has also been extracted from wordnet and unannotated corpora for computing the semantic compatibility/similarity between two common"}
{"pre": "[CLS] the second dataset is the use of a word sense disambiguation algorithm which is based on the most similar", "cit": "[CLS] recently, graph-based methods have been employed to wsi #otherefr; #refrb). [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the generation of referring expressions is a subject of research in natural language generation (nlg) in natural language generation", "cit": "[CLS] they go on to argue that besides eliminating a great deal of unnecessary 'generation from first principles,' this approach"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from the sentence pairs in the input sentence by", "cit": "[CLS] the training data released by the task organizers comes from the publicly available fce corpus #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to handle the hierarchical phrase-based model #refr", "cit": "[CLS] the use of various synchronous grammar based formalisms has been a trend for statistical machine translation #otherefr; #refr. [SEP]"}
{"pre": "[CLS] we use the stanford ner tagger #refr to train the classifier to generate the feature templates and the feature", "cit": "[CLS] our joint system outperformed the baseline with statistical significance (with p < 0.05 and according to a bootstrap resampling"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with a beam search", "cit": "[CLS] we also show how perceptron learning with beam-search #refr can be extended to handle the additional ambiguity, by adapting"}
{"pre": "[CLS] the second approach is to use a small number of seed examples for example outputs #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] semi-markov crf #otherefr: to train semi-crf, a stochastic gradient descent (sgd) training for l1-regularized with cumulative penalty #refr was"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] it compares favorably with conventional phrase-based translation #refr on chinese-english news translation #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of meaning that are not", "cit": "[CLS] we evaluate our approach on word similarity #otherefr and lexical substitution #refr and show improvements over competitive baselines. [SEP]"}
{"pre": "[CLS] the first is the automatic evaluation of subjectivity analysis in text #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] later studies have used more complex linguistic and structural features, such as formality #otherefr, and thread structure #refr. [SEP]"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the penn treebank #otherefr. [SEP] s", "cit": "[CLS] in this paper, we compare cfg filtering techniques for ltag #refr and hpsg #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of semeval 2012 sts task #refr was the first to study the task of semeval 2012 task", "cit": "[CLS] we will also conduct an analysis of the complementary and redundant nature of lexical and syntactic features, as we"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] of a text in", "cit": "[CLS] cb was annotated using model3 from #refr with a linear model of discourse st ructure . [SEP] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] model weights are optimized to maximize bleu #otherefr with batch mira #refr on 1000-best lists. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr proposed a method for learning a word clustering based on a probabilistic model that learns vector space representations", "cit": "[CLS] recently, extracting questions, contexts and answers from post discussions of online forums incurs increasing academic attention #otherefr; #refr. [SEP]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] others have introduced alternative discriminative training methods #otherefr; #refr, in which a recurring challenge is scalability: to train many"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with the one of", "cit": "[CLS] in contrast, the dynamic oracle of goldberg et al. #otherefr for the projective case achieves a time complexity ofo(n"}
{"pre": "[CLS] the task of identifying the polarity of a document has received a significant amount of attention in the subject", "cit": "[CLS] many algorithms have been developed to automatically acquire semantic class members using a variety of techniques, including co-occurrence statistics"}
{"pre": "[CLS] in the context of machine translation, itg has been explored for applications in word sense disambiguation #otherefr, and machine", "cit": "[CLS] that is why previous work explored methods to assess annotation quality and to aggregate multiple noisy annotations for high"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a text with a text that are semantically", "cit": "[CLS] some of these distinctions have become blurred with time: collins and singer #otherefr, ng and #refr and chan et"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to learn a translation probabilities from", "cit": "[CLS] we used the stanford chinese segmenter #otherefr and stanford pos- tagger #refr to obtain the segmentation and pos-tagging of"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for each sentence in the sentence with", "cit": "[CLS] as to the tasks involved in our scenario, several papers address al for ner #otherefr; #refr and syntactic parsing"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] the", "cit": "[CLS] there are two main aims of this work: (1) to create a rule-based temporal expression annotator that includes knowledge"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to learn a translation models from a", "cit": "[CLS] the moses smt toolkit #refr, which we used in our experiments, is suitable for implementing factored translation models. [SEP]"}
{"pre": "[CLS] the idea of using a set of features is presented in #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] this type of approach has led to two highly valued semantic resources: the princeton wordnet #otherefr and the berkeley"}
{"pre": "[CLS] we use the stanford ner tagger #refr to train the training data and test data for pos tags for", "cit": "[CLS] recent topic models consider word sequence information in documents #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] algorithm with the average perceptron algorithm #refr with the", "cit": "[CLS] this does not scale well to large treebanks, forcing the use of implicit representations #otherefr or heuristic subsets #refr."}
{"pre": "[CLS] the first is the task of identifying the event and event extraction of event descriptions of event descriptions of", "cit": "[CLS] 4.1.6 discourse relations rhetorical relations such as causation, elaboration and enablement could aid in tracking the temporal progression of"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] the transformation t combines the re-estimated model c i and the smoothed tree- 4we use a frequency-based notation because"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the training data is to", "cit": "[CLS] labeled syntactic dependencies (lsd) #refr - the f1 for comparing the sets of directed bilexical syntactic dependencies extracted from"}
{"pre": "[CLS] the first is the task of converting natural language processing #otherefr; #refr. [SEP] [SEP] [SEP] to be a number", "cit": "[CLS] probabilistic parsers trained over labeled data have high accuracy on indomain data: lexicalized parsers get an f-score of up"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to learn a phrase pairs of", "cit": "[CLS] encouraged by the improvements that can be achieved with part-of-speech (pos) reordering rules #refr, we apply such rules on"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the sentence in", "cit": "[CLS] kate and mooney #otherefr and #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second approach is to use a word sense disambiguation algorithm which is based on the most similar to", "cit": "[CLS] one of the major approaches to disambiguate word senses is supervised learning #otherefr, (ng and #refr, #otherefr. [SEP] [PAD]"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] the", "cit": "[CLS] their starting point is a corpus of 10.000 words comprising a variety of genres (newswire text, emails, instructions, etc.)"}
{"pre": "[CLS] the similarity between two sentences is computed using the wordnet #otherefr and the wordnet similarity measure proposed by #refr.", "cit": "[CLS] sari?c et al., 2012; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the system used in the experiments reported in this paper is based on the senseval-3 competition #refr, which was", "cit": "[CLS] using bioscope for training and evaluation, morante and daelemans #otherefr developed a scope detector following a supervised sequence labeling"}
{"pre": "[CLS] we use the berkeley parser #refr to obtain the syntactic parse trees for each sentence and the english and", "cit": "[CLS] significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the elements is to use", "cit": "[CLS] iii and marcu, 2006; #refr, but their task is multi-document summarization while ours is to induce summary templates. [SEP]"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be a given by using", "cit": "[CLS] #refr replaced phrases with paraphrases in a number of sentences and asked judges whether the substitutions ?preserved meaning and"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the word alignments for each sentence and then extract the word", "cit": "[CLS] we restrict the target side to the so called wellformed dependency structures, in order to cover a large set"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of meaning that are not", "cit": "[CLS] table 3: results as f1 values of top performing systems for the semeval07 task07 (upv = #refr, nus-pt ="}
{"pre": "[CLS] we use the stanford parser #refr to obtain the sentences and extract dependency trees for each sentence with dependency", "cit": "[CLS] they include entity approaches for local coherence which track the repetition and syntactic realization of entities in adjacent sentences"}
{"pre": "[CLS] id participant cmu carnegie mellon university #otherefr cu-bojar charles university - bojar #refr cu-depfix charles university - depfix #otherefr", "cit": "[CLS] id participant cmu carnegie mellon university #otherefr its-latl language technology laboratory @ university of geneva #refr jhu johns hopkins"}
{"pre": "[CLS] in the literature, supervised learning methods have been applied to part-of-speech tagging and unsupervised learning of part-of-speech tagging #otherefr;", "cit": "[CLS] pos tagging is generally regarded as a classification task, and zero-one loss is commonly used in learning classifiers #refr."}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] the only successful applications of word sense information to parsing that we are aware of are xiong et al"}
{"pre": "[CLS] the first is the automatic evaluation of the automatic metric of the automatic evaluation of the automatic evaluation of", "cit": "[CLS] currently, rftagger #refr and the norma tool for automatic normalization #otherefr are supported, but in principle any other annotation"}
{"pre": "[CLS] the task of identifying paraphrases of identifying the semantic textual similarity of a text is a document #otherefr; #refr.", "cit": "[CLS] proposed approaches to modelling nc semantics have used semantic similarity #otherefr and paraphrasing #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the second international chinese word segmentation bakeoff #otherefr; #refr and the same task as the best results are", "cit": "[CLS] state-of-the-art unsupervised dependency parsers, except #refr, also rely on manually annotated text or text processed by supervised pos taggers."}
{"pre": "[CLS] the first is the sentiment lexicon provided by #refr, which is a large corpus of tweets from twitter messages", "cit": "[CLS] the imdb dataset #refr contains movie reviews with their associated binary sentiment polarity labels. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the semantic role labeling task of translating natural language processing #otherefr; #refr. [SEP]", "cit": "[CLS] the learning algorithm requires sample corpora to be constituted by verb-noun, noun-verb, or verb-prepnoun dependencies, where the nouns are"}
{"pre": "[CLS] #refr proposed a method for learning a word segmentation and a word segmentation model that uses a word segmentation", "cit": "[CLS] internet provides large amounts of raw text, and statistics collected from it have been used to improve parsing performance"}
{"pre": "[CLS] the learning algorithm used in this paper is a variation of the winnow update rule based statistical parsing algorithm", "cit": "[CLS] most constraints that prove useful for srl #refr also require customization when applied to a new language, and some"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] one particular variety of this approach, proposed by #refr, uses a large set of linguistic features to automatically learn"}
{"pre": "[CLS] the first is the task of coreference resolution #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] ? the typed-similarity [SEP] ?", "cit": "[CLS] specifically, in these approaches, one has to employ knowledge of the target language to design coreference rules #otherefr, #refr)"}
{"pre": "[CLS] the semantic relation between nominals in the verb and the verb frame acquisition system has been used in the", "cit": "[CLS] the berkeley framenet project #refr was developed as a machine readable database of distinct frames and lexical units (words"}
{"pre": "[CLS] the translation system is evaluated using bleu #refr, which is calculated by the quality of the case-insensitive bleu-4 metric", "cit": "[CLS] we introduce the terp #refr alignment metric for the english-targeted combination. . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the automatic acquisition of subcategorization frames from corpora #refr, which is the annotated", "cit": "[CLS] #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to compute sentence importance. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] iii, 2006; zajic et al, 2007; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a text with a text that are semantically", "cit": "[CLS] for every entity in our training corpus we extract event chains similar to those proposed by #refr. [SEP] [PAD]"}
{"pre": "[CLS] the most popular approach to phrase-based translation models #otherefr; #refr rely on the source side and the target side", "cit": "[CLS] second, #otherefr and #refr modify the distortion model of the hmm alignment model #otherefr modify an itg aligner by"}
{"pre": "[CLS] the first is the case for the word sense disambiguation task of the disambiguation task of word sense disambiguation", "cit": "[CLS] it gave f score of 81.45% for sentences with less than 40 words and 78.3% for all sentences, significantly"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] in particular, phrase-based smt models such as #refr and chiang #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the automatic evaluation of the automatic metric of the automatic evaluation of the automatic evaluation of", "cit": "[CLS] there have been a number of approaches proposed for the use of multilingual resources for mwe identification #otherefr; #refr."}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] using morphological features such as case has also improved parsing for russian, turkish and hindi #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] in the second international chinese word segmentation bakeoff #otherefr; #refr and the best results are achieved by the best", "cit": "[CLS] similar work is done by li and #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking approaches #otherefr; #refr to learning for dependency", "cit": "[CLS] note 1iwatate et al #otherefr, and one in #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the automatic metric of text with human annotation of text #refr. [SEP] quality estimation (qe) and", "cit": "[CLS] for instance, using linear combinations #otherefr; #refra; albrecht and hwa, 2007b) or a variety of supervised machine learning algorithms#otherefr."}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] shared", "cit": "[CLS] the faust system explores several stacking models for combination using as base models the umass dual decomposition #refr and"}
{"pre": "[CLS] the first is the automatic evaluation of discourse structure of discourse relations #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] a condition of attending the muc workshops was participation in a required evaluation #otherefr #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the sentence in", "cit": "[CLS] to address this problem with unstructured representations we propose to represent the structure of an image using the visual"}
{"pre": "[CLS] the first is the process of annotating the discourse structure in text with discourse structure in text with discourse", "cit": "[CLS] along similar lines, past work has also focused on interpreting natural language instructions #refr, which takes into account the"}
{"pre": "[CLS] the system used for the experiments in this paper is based on the senseval-3 english lexical sample task #refr.", "cit": "[CLS] the *sem 2013 shared task on semantic textual similarity #refr required participants to implement a software system that is"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] algorithm with the best derivation = exp ( m?", "cit": "[CLS] the same grammar assigns an average of 22 lexical categories per word #refra), resulting in an enormous space of"}
{"pre": "[CLS] the grammar we are extracted from the treebank using the stanford parser #refr. [SEP] parser #refr with the self-trained", "cit": "[CLS] for the partial grammar acquisition, in our previous works, we have proposed a mechanism to acquire a partial grammar"}
{"pre": "[CLS] in the second international chinese word segmentation bakeoff #otherefr; #refr and the state-of-the-art accuracies of the state-of-the-art accuracies of", "cit": "[CLS] those methods are very simple and guaranteed to converge, but as minka #otherefr and #refr showed for classification, their"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] approaches that do not use a parser exist as well and typically induce a hierarchical representation that also allows"}
{"pre": "[CLS] the first is the annotation tool which was the annotation tool for the annotation tool #refr. [SEP] project at", "cit": "[CLS] the scheme has served as the basis for a number of annotation projects, such as the development of the"}
{"pre": "[CLS] the first is the automatic evaluation of discourse coherence models in discourse structure of the discourse structure of the", "cit": "[CLS] as a unit is reached and selected it is mapped to the first linguistic level of representation, text structure"}
{"pre": "[CLS] the first is the task of identifying the event extraction of event descriptions of event descriptions of event extraction", "cit": "[CLS] verbal and nominal semantic role labeling #otherefr as well as jointly #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to learn a translation probabilities from", "cit": "[CLS] in order to make bilingual context available, we use a bilingual language model #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best derivation of the best", "cit": "[CLS] #refr, although not focusing on disfluency detection, show that using written language data for modelling spoken language can improve"}
{"pre": "[CLS] #refr use a similar approach to identify opinion polarity lexicons for opinion holders and opinion holders and opinion holders", "cit": "[CLS] other work in this area includes #otherefr; #refr which uses text mining in the context product reviews, but none"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? ? ? ?", "cit": "[CLS] current approaches employ various machine learning techniques for this task, such as inductive logic programming in earlier systems #otherefr;"}
{"pre": "[CLS] the work of #refr is similar to ours in that it is possible to use a more complex model", "cit": "[CLS] #refrb) presented an algorithm for generating, from a very general class of dominance graphs, an rtg that describes exactly"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the entire sequence of the entire", "cit": "[CLS] each clustering is learned for 30 iterations of em over english gigaword #otherefr, parsed with the berkeley parser #refr"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the training data is to", "cit": "[CLS] iii #otherefr and #refr showed techniques for training models that attempt to separate domainspecific and general properties. [SEP] [PAD]"}
{"pre": "[CLS] the system is based on the inprotk toolkit for the best performing system described in #refr. [SEP] system, which", "cit": "[CLS] however, it has been criticized for being ?too committed to a particular approach to linguistic analysis and representation? ("}
{"pre": "[CLS] #refr proposed a method for learning a bootstrapping method for bootstrapping a sequence of topic modeling the sequence of", "cit": "[CLS] in concurrent work, #refr propose integer linear program formulations for inferring types of emerging entities from the way their"}
{"pre": "[CLS] the most popular approach to paraphrase generation has been the extraction #otherefr; #refr. [SEP] input text is based on", "cit": "[CLS] this is similar to the efforts of #refr, who made use of multiple resources to derive feature functions and"}
{"pre": "[CLS] the most popular approach to paraphrase generation has been the extraction #otherefr; #refr. [SEP] input text is based on", "cit": "[CLS] to create a paraphrase grammar from a translation grammar, we extend the syntactically informed pivot approach of #refr to"}
{"pre": "[CLS] we use the stanford word segmenter #refr to extract the word alignments for each word and then extract the", "cit": "[CLS] #refr propose a method that utilizes a pre-existing phrase table and a small bilingual lexicon, and performs bli using"}
{"pre": "[CLS] we use the berkeley parser #refr to obtain the syntactic parse trees for each sentence and the english and", "cit": "[CLS] we create a forest of possible synchronous derivations (cf. #refr). [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] the hierarchical phrase-based (hpb) translation model #refr has been widely adopted in statistical machine translation (smt) tasks. [SEP] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the elements is to use", "cit": "[CLS] first, two estimates of importance on words have been used very successfully both in generic and query-focused summarization: frequency"}
{"pre": "[CLS] the first is the polarity of a document #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ,", "cit": "[CLS] within the last decade, several absa systems of this kind have been developed for movie reviews #otherefr, services #refr,"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] this may be partially compensated for by including features about the surrounding words #refr, but any feature templates which"}
{"pre": "[CLS] we use the stanford named entity recognizer #refr to extract named entities from the text to identify named entities", "cit": "[CLS] while numerous extractive summarization techniques have been proposed for multidocument summarization #otherefr; #refr, few techniques have been specifically designed"}
{"pre": "[CLS] the system used in this paper is based on the c&c parser #otherefr shared task #refr. [SEP] [SEP] [SEP]", "cit": "[CLS] while early shallow approaches have been proven useful for several natural language processing applications #otherefr; #refr, the field is"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] this problem has been widely acknowledged, and the conventional solution, which has been shown to work well for many"}
{"pre": "[CLS] the generation of referring expressions is a subject of research in natural language generation (nlg) in natural language generation", "cit": "[CLS] previous work on reference in sentence generation, e.g., \\[appelt1985, dale1992, dale and reiter1995, heeman and hirst1995, horacek1997, #refr\\], has"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] ace2004-nwire: ace 2004 newswire set to compare against #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the berkeley parser #refr to obtain the syntactic parse trees for each sentence and the sentence and", "cit": "[CLS] recent work shows that probabilistic data-driven parsing with lcfrs is indeed feasible and gives acceptable results #refr. [SEP] [PAD]"}
{"pre": "[CLS] we use the stanford ner tagger #refr to train the classifier to extract dependency trees for each word and", "cit": "[CLS] methods for blending discriminative and generative models #otherefr; #refr, would enable incorporation of completely unlabeled data as well. [SEP]"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] score #refr with the entire candidate set. [SEP]", "cit": "[CLS] #refr have an ilp model to jointly determine anaphoricity and coreference, but take neither transitivity nor exclusivity into account."}
{"pre": "[CLS] the most common approach to word alignment is to use a log-linear model #otherefr; #refr. [SEP] model to handle", "cit": "[CLS] this model still relies on the generative story and achieves only a limited freedom in choosing the features. #refr"}
{"pre": "[CLS] #refr proposed a method for extracting parallel corpora from comparable corpora. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] clarke et al #otherefr and #refr used this corpus for their question answering system. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained on the penn treebank corpus and tested on the penn treebank wall street journal, parser", "cit": "[CLS] however, internal heuristics of the charniak search, such as attention shifting #otherefr; #refr, can make this accuracy/efficiency tradeoff somewhat"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] however, given the dmv?s purely syntactic pos tag-based approach #otherefr, and recently unsupervised pos tagging #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the task of event extraction has been the focus of event extraction task #otherefr; #refr. [SEP] event extraction systems", "cit": "[CLS] leuven #otherefr, tees-2.1 (bjo?#refr, irisa-texmex #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of relation extraction has received much attention in recent years #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] here one aligns existing database records with the sentences in which these records have been ?rendered???effectively labeling the text?and"}
{"pre": "[CLS] the task of identifying the semantic relations between nominals of noun phrases and their arguments are also key to", "cit": "[CLS] the data used for empirical evaluation was taken from #refr and consists of 1436 sentences, which is split into"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] other models #otherefr, (al-onaizan and #refr generalize this to include lexical dependencies on the source. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the grammar we are extracted from the parallel corpus using the berkeley parser #refr and the berkeley parser #otherefr.", "cit": "[CLS] we are introducing some semantic taxonomy and the semantic distance measurement algorithm#refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the generation of referring expressions is a useful source of linguistic phenomena in a range of natural language processing", "cit": "[CLS] joshi #otherefr; schabes, abeille &5 #refr to natural language generation. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the semantic textual similarity of the words in the context of a sentence.", "cit": "[CLS] 17-23 source texts #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a text with a text that are semantically", "cit": "[CLS] cluto demonstrated robust performance in several related nlp tasks #otherefr; #refr. . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of coreference resolution #otherefr; ng and #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ?", "cit": "[CLS] for coreference res- 1http://conll.bbn.com 2http://www.bbn.com/ontonotes/ features describing ci or cj words the first and last words of the given"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a text with a text that are semantically", "cit": "[CLS] the simplest strategy for ordering adjectives is what #refr call the direct evidence method. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second approach is to use a word lattice as a sequence of a sequence of a sequence of", "cit": "[CLS] all the meetings have been transcribed and annotated with dialog acts #otherefr, topics, and extractive summaries #refr. [SEP] [PAD]"}
{"pre": "[CLS] in the context of machine translation, itg has been explored for a variety of nlp tasks such as pos", "cit": "[CLS] recent work addresses this problem by scoring a particular dimension of essay quality such as coherence #otherefr, technical errors,"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the sentences and the syntactic parse trees of the sentences in", "cit": "[CLS] the next wave of work in computational discourse processing sought greater generality through stronger theoretical grounding, appealing to thencurrent"}
{"pre": "[CLS] the grammar is a grammar that is a parsing system for data-driven parsing and a grammar that is a", "cit": "[CLS] second, eisner-satta o(n3) pbdg parsing algorithms are extremely fast #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of identifying the event descriptions of event extraction and event descriptions has been well studied in the", "cit": "[CLS] we also exclude comparative anaphora #refr 2examples are from ontonotes #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to this problem is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] turboparser attacks the parsing problem using a compact integer linear programming #otherefr, then employing alternating directions dual decomposition (ad"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the sentences and the syntactic parse trees of the sentences and", "cit": "[CLS] we experimentally evaluated the test collection for single document summarization contained in the rst discourse treebank (rst-dtb) #refr distributed"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] for instance, the current framenet corpus #refr consists of 130,000 manually annotated sentences. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second approach is to use a small number of approximate inference rules, and unsupervised methods #otherefr; #refr. [SEP]", "cit": "[CLS] specifically, clitic separation has been shown to improve performance on arabic parsing #refr and arabic-english machine translation #otherefr. [SEP]"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a task of identifying the degree of semantic relations between words", "cit": "[CLS] these relations are extracted from unstructured or semi-structured text using ontology learning from scratch #otherefr; #refr which mainly stem"}
{"pre": "[CLS] the system used in this paper is based on the c&c parser #otherefr and the stanford parser #refr. [SEP]", "cit": "[CLS] next, sentences are analyzed by a state-of-the-art syntactic parser #refr the output of which provides useful information for the"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with a probability distribution", "cit": "[CLS] one such technique is markov chain monte carlo, and in particular gibbs sampling #refr, another is #otherefr. [SEP] [PAD]"}
{"pre": "[CLS] the second approach is to use a maximum entropy model #otherefr; #refr. [SEP] model for the probability distribution over", "cit": "[CLS] with the standard split of ctb 5.0 data into training, development and test sets #refr, the result are summarized"}
{"pre": "[CLS] the parser is trained on the wall street journal (wsj) portion of the penn treebank #otherefr; #refr and the", "cit": "[CLS] statistical parsers are major components in nlp applications such as qa #otherefr and srl #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second approach is to use a small number of seed examples for example outputs #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] it has been applied to various nlp/ie tasks, including named entity recognition #otherefr and parse selection #refr with rather"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with a probability distribution", "cit": "[CLS] on both efficiency and statistical grounds, much recent tsg work has focused on fragment selection #otherefr; #refr. [SEP] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders and predict opinion holders in opinion holders and opinion holders", "cit": "[CLS] #refr presented an unsupervised method to recognize discourse relations held between arbitrary spans of text. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the discourse structure of discourse structure of discourse structure of discourse relations in the discourse structure", "cit": "[CLS] the method combines a constraint-based approach with an approach based on preferences: we exploit the hpsg type hierarchy and"}
{"pre": "[CLS] #refr proposed a method for learning a word clustering based on a probabilistic model that learns vector space representations", "cit": "[CLS] many computational models of compositionality focus on learning vector spaces #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] the hand-aligned parallel corpora in our experiments come from the copenhagen dependency treebank #refr, for five different language pairs,"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] crego and #refr modified the phrasebased lexical reordering model of tillman #otherefr for an n-gram-based system. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the semantic role labeling task of the predicate-argument structure of the first time", "cit": "[CLS] it is close to the formal definition offered in mel?c?uk #otherefr, and is applied computationally in #refr however, using"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] various methods are used to optimize log-linear models in re-ranking #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the grammar is a grammar that is a synchronous grammar (stsg) #refr. [SEP] grammars are capable of synchronous context-free", "cit": "[CLS] to overcome this problem and certain dependency problems, shieber and schabes #otherefr suggest a stronger model called synchronous tree-adjoining"}
{"pre": "[CLS] the first is the task of relation extraction #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ?", "cit": "[CLS] in contrast with our discriminative model, a statistical parsing based generative model #refr has been proposed for a related"}
{"pre": "[CLS] in the context of machine learning techniques have been applied to many natural language processing tasks with promising results,", "cit": "[CLS] table 1: sample members of four clusters from the wall street journal corpus. of researchers have therefore sought to"}
{"pre": "[CLS] we use the stanford corenlp suite to lemmatize and partof-speech tag each word #otherefr; #refr, and parse the input", "cit": "[CLS] bllip: 5k articles of newswire parsed with the #refr parser. . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to identify the topics of discourse relations. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] we use simfinder #refr for sentence clustering and its similarity metric to evaluate cluster quality; simfinder outputs similarity values"}
{"pre": "[CLS] the task of identifying the event descriptions of event extraction and event extraction of event descriptions has been well", "cit": "[CLS] we used j.depp,7 an efficient shift-reduce parser with feature sequence trie #otherefr; #refr, for parsing. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the longrange reordering model is to use a word", "cit": "[CLS] multilingual learning has been successful for other linguistic induction tasks such as lexicon acquisition, morphological segmentation, and part-of-speech tagging"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] event and semantic orientation", "cit": "[CLS] this paper describes an extended annotation scheme for marking the attribution of discourse relations and their arguments annotated in"}
{"pre": "[CLS] in the literature, slight variations of this problem are also referred to as unsupervised learning #otherefr, and semi-supervised learning", "cit": "[CLS] chodorow and leacock #otherefr and #refr argue that precision-oriented is better, but they do not give any concrete reason."}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to learn a translation models from a", "cit": "[CLS] possibly the most remarkable evolution of recent years in statistical machine translation is the step from word-based models to"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with a probability distribution", "cit": "[CLS] latent variable models have recently been of increasing interest in natural language processing, and in parsing in particular #otherefr;"}
{"pre": "[CLS] the most common approach to coreference resolution is to use a seed lexicon to generate a set of seed", "cit": "[CLS] we evaluate our proposed model with the most commonly used metrics for coreference resolution: for the ontonotes data sets"}
{"pre": "[CLS] the semantic parser used in this paper is the penn treebank #otherefr, which is based on the c&c parser", "cit": "[CLS] as noted by #refr, these trees are simpler than tag elementary trees, which can favourably impact performance. [SEP] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] approaches that do not use a parser exist as well and typically induce a hierarchical representation that also allows"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best results for the best", "cit": "[CLS] precision and recall rates were 92.4% on the same data used in #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the system used in this paper is based on the c&c parser #otherefr and the c&c parser #refr. [SEP]", "cit": "[CLS] subsequent research has yielded systems for english #otherefrb; #refr), besides systems for a number of other languages #otherefr). [SEP]"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best results for the best", "cit": "[CLS] however, there does exist ample work on extending crf training to the semi-supervised setting (for example, see #refr and"}
{"pre": "[CLS] the translation quality is measured by bleu #refr. [SEP] score #refr on the test set. [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] two approaches have emerged to alleviate the problem of da-english parallel data scarcity: using msa as a bridge language"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for each sentence in the training corpus.", "cit": "[CLS] there is only a few existing work in spontaneous speech domains. #otherefr modeled it as a sequence labeling problem"}
{"pre": "[CLS] the most common approach to semantic role labeling is to use syntactic information #otherefr; #refr. [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] as another reference point, out of 500,000 relations extracted by the reverb open ie system #refr, only about 10,000"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] sentence pair of a", "cit": "[CLS] the visualization in this case focuses on the sentiment quantity of either 1 (very bad) or 4 (very good)"}
{"pre": "[CLS] the most common approach to automatic acquisition is to use syntactic information #otherefr; #refr. [SEP] quality estimation or semantic", "cit": "[CLS] #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a text that are semantically", "cit": "[CLS] mwe research has focussed largely on their implications in language understanding, fluency and robustness #otherefr; #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] in addition to their use in machine translation #otherefr; #refr, translation models can be applied to machineassisted translation #otherefr."}
{"pre": "[CLS] the parser is trained on the penn treebank corpus and tested on the penn treebank wall street journal, parser", "cit": "[CLS] #refr raise a similar question of evaluating parsers trained on different annotation standards, including for coordination, but take a"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with the one of", "cit": "[CLS] approximate parsers have therefore been introduced, based on belief propagation #otherefr, or multi-commodity flows #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to translation models #otherefr; #refr rely on parallel data #otherefr. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] our paper fits into the recent line of work for jointly inducing the phrase table and word alignment #refr."}
{"pre": "[CLS] we use the averaged perceptron algorithm #refr to train the parameters of the feature weights for the log-linear models.", "cit": "[CLS] the tree-to-string models of #otherefr naturally consider syntax, but special modeling considerations are needed to allow any deviations from"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] these methods use categorized semantic and pragznatic constraints such as verbal semantic attributes #refr and types of modal expressions"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] the", "cit": "[CLS] 2this task is popularized by various recently held shared tasks #refr. negation scopes, it is not clear how to"}
{"pre": "[CLS] the parser is trained on the penn treebank corpus and tested on the penn treebank wall street journal, parser", "cit": "[CLS] previous work on parser performance and domain variation by #refr showed that by training a parser on the penn-ii"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with the one of", "cit": "[CLS] throughout this paper we will use split bilexical grammars, or sbgs #otherefr, a notationally simpler variant of split head-automaton"}
{"pre": "[CLS] the system is based on the inprotk project #refr. [SEP] system, which was developed at the level of the", "cit": "[CLS] this includes generating back-channels, dynamic content reordering #refr, and surface generation that models coherent discourse phenomena, such as pronominalisation"}
{"pre": "[CLS] #refr proposed a method for learning a latent variable model to learn latent variable model for each document with", "cit": "[CLS] text summarization is often addressed as a task of simultaneously performing sentence extraction and sentence compression #otherefr; #refr. [SEP]"}
{"pre": "[CLS] the second approach is to use a word segmentation and pos tagger to tag dictionary to tag each word", "cit": "[CLS] sighan, the special interest group for chinese language processing of the association for computational linguistics, successfully conducted four prior"}
{"pre": "[CLS] the first step is to use a stochastic gradient descent (sgd) and then applied to the output of a", "cit": "[CLS] in this work we use the abstraction of the grammatical structure provided by ltag, but the same or a"}
{"pre": "[CLS] the grammar is a grammar that is a given a grammar that is a grammar that is a grammar", "cit": "[CLS] when all constituents are typed, it becomes possible to use arrays or hash-tables to store fds in lisp, which"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] semantic role labeling based on (large) annotated corpora need to deal with a number of issues, such as the"}
{"pre": "[CLS] the model is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the em algorithm #refr with the", "cit": "[CLS] the hypertagger generalizes #refr method of using supertags in generation by using maximum entropy models with a larger local"}
{"pre": "[CLS] we use the berkeley parser #refr to obtain the syntactic parse trees for parse trees for parse trees for", "cit": "[CLS] meanwhile, translation grammars have grown in complexity from simple inversion transduction grammars #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the penn treebank #otherefr. [SEP] s", "cit": "[CLS] its applications range from sentence boundary disambiguation (reynar and #refr to part-of-speech tagging #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the english side of the parallel corpus was parsed using the berkeley parser #refr, and the berkeley parser #otherefr", "cit": "[CLS] previous work focusing on machine translation includes statistical methods for estimating correctness scores or correctness probabilities, following a rich"}
{"pre": "[CLS] we use the stanford ner tagger #refr to train the training data with the training data and test data", "cit": "[CLS] reinforcement learning can be used to learn to read instructions and perform actions in an external world #refr. [SEP]"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a similar to the similarity of two sentences that is the", "cit": "[CLS] this idea has been firstly used in #otherefr and it has been explored for extracting semantic relations between nouns"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] so far, no system using tightly integrated syntactic and semantic processing has been competitive with the best systems, which"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal (wsj) section of the penn treebank #otherefr; #refr.", "cit": "[CLS] corresponding manipulations in the form of tree transformations for dependency-based parsers have recently gained more interest #otherefr; #refr but"}
{"pre": "[CLS] the most common approach to machine translation is to use a log-linear combination of statistical machine translation #otherefr; #refr.", "cit": "[CLS] factored translation has been successfully applied to many language pairs and with diverse types of information encoded in the"}
{"pre": "[CLS] the grammar is a grammar that is a given a grammar that is a grammar that is a grammar", "cit": "[CLS] in the past two decades, there has been tremendous progress in natural language processing and various undergraduate/graduate programs in"}
{"pre": "[CLS] the first is the case for the generation of referring expressions that we will use the same type of", "cit": "[CLS] unlike the current mainstream in automatic linguistic knowledge acquisition, which can be characterized as quantitative, surface-oriented bulk processing of"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking approaches #otherefr; #refr to learning for dependency", "cit": "[CLS] we test our model on 9 conll 2006 and 2007 shared task data sets #otherefr; #refr and wsj part"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a text that are semantically", "cit": "[CLS] in the future, it will also allow us to use corpora tagged with comlex subcategorization frames, e.g., #refr. [SEP]"}
{"pre": "[CLS] the system is based on the standard approach of #refr and the standard log-linear translation model. [SEP] [SEP] [SEP]", "cit": "[CLS] see #otherefr, #refr and #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second approach is to use a maximum entropy model #otherefr, which is based on the conditional probability of", "cit": "[CLS] more recently, two parallel works #otherefr; #refr combined, with a remarkable success, the output of a set of four"}
{"pre": "[CLS] the task of identifying the semantic relations between nominals of noun phrases and their semantic relations has been used", "cit": "[CLS] temporal relations such as happens-before #otherefrb; #refr are important for enhancing deep semantic processing. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the language grounding problem has assumed several guises in the literature such as speech recognition and machine translation #otherefr,", "cit": "[CLS] in a number of works, #refr describes a finite-state morphological analyzer of modern standard arabic which handles both inflectional"}
{"pre": "[CLS] the first is the task of converting natural language processing #otherefr; #refr. [SEP] [SEP] to be able to help", "cit": "[CLS] probably mostly related to the work reported here are #otherefr and #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for learning and beam-search decoding #otherefr;", "cit": "[CLS] #refr proposed a boosting-based method that repeats the learning of rules represented by feature conjunctions. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to learn a translation models from a", "cit": "[CLS] also the use of lossy data structures based on bloom filters has been demonstrated to be effective for lms"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] while it has been shown that paraphrasing methods are useful for question answering #otherefr and relation extraction #refr, this"}
{"pre": "[CLS] the task of identifying the semantic relations between nominals of noun phrases and their arguments are also key to", "cit": "[CLS] #refra) used pattern clusters to disambiguate nominal compound relations. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the identification of the dialogue manager sends dialogue acts from the situated interactive", "cit": "[CLS] figure 1: screenshot of our story encoding interface. recent work has centered on markup languages for complex temporal information"}
{"pre": "[CLS] the most common approach to selectional preference acquisition is to use a similarity measure which is to automatically identify", "cit": "[CLS] other attempts towards automatic generation of image captions generate captions based on the immediate textual context of the image"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the longrange reordering model is to use of the", "cit": "[CLS] recent years have witnessed increased interest on a simpler instantiation of the compression problem, namely word deletion #otherefr; #refr."}
{"pre": "[CLS] the first is the case for the automatic evaluation of the automatic evaluation of translation quality measured by bleu", "cit": "[CLS] #refr provides a good summary of early work in weight-based analysis, as well as a weight-oriented approach to attachment"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] the behavior of mstparser and maltparser in this respect is consistent with the results of #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal (wsj) section of the penn treebank #otherefr; #refr.", "cit": "[CLS] caught between the scylla of linguistically inadequate projective trees and the charybdis of computationally intractable non-projective trees, some researchers"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree"}
{"pre": "[CLS] the most common approach to joint unsupervised learning of dependency parsers is to use syntactic structures in nlp tasks", "cit": "[CLS] it is often straightforward to obtain large amounts of unlabeled data, making semi-supervised approaches appealing; previous work on semisupervised"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] model with respect to the entire candidate set.", "cit": "[CLS] #refr proposed a domain-dependent hmm model to capture topic shift in a text, where topics are represented by hidden"}
{"pre": "[CLS] the first is the case for the generation of referring expressions from the linguistic phenomena of referring expressions and", "cit": "[CLS] 5 .3 boguraev and puste jovsky #refr argue that the normal conceptions of the structure of the lexicon are"}
{"pre": "[CLS] the first is the case for the automatic evaluation of the automatic evaluation of the automatic evaluation of the", "cit": "[CLS] conpars is based on the output of * we thank inist-cnrs for providing us with thesauri and corpora in"}
{"pre": "[CLS] the first is the second part of the penn treebank #otherefr, which is the penn treebank #refr, and the", "cit": "[CLS] instead, we parse them into a default analysis, which can then be expanded and disambiguatcd at later stages of"}
{"pre": "[CLS] the evaluation metric is bleu #refr, meteor #otherefr, which is calculated by the most popular metric scores computed on", "cit": "[CLS] first, it is known that the domain and genre can influence mt performance #refr, so we wanted to control"}
{"pre": "[CLS] the grammar is a grammar that is a grammar that is a given a grammar that is a grammar", "cit": "[CLS] it will be shown that this model is both categorical enough to handle standard generalizations about quantifier scope, such"}
{"pre": "[CLS] the system uses a combination of the maximum entropy model of #refr to generate multiple reference translations for the", "cit": "[CLS] clte has as main applications content synchronization and aggregation in different languages #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] event", "cit": "[CLS] the tempeval #otherefr challenge has led to a number of works on temporal relation extraction #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to semi-supervised learning algorithms use a small set of seed examples #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] the feature combinations play an essential role in obtaining a classifier with state-of-the-art accuracy for several nlp tasks; recent"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for english sentences. [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] as discussed above, all state-of-the-art published methods rely on lexical features for such tasks #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the source sentence", "cit": "[CLS] the simplest of these #otherefr make no use of information from syntactic theories or syntactic annotations, whereas others have"}
{"pre": "[CLS] the first is the task of identifying the polarity of a sentence, a word is a word is analyzed", "cit": "[CLS] #refr and s?aghdha and korhonen #otherefr introduced a probabilistic model to represent word meanings by a latent variable model."}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders and predict the polarity of words in a sentence and", "cit": "[CLS] therefor, several studies proposed using contextual features to determine the subjectivity of a given word within its context #refr."}
{"pre": "[CLS] the first is the task of identifying the semantic relations between nominals of noun phrases and the verb classes", "cit": "[CLS] tiered clustering is a discrete clustering method, as opposed to methods such as #refr that assign a distribution of"}
{"pre": "[CLS] #refr used a similar approach to sentiment classification for sentiment classification of words and polarity in the polarity of", "cit": "[CLS] however, because of the sarcasm in the second tweet (in ?cold? pizza, an undesirable situation followed by a positive"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the sentences and the syntactic parse trees of the sentences in", "cit": "[CLS] this is similar to the hierarchical joint learning approach of #refr, except that our goal is to learn a"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] our data consists of the europarl training, development and test definitions for german-english and finnish-english of the 2005 acl"}
{"pre": "[CLS] the first is the process of annotating the word in a sentence planner is described in #refr. [SEP] sentence", "cit": "[CLS] 154?156) for computing pi? is used that is detailed by #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of meaning that are not", "cit": "[CLS] word sense disambiguation has long been studied as an important problem in natural language processing #otherefr; #refr. [SEP] [PAD]"}
{"pre": "[CLS] #refr proposed a joint model for unsupervised semantic parsing and semantic role labeling using a variety of features including", "cit": "[CLS] in order to leverage a training corpus to recover weight parameters w? for the above features that encourage good"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] a number of recent studies rely on this notion of parallelism and demonstrate that annotations can be adequately projected"}
{"pre": "[CLS] the first is the task of converting natural language processing #otherefr; #refr. [SEP] [SEP] [SEP] to be a number", "cit": "[CLS] among several options, brooke and hirst #otherefr explored using non-lexicalized cfg production rules in a binary feature encoding on"}
{"pre": "[CLS] the first is the task of identifying the polarity of a word is a word in a word in", "cit": "[CLS] germanet has been modelled along the lines of the princeton wordnet for english #otherefr and shares its general design"}
{"pre": "[CLS] the english side of the parallel corpus was parsed using the berkeley parser #refr trained on the penn treebank", "cit": "[CLS] parsing accuracy comparison and error analysis under the conll-x dependency shared task data #refr have been performed by mcdonald"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] for the french?english systems the phrase table is based on a giza++ word alignment, while the systems for german?english"}
{"pre": "[CLS] in this paper, we propose a method for extracting parallel bilingual news articles from monolingual corpora #refr. [SEP] [SEP]", "cit": "[CLS] extractive methods are generally simpler and have dominated the sentence compression literature #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be np-hard #refr. [SEP] s", "cit": "[CLS] recently, inversion transduction grammars #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second approach is to use a word lattice as a word lattice #refr, which is trained on the", "cit": "[CLS] many researchers have used a corpus based approach to pos tagging such as trigram model #otherefr and to spelling"}
{"pre": "[CLS] the feature weights were tuned using minimum error rate training (mert) #refr on the development set using minimum error", "cit": "[CLS] more recently, #refr extended phrase-pairs (or blocks) to hierarchical phrase-pairs where a grammar with a single non-terminal allows the"}
{"pre": "[CLS] we use the stanford corenlp suite to lemmatize and partof-speech tag each word #otherefr; #refr, and parse the input", "cit": "[CLS] that is, we apply the best-first strategy (ng and #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the sentiment analysis task of mining has been the web #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] to handle the data format, in the input and on the outcome of the system, we chose to use"}
{"pre": "[CLS] the most popular approach to word alignment is the hmm model #refr. [SEP] model is based on the hmm", "cit": "[CLS] multilingual pseudo-relevance feedback (multiprf) #refra) is a novel framework for prf to overcome the above limitations of prf. [SEP]"}
{"pre": "[CLS] the second approach is to use a small number of seed examples for example outputs #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] previous approaches to story segmentation have largely focused lexical features, such as word similarily #otherefr; #refr, and adaptive language"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] this kind of information has been most exploited in structural mt systems, employing semantic relations #otherefr; #refr. [SEP] [PAD]"}
{"pre": "[CLS] the most common approach to deriving selectional preference acquisition is to use a lexicon to measure the similarity between", "cit": "[CLS] in particular, text alignment technique is generally used to find sentence level paraphrases #otherefr; #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] #refr, in a more systematic study, find that, of sentences where the tree-to-tree constraint blocks rule extraction, the majority"}
{"pre": "[CLS] the first is the case for the syntactic parsing of the english lexical substitution task #refr. [SEP] grammars in", "cit": "[CLS] however, although resnik #otherefr, #refr, carroll & weir #otherefr 'sparkle: shallow parsing and knowledge extraction for language engineering', and"}
{"pre": "[CLS] we use the berkeley parser #refr to obtain the syntactic parse trees for each sentence and the english and", "cit": "[CLS] mrl 5the beam search decoder in phrase-based system #otherefr; #refr 2-tuple (gspan, ghyps), where ghyps is a list of"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a text that is a", "cit": "[CLS] arguments are replaced with their most important word #otherefr on word sequences (wsk) and pos- tag sequences (possk), (3)"}
{"pre": "[CLS] the parser is trained on the penn treebank corpus and tested on the penn treebank wall street journal, parser", "cit": "[CLS] this table indicates that human annotators strongly prefer ?s? operation to others, and that the manual annotation on the"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? 2008. [SEP] ?", "cit": "[CLS] we use apposition-annotated documents from the english section of ontonotes 4 #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr proposed a method for learning a bootstrapping method for bootstrapping a sequence of topic modeling the sequence of", "cit": "[CLS] #refr, chunked, and labeled with irex 8 named entity types by crfs using minimum classification error rate #otherefr, and"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr. [SEP]", "cit": "[CLS] our work brings together several strands of research including bayesian non-parametric hmms #otherefr, pitman-yor language models #refrb; goldwater et"}
{"pre": "[CLS] the work of #refr is similar to ours in that it is possible to use a more complex model", "cit": "[CLS] we also briefly overview the constraint-based control architecture that enables such an approach and facilitates near linear-time processing with"}
{"pre": "[CLS] the first is the case for the syntactic dependency parsing of the english lexical substitution task #refr. [SEP] parser", "cit": "[CLS] magerman #otherefr, #refr, ratnaparkhi #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the system used in this paper is based on the stanford ner tagger #refr. [SEP] parser #otherefr and the", "cit": "[CLS] iii and #refr grammatical features: pronoun, demonstrative noun phrase, embedded noun, gender agreement, number agreement #otherefr for english, the"}
{"pre": "[CLS] the task of identifying the semantic textual similarity of a document is a text #refr. [SEP] of a text", "cit": "[CLS] these characteristics make hard the identification of entities and the semantics of their relationships #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the literature, supervised approaches have been applied to part-of-speech tagging and unsupervised #otherefr and unsupervised #refr, semi-supervised learning", "cit": "[CLS] #refr introduce a semi-supervised extension of conditional random fields that combines supervised and unsupervised probability models by so-called mdf"}
{"pre": "[CLS] the first is the automatic evaluation of discourse structure of discourse relations #refr. [SEP] discourse markers are the discourse", "cit": "[CLS] later studies have used more complex linguistic and structural features, such as formality #refr, dialog acts #otherefr. [SEP] [PAD]"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking approaches #otherefr; #refr to learning for dependency", "cit": "[CLS] headden, johnson and #refr introduced the extended valence grammar (evg) and added lexicalization and smoothing. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the grammar is a grammar that is a parsing model that of #refr and a probability distribution over a", "cit": "[CLS] secondly, we use techniques developed in #refr, namely the so-called rule filter and the quick-check method. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] in contrast to these approaches, #refr apply six manually defined transformations to german parse trees which yield an improvement"}
{"pre": "[CLS] we use the stanford word segmenter #refr to extract the word pairs from the target side of the parallel", "cit": "[CLS] gimenez and ma#refr explore lexical, syntactic and shallow semantic features and focus on measuring the similarity of mt output"}
{"pre": "[CLS] in the second international chinese word segmentation bakeoff #otherefr; #refr and the best results are achieved by the best", "cit": "[CLS] for example, the english language has at least four schemes based on the penn treebank #otherefr scheme #refr, the"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] we run all questions through the stanford corenlp pipeline (toutanova and #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the maximum entropy model #otherefr, which is based on the maximum entropy model #refr,", "cit": "[CLS] to our knowledge, the first to use stochastic techniques in nlg were #refra) and #otherefrb). [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] model with the average number of the average", "cit": "[CLS] this model is similar to the logarithmic opinion pool (lop) crf suggested by #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second approach is to use a word lattice as a sequence of a sequence of a sequence of", "cit": "[CLS] note that these meetings are research discussions, and that the annotators may not be very familiar with 1we selected"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a text with a text that are semantically", "cit": "[CLS] for example, nlp components such as parsers and co-reference resolution algorithms could be compared in terms of how much"}
{"pre": "[CLS] #refr proposed a method for learning a word segmentation and a word segmentation model that uses a word segmentation", "cit": "[CLS] using these optimizations, we obtain significant improvements compared to other algorithms based on the choi dataset and also on"}
{"pre": "[CLS] the system uses a combination of natural language processing (nlp) systems to produce various types and machine learning techniques", "cit": "[CLS] the techniques used for this task vary from rule-based methods #otherefr, to statistical methods #refr and statistical-rulebased hybrids #otherefr."}
{"pre": "[CLS] the task of identifying positive and negative sentiments and negative sentiments from a passage, has been the sentiment of", "cit": "[CLS] we previously tackled this problem within the context of biological text in the bionlp?09 shared task #refr. [SEP] [PAD]"}
{"pre": "[CLS] the model is trained using the averaged perceptron algorithm #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] = ? ?", "cit": "[CLS] the context of a word is often defined as the words appearing in a window of fixed-length #otherefr; #refr."}
{"pre": "[CLS] the task of identifying the event descriptions of event extraction and event descriptions has been addressed in the extraction", "cit": "[CLS] part-of-speech #otherefra) and lemmatisation is done using morpha #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the unsupervised pos induction task has been extensively studied in the nlp community, and has been studied in the", "cit": "[CLS] others have exploited the automatic transfer of some already existing annotated resource in a different medium or language (such"}
{"pre": "[CLS] the task of identifying positive and negative sentiments and negative sentiments from a text #otherefr; #refr. [SEP] of a", "cit": "[CLS] existing methods can be cast into three main categories, namely rule-based #otherefr, supervised machine learning #refr, and semi-supervised approaches"}
{"pre": "[CLS] we use the stanford ner tagger #refr to train the classifier to extract named entities from the training data.", "cit": "[CLS] some previous work used relations and entity mentions to enhance each other in joint inference frameworks, including re-ranking #otherefr;"}
{"pre": "[CLS] the first is the stanford parser #refr and the stanford parser #otherefr. [SEP] shared task on dependency parsing #otherefr.", "cit": "[CLS] then the parse trees are processed by the stanford parser #refr to obtain syntactic dependencies. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the moses toolkit #refr to train the weights for the feature weights for the log-linear models. [SEP]", "cit": "[CLS] however, with limited scope and too many redundancies, it is inefficient to search separately on each of these similar"}
{"pre": "[CLS] the second approach is to use a small number of seed examples for example outputs #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] figure 1: graphical models. a) 1st order hmm. b) variant used in experiments (one model/preposition, thus no conditioning on"}
{"pre": "[CLS] #refr proposed a method for learning a word clustering based on a word clustering words from a target language", "cit": "[CLS] while there are several graph-based ranking algorithms previously proposed in the literature #otherefr, #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to translation models #otherefr; #refr rely on the source side and the target side #otherefr.", "cit": "[CLS] since their introduction in statistical mt by #refr, log-linear models have been a standard way to combine sub-models in"}
{"pre": "[CLS] the system is based on the inprotk project #refr. [SEP] system, which is a set of possible antecedents of", "cit": "[CLS] this paper applies reintbrcement learning #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] state of the art statistical parsers #otherefr; #refr are trained on manually annotated treebanks that are highly expensive to"}
{"pre": "[CLS] in the context of sentiment analysis and opinion mining has been used to determine if the polarity of a", "cit": "[CLS] task properties determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an"}
{"pre": "[CLS] the most common approach to this problem is to use a similarity measure which is to measure the similarity", "cit": "[CLS] because most relevance scores do not capture this effect, ir systems resort to techniques like query expansion which includes"}
{"pre": "[CLS] in the context of machine translation, itg has been explored for a variety of nlp tasks such as sentiment", "cit": "[CLS] the response generator that is aware of the emotion of an addressee is also useful for text completion in"}
{"pre": "[CLS] the grammar we are extracted from the treebank using the stanford parser #refr. [SEP] parser #refr to generate the", "cit": "[CLS] for this preprocessing of the hpsg grammar, we adapted the 'hpsg to tag compilation' process described in \\[#refr\\]. [SEP]"}
{"pre": "[CLS] the first is the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] this requires the development of generalized features or the definition of semantic similarities between them, e.g. as proposed in"}
{"pre": "[CLS] the first is the automatic evaluation of discourse structure of discourse structure of discourse relations #refr. [SEP] discourse markers", "cit": "[CLS] several gre algorithms have addressed the issue of generating locative expressions [#refr; horacek, 1997; gardent, 2002; krahmer and theune,"}
{"pre": "[CLS] #refr use a generative model to predict the probability of a word in a word in a word in", "cit": "[CLS] there has been considerable interest in automating the labeling process #otherefr; #refr. #otherefr propose a measure of saliency: a"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] unsupervised objectives are, at best, loosely correlated with extrinsic performance #refr; merialdo, 1994; liang and klein, 2008, inter alia)."}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] linguistically motivated syntactic units such as noun phrases #refr, head-modifier pairs #otherefr have also been integrated in information retrieval."}
{"pre": "[CLS] the most common approach to paraphrase generation is to paraphrase generation from corpora #otherefr; #refr. [SEP] words to a", "cit": "[CLS] paraphrases, i.e. differing textual realizations of the same meaning, are a crucial components of text-to-text generation systems, and have"}
{"pre": "[CLS] in the second international chinese word segmentation bakeoff #otherefr; #refr and the same task as the best results are", "cit": "[CLS] we use character-based sequence labeling #refr to find the boundaries of words. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr proposed a method for identifying word pairs of words in a target word in a target word in", "cit": "[CLS] later in #otherefr; #refr, people use a fixed window size containing around 12 neighbor words for wsd. [SEP] [PAD]"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] the paraphraser implements the pivot-based method as described by #refr with several additional filtering mechanisms to increase the precision"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to learn a translation models from a", "cit": "[CLS] to overcome these limitations, many syntaxbased smt models have been proposed #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the sentence planning problem that a text planning problem is central to", "cit": "[CLS] in other papers #refr, we have already given some descriptive statistics on corrections and aware sites and we have"}
{"pre": "[CLS] the first is the case for the generation of referring expressions that we will use the discourse treebank (rst-dt)", "cit": "[CLS] our central model of a discourse grammar is the linguistic discourse model #otherefr, #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the training data is to", "cit": "[CLS] we used datasets distributed for the 2006 and 2007 conll shared tasks #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to parsing is to use a log-linear parsing model and maximise the conditional random field", "cit": "[CLS] these filters could also facilitate expensive learning algorithms, such as semi-supervised approaches #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] id institution balagur yandex school of data analysis #otherefr commercial-1,2,3 anonymized commercial systems online-a,b,g anonymized commercial systems online-a,b,g anonymized", "cit": "[CLS] id institution balagur yandex school of data analysis #otherefr dcu dublin city university #refra) dcu-fda dublin city university #otherefr"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal (wsj) corpus #refr. [SEP] trees and the penn", "cit": "[CLS] 12although mate?s performance was not significantly better than berkeley?s in our setting, it has the potential to tap richer"}
{"pre": "[CLS] the most common approach to semi-supervised learning algorithms use a small set of seed examples #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] the kernelbased classification is, however, known to be very slow in nlp tasks, so efficient classifiers should sum up"}
{"pre": "[CLS] the second approach is to use a word sense disambiguation algorithm which is based on the most similar to", "cit": "[CLS] we estimated these probabilities using the waterloo multitext system with a corpus of about one terabyte of unlabeled text,"}
{"pre": "[CLS] the task of relation extraction has received much attention in recent years #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] in this work, we use the brown clustering algorithm #otherefr, named entity recognition #refr, and relation extraction #otherefr. [SEP]"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] model with respect to the final vote [SEP]", "cit": "[CLS] also relevant here is the work of mitchell and lapata #otherefr and #refr, which provide several alternative procedures for"}
{"pre": "[CLS] the first is the automatic evaluation of the automatic metric of the automatic evaluation of the automatic evaluation of", "cit": "[CLS] two attempts to overcome this drawback are presented in nerbonne #otherefr and #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] instance weighting approaches #otherefr; #refr1285 typically use a rich feature set to decide on weights for the training data,"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] learning to interpret language from a situated context has become a topic of much interest in recent years #otherefr;"}
{"pre": "[CLS] the second approach is to use a maximum entropy model #otherefr; #refr. [SEP] model to predict the probability of", "cit": "[CLS] in our implementation we add the seed rules to each subsequent dl.3 1large-scale information extraction, e.g. #refr, snowball #otherefr"}
{"pre": "[CLS] the model is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the decoder with the decoder with", "cit": "[CLS] for some restricted combinatorial spaces of alignments?those that arise in itg-based phrase models #refr or local distortion models #otherefr?inference"}
{"pre": "[CLS] the most popular approach to paraphrase generation is the source sentences into a target language to be a target", "cit": "[CLS] different application scenarios of paraphrase have different demands on the paraphrasing results and up to now, the widely mentioned"}
{"pre": "[CLS] #refr use a similar approach to induce a set of sentences from a small set of seed words to", "cit": "[CLS] whereas most sentences correspond to a well-formed subtree, #refr report that over 20% of the paragraph boundaries in the"}
{"pre": "[CLS] the model is trained with a maximum entropy model #otherefr, which is trained on the penn treebank wall street", "cit": "[CLS] this has been recently exploited for modeling phrase structure treebanks with discontinuous constituents #otherefr, as well as non-projective dependency"}
{"pre": "[CLS] #refr proposed a method for learning a word clustering based on a probabilistic model that uses a hidden markov", "cit": "[CLS] in fact, many studies that try to exploit wikipedia as a knowledge source have recently emerged #otherefr; #refr. [SEP]"}
{"pre": "[CLS] the task of event extraction was first introduced by #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] similarly, #refr used a log-linear model to represent the probability of a named entity being referred to by a"}
{"pre": "[CLS] the system is based on the inprotk project #refr. [SEP] score for the number of the number of the", "cit": "[CLS] previous work on nominal anaphor resolution has used lexical knowledge in different ways. #refr presented results concerning the resolution"}
{"pre": "[CLS] in the case of dependency parsing, previous work has shown that it is possible to improve the performance of", "cit": "[CLS] to address these problems, there have been several attempts at unsupervised parsing #otherefr, grammar induction #refr, and cross-lingual transfer"}
{"pre": "[CLS] the work of #refr is similar to ours in that it is possible to use a more complex model", "cit": "[CLS] following the framework of global linear models in #refr, we cast this task as learning a mapping f from"}
{"pre": "[CLS] the most common approach to parsing is to use a log-linear parsing model and maximise the conditional likelihood of", "cit": "[CLS] combining the 1-best output of multiple parsers via parse selection or parse hybridization improves f-score over the best individual"}
{"pre": "[CLS] the translation model is a log-linear model #refr. [SEP] model weights ? ? ? ? ? ? ? ?", "cit": "[CLS] the translation model used in this paper is a phrasebased model #refr, where the translation units are so-called blocks:"}
{"pre": "[CLS] the most popular approach to paraphrase generation is the source sentences into a target language #otherefr; #refr. [SEP] input", "cit": "[CLS] gale and church #otherefr, and #refr), and word sense disambiguation for mt (e.g. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the polarity of a text with a text that is a text", "cit": "[CLS] alternatively, it may be included in the processing pipeline at a later stage, namely at the interface between the"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr.", "cit": "[CLS] we also used the cbc word clusters of pantel and lin #otherefr and #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for learning and beam-search decoding #otherefr;", "cit": "[CLS] in earlier work, #refr adopted a ?local first? iterative merge strategy for discovering phrase structure. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to word alignment is the hmm model #refr. [SEP] model is based on the hmm", "cit": "[CLS] class-based n-gram models have also been shown to benefit from their reduced number of parameters when scaling to higher-order"}
{"pre": "[CLS] the first is the task of coreference resolution #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] ? ? ? ?", "cit": "[CLS] iii and marcu, 2005) or structured knowledge bases such as wikipedia #otherefr and yago #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the generation of referring expressions that we will use the discourse treebank (rst-dt)", "cit": "[CLS] #refr show that for statistical machine translation, a simple smoothing method (dubbed stupid backoff) approaches the quality of kneser-ney"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of meaning that are not", "cit": "[CLS] klapaftis & manandhar #otherefr 86.4 hac 86.0 cwu 85.1 cww 84.7 #refr 84.5 mfs 80.9 [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the generation of referring expressions is a subject of research in natural language generation (nlg) in natural language generation", "cit": "[CLS] when vague input is available (e.g., in the generation component of a machine translation system, or in wvslwym-style generation"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of meaning that are not", "cit": "[CLS] the uses of this procedure include information retrieval #otherefr, text understanding, anaphora resolution #refr, language modelling #otherefr. [SEP] [PAD]"}
{"pre": "[CLS] we use the stanford named entity recognizer #refr to extract named entities from the text and extract named entities", "cit": "[CLS] there has been growing interest in characterizing social media users based on the content they generate; that is, automatically"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] a notable approach is lexicalized reordering #otherefr and #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second approach is to use a word segmentation and pos tagger to tag dictionary to tag each word", "cit": "[CLS] #refr present a framework based on direct maximum entropy model to construct the machine translation system. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] bold numbers indicate no significant difference from the best system (bootstrap resampling with p > 0.05) #refr. sent. word"}
{"pre": "[CLS] #refr proposed a method for learning a word alignment. [SEP] model that learns to predict the probability of a", "cit": "[CLS] #refr ranks each candidate compression using a function based on the dot product of a vector of weights with"}
{"pre": "[CLS] the similarity between two sentences is computed using the wordnet #otherefr and the wordnet similarity measure proposed by #refr.", "cit": "[CLS] other measures in dkpro similarity allow to compare texts by part-of-speech ngrams, and order and distance features for pairs"}
{"pre": "[CLS] the most common approach to paraphrase generation is to use a lexicon to paraphrase database into a target language", "cit": "[CLS] syntactic score (sc) some erroneous sentences often contain words and concepts that are locally correct but cannot form coherent"}
{"pre": "[CLS] the first is the task of identifying the polarity of a text that is a text that a text", "cit": "[CLS] such recognition comes from the ideas that crucial progress may derive from decomposing the complex rte task into basic"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a text with a text that are semantically", "cit": "[CLS] some recent studies have investigated the use of features related to a spoken response?s content, such as #refr. [SEP]"}
{"pre": "[CLS] in this paper, we propose a new model for semantic role labeling and semantic role labeling #refr. [SEP] [SEP]", "cit": "[CLS] recently, #refr presented the resource argument-mapped wordnet, providing entailment relations for predicates in wordnet. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the automatic evaluation of discourse structure of discourse structure of discourse relations #refr. [SEP] [SEP] [SEP]", "cit": "[CLS] these phenomena are well-known to be important to language understanding and rte #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a task of identifying the degree of semantic relations between words", "cit": "[CLS] framenet1 #otherefr; #refr is a lexicographic research project which aims to produce a lexicon containing very detailed information about"}
{"pre": "[CLS] in the case of dependency parsing, previous work has shown that it is possible to improve parsing performance on", "cit": "[CLS] the deterministic shift/reduce classifier-based dependency parsing approach #otherefr has been shown to offer state-of-the-art accuracy #refr with high efficiency"}
{"pre": "[CLS] the first is the automatic evaluation of subjectivity analysis in text classification #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] first, i identified the verb lemmas using nltk wordnet #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the one of the most popular approach to use crowdsourcing, which has been to use a", "cit": "[CLS] periment, we try to clarify the upper bound of the performance of the text segmentation task, which can be"}
{"pre": "[CLS] the first is the case for the syntactic dependency parsing of the english lexical substitution task #refr. [SEP] and", "cit": "[CLS] so although it is now possible to extract at least subcategorisation data from large corpora 2 with 2grishman &"}
{"pre": "[CLS] the grammar is a grammar that is a grammar that is a grammar that is a given a grammar", "cit": "[CLS] wide-coverage grammars are being developed for various languages #refr in several theoretical frameworks, e.g., lfg #otherefr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] we use the scfg decoder cdec #refr4 and build grammars using its implementation of the suffix array extraction method"}
{"pre": "[CLS] the most popular approach to translation models #otherefr; #refr rely on the source side and the target side #otherefr.", "cit": "[CLS] these techniques can be broadly divided into pre-ordering techniques, which first parse and reorder the source sentence into the"}
{"pre": "[CLS] the translation quality is measured by bleu #refr. [SEP] score #refr on the test set. [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] metrics such as translation edit rate #otherefr; #refr and meteor1 #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within the source sentence into a target sentence into", "cit": "[CLS] it is useful as a preprocessing step for a variety of nlp systems such as parsers and machine translation"}
{"pre": "[CLS] the grammar is a grammar that is a grammar that is a grammar that is a grammar that is", "cit": "[CLS] the best-known example is generalized lr parsing, also known as tomita is algorithm, described by tomita #otherefr and #refra)."}
{"pre": "[CLS] the second approach is to use a maximum entropy model #otherefr; #refr. [SEP] model to predict the probability distribution", "cit": "[CLS] in our segmentation system, a hybrid strategy is applied (figure 1): first, forward maximum matching #refr, which is a"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] it firstly introduced in #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the translation quality is measured by bleu #refr. [SEP] score #refr with the bleu score of the translation quality.", "cit": "[CLS] to automatically align the parallel corpora we used mgiza #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of meaning that are not", "cit": "[CLS] they proposed that text segments with similar vocabulary are likely to be part of a coherent opic segment. hnplementations"}
{"pre": "[CLS] we use the stanford pos tagger #refr to tokenize the english and then extract the parallel sentences and then", "cit": "[CLS] in recent years there have been several attempts to integrating topical information into smt either by learning better word"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] traditionally, backoff n-gram language models (bnlms) #refr are being widely used for these tasks. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to compute sentence importance. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] there has been a lot of research in determining the sentiment of words and constructing polarity dictionaries #otherefr; #refr."}
{"pre": "[CLS] we use the averaged perceptron algorithm #refr to train the parameters of the model with the averaged perceptron algorithm", "cit": "[CLS] hierarchical bayesian segmentation models have also become popular in statistical machine translation where there is a need to learn"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set.", "cit": "[CLS] we extract all phrase pairs of length 6 and below, that are consistent #refr with the word alignments. [SEP]"}
{"pre": "[CLS] in the last decade, there has been a lot of work on learning parsing with semantic parsers #otherefr; #refr.", "cit": "[CLS] early work showed much promise for this strategy #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the translation model is a log-linear model #refr. [SEP] model with the decoder that we use the model to", "cit": "[CLS] among these, smoothing techniques, such as good-turing, witten-bell and kneser-ney smoothing schemes (see #refr for an empirical overview and"}
{"pre": "[CLS] the model is trained using the giza++ implementation of ibm model 4 alignments #refr. [SEP] model 4 alignments #otherefr", "cit": "[CLS] #refr developed a statistical model to find word alignments, which allow easy integration of context-specific features. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the first is the automatic evaluation of discourse coherence evaluation in the discourse structure of the discourse structure of", "cit": "[CLS] our medium term outlook hopes to furtherincorporate grassroot acl resources such as the acl anthology network #otherefr and the"}
{"pre": "[CLS] the first is the same as #refr, which was used to create a word in the context of a", "cit": "[CLS] using an maximum entropy approach to pos tagging, #refr reports a tagging accuracy of 96.6% on the wall street"}
{"pre": "[CLS] the model is trained using the averaged perceptron algorithm #refr. [SEP] score #refr with the best results for the", "cit": "[CLS] dependency treebanks are becoming available in many languages, and several approaches to dependency parsing on multiple languages have been"}
{"pre": "[CLS] the most common approach to paraphrase generation is to use a lexicon to paraphrase database into a single sentence", "cit": "[CLS] paraphrase identification has been addressed previously, both using features computed from an offline corpus #refr and features computed from"}
{"pre": "[CLS] #refr use a similar approach to compute similarities between words in a sentence. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] we extend the state-of-the-art feature model recently introduced by #refr by adding an additional word cluster based feature template"}
{"pre": "[CLS] we use the stanford word segmenter #refr to tokenize the word segmenter for each word in the training and", "cit": "[CLS] table 4 shows translation results in terms of bleu #refr, ribes #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to translation models #otherefr; #refr rely on parallel data #otherefr. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] in the first setting, the goal is to effectively make use of both the old-domain and the new-domain parallel"}
{"pre": "[CLS] the model is trained with the perceptron algorithm of #refr. [SEP] score calculation avoids the entire candidate set. [SEP]", "cit": "[CLS] iii and #refr) and joint inference #otherefr) for coreference resolution and a related extraction task. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal (wsj) corpus #refr. [SEP] trees and the penn", "cit": "[CLS] since the initial release of the penn treebank #otherefr; #refr). [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the automatic evaluation of discourse structure of discourse structure of discourse relations #refr. [SEP] [SEP] [SEP]", "cit": "[CLS] sentence segmentation off-the-shelf sentence segmentators tend to be trained on newswire texts #refr, which significantly differ from the noisy"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] model with the average number of the average", "cit": "[CLS] other work has learnt semantic analyses from text in the context of interactions in computational environments #otherefr, #refr); text"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] in recent years several authors #otherefr; #refr proposed discriminative word alignment frameworks and showed that this leads to improved"}
{"pre": "[CLS] the first is the task of identifying the temporal ordering of the events and temporal relations between events #refr.", "cit": "[CLS] in order to refer to an intended object #otherefr; #refr utilized attributes of the target and binary relations between"}
{"pre": "[CLS] the first is the automatic evaluation of discourse coherence models in the natural language processing (nlp) tasks such as", "cit": "[CLS] for example, ward and litman #otherefr have shown that motivation can significantly affect which students benefit from a reflective"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for each sentence with dependency trees for", "cit": "[CLS] for sparse features we use word-edges features #refr which are shown to be extremely effective in both parsing and"}
{"pre": "[CLS] the model is trained with a maximum entropy model #otherefr, which is trained on the penn treebank wall street", "cit": "[CLS] this framework subsumes and justifies the popular heuristic ?early-update? for perceptron with beam search #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the sentence in", "cit": "[CLS] we test the statistical significance of all above-baseline results using randomised estimation (p < 0.05; #refr), and present all"}
{"pre": "[CLS] #refr use a bootstrapping method to identify opinion holders and extract opinion expressions from documents in a document pairs", "cit": "[CLS] most existing work in opinion summarization focus on predicting sentiment orientation on an entity #otherefr#refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second approach is to use a word lattice as a word segmentation and its context #otherefr; #refr. [SEP]", "cit": "[CLS] theoretically, there might be 330,000 possible morphological tags, but in practice, #refr extracted 2,200 different tags from their corpus,"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking approaches #otherefr; #refr to learning for dependency", "cit": "[CLS] note 1iwatate et al #otherefr compare their proposed algorithm with various ones that include sassano?s, cascaded chunking #refr, and"}
{"pre": "[CLS] the first is the stanford parser #refr and then converted to dependency structures for relation extraction #otherefr. [SEP] [SEP]", "cit": "[CLS] sonex, ollie, patty, treekernel, swirl, lund and our two variants of our method, ex- emplar, explained in detail in"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for english sentences. [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] #refr?s constituent context model (ccm) obtains 51.2% f-score on atis part-of-speech strings. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr.", "cit": "[CLS] some dependency parsing systems prefer two-stage architecture: unlabeled parsing and dependency classification #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the semantic relations between nominals of noun phrases and the verb classes", "cit": "[CLS] details of the task are reported in #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] sentence pair of a", "cit": "[CLS] state-of-the-art techniques for extraction of causal relations from text use automatic classifiers trained on lexical features to recognize relations"}
{"pre": "[CLS] the second approach is to use a word lattice as a sequence of a sequence of a sequence of", "cit": "[CLS] transformation-based l arning has also been successfully applied to text chunking #refr, morphological disambiguation #otherefr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the one of the most popular approach to use crowdsourcing, which has been to use a", "cit": "[CLS] dependency structures are more efficient to parse #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the sense inventory is a very similar to the one used in the one used in the most popular", "cit": "[CLS] semeval-2010 tasks on cross-lingual word sense disambiguation (lefever and #refr and cross-lingual lexical substitution #otherefr were organized. [SEP] [PAD]"}
{"pre": "[CLS] the task of identifying the semantic textual similarity of a text is a document #otherefr; #refr. [SEP] of a", "cit": "[CLS] exact matching was used as a baseline in previous recognizing textual entailment challenges #otherefr; #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to content determination is to use a document #otherefr; #refr. [SEP] model to predict the", "cit": "[CLS] some of these approaches to single document summarization have been extended to deal with multi-document summarization#otherefr, #refr, #otherefr. [SEP]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] the ibm and itg constraints #refr are used to restrict reorderings in practical phrase-based systems. #otherefr introduces the concept"}
{"pre": "[CLS] the first is the task of converting natural language processing #otherefr; #refr. [SEP] [SEP] to be able to help", "cit": "[CLS] some tasks can thrive on a nearly pure diet of unlabeled data #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the work of #refr is similar to ours in that it is possible to use a discourse representation of", "cit": "[CLS] the units that hierarchically precede a given unit are determined according to veins theory (vt) #refr, which is described"}
{"pre": "[CLS] the grammar is a grammar that is a grammar that is a grammar that is a given a grammar", "cit": "[CLS] they have been applied to incremental parsing #otherefr; #refr, discourse #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second approach is to use a word similarity measure and the similarity measure proposed by #refr. [SEP] score", "cit": "[CLS] at the same time, new research on the topic has been done, including the use of statistical translations of"}
{"pre": "[CLS] the system is based on the inprotk project at the university of pennsylvania and the xerox implementation of the", "cit": "[CLS] #refr showed that a more f lex ib le c lass i f i cat ion lead to better"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be a phrase-based system #otherefr;", "cit": "[CLS] but, of course, we never mixed development and test data in any result reported. from the question bank #refr."}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the sentence in", "cit": "[CLS] recently, researchers have developed algorithms that learn to map natural language sentences to representations of their underlying meaning #otherefr;"}
{"pre": "[CLS] the first is the task of relation extraction #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ?", "cit": "[CLS] most relation extraction systems #otherefr; #refr generalize semantic relations by taking into account statistics about the syntactic construction of"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for learning and beam-search decoding #otherefr;", "cit": "[CLS] 2.2.2 kernel expansion #refr and kudo and matsumoto #otherefr proposed kernel expansion, which explicitly maps both support set s"}
{"pre": "[CLS] the grammar is a grammar that is a grammar that is a given a grammar that is a grammar", "cit": "[CLS] 2see berwick and weinberg #otherefr, #refr, htgria and stallard #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the automatic evaluation of discourse structure of discourse relations #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] another study used preference voting to combine up to 5 mturk rankings of machine translation quality and showed that"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] #refr propose latent variable model and lexical network to determine so of phrases, focusing on ?noun+adjective? pairs. [SEP] [PAD]"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be np-hard #refr. [SEP] s", "cit": "[CLS] the approach started with the so-called ibm models #refr, implementing a set of elementary operations, such as movement, duplication"}
{"pre": "[CLS] the most common approach to paraphrase generation is to use a lexicon to paraphrase generation #otherefr; #refr. [SEP] words", "cit": "[CLS] pang et al provide a remedy to this problem by performing alignment on the charniak parse trees of the"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] some grammar-based models such as the hierarchical model #refr and the syntactified target language phrases model #otherefr have shown"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the word alignments for each word and then extract the word", "cit": "[CLS] this is akin to post-editing (though definitely not akin to the much more sophisticated approach in described in #refr)."}
{"pre": "[CLS] the second approach is to use a small number of manually annotated corpora such as the same data set", "cit": "[CLS] previous work has also described supervised and semi-supervised approaches to predicting inflectional morphology #otherefr; #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to learn a translation probabilities from", "cit": "[CLS] some of these models are contentindependent, such as distortion models #otherefr; #refrwhich assigns constant probabilities for monotone order and"}
{"pre": "[CLS] we use the moses toolkit #refr to train a 5-gram language model with kenlm and then generate the srilm", "cit": "[CLS] 3a hypergraph is analogous to a parse forest #refr. #otherefr). [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford word segmenter #refr to extract the word pairs from the target word alignments and then", "cit": "[CLS] case-insensitive nist bleu #refr was used to measure translation performance. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best results for the best", "cit": "[CLS] johnson et al #otherefr and #refr show how adaptor grammars can model the association between words and non-linguistic ?topics?;"}
{"pre": "[CLS] #refr used a similar approach to compute the sentiment lexicon for the polarity of words in a sentence. [SEP]", "cit": "[CLS] word clustering features contain some semantic information and have been successfully used in several natural language processing tasks, including"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] following #refr, lafferty et al #otherefr, we are looking for the set of feature weights ? maximizing the regularized"}
{"pre": "[CLS] the first is the annotation tool which was the annotation tool #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] the noisy dataset 2details omitted, see #refr for more details. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a text that is a", "cit": "[CLS] however, only recently has work been done on the automatic computation of such relationships from text, quantifying similarity between"}
{"pre": "[CLS] #refr proposed a method for learning a word sense disambiguation which is a word sense disambiguation method to be", "cit": "[CLS] the use of coarse-grained sense groups #otherefr has led to considerable advances in wsd performance, with accuracies of around"}
{"pre": "[CLS] the task of identifying the event extraction of event descriptions is a single document #otherefr; #refr. [SEP] [SEP] [SEP]", "cit": "[CLS] we retrieve all sentences from the corpus comprising at least two entities and a temporal expression, where we use"}
{"pre": "[CLS] the model is trained with a maximum entropy model #otherefr, which is trained on the penn treebank wall street", "cit": "[CLS] the beam-width can be defined in terms of a threshold in the number of edges allowed, or in terms"}
{"pre": "[CLS] the generation of referring expressions is a subject of research in natural language generation (nlg) in natural language generation", "cit": "[CLS] text planning is one of the distinct tasks identified in reiter is \"consensus\" architecture for natural language generation #refr:"}
{"pre": "[CLS] #refr used a similar method to identify the polarity of words in a sentence. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] such a lexicon can be used, e.g., to classify individual sentences or phrases as subjective or not, and as"}
{"pre": "[CLS] the task of identifying the semantic textual similarity of a text is a document #otherefr; #refr. [SEP] of a", "cit": "[CLS] while most prior work addressed only specific context matching scenarios, #refr presented a broader view, proposing a generic framework"}
{"pre": "[CLS] the system is based on the moses toolkit #otherefr and the moses toolkit #refr. [SEP] decoder with the best", "cit": "[CLS] expectation semiring the sufficient statistics for graph expected bleu can be computed using expectation semirings (li and #refr. [SEP]"}
{"pre": "[CLS] in the context of machine translation, there is a large body of research in determining the polarity of words", "cit": "[CLS] #refr scaled-up their work by using a crowdsourced emotion lexicon to track emotion dynamics over the course of many"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the longrange reordering problem as a sequence of words", "cit": "[CLS] active learning also has been applied to many nlp applications, including pos tagging #otherefr and parsing #refr. [SEP] [PAD]"}
{"pre": "[CLS] in the context of machine translation, itg has been explored for statistical word alignment in both the context of", "cit": "[CLS] currently, most dialogue systems rely on either canned responses or templates for generation, which can result in utterances which"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr.", "cit": "[CLS] there has been significant interest recently in machine learning methods that induce generative models with high-dimensional hidden representations, including"}
{"pre": "[CLS] the work of #refr is similar to ours in that it is possible to use a more complex model", "cit": "[CLS] our usage of inference is somewhat similar to the one proposed by ftri; however, with the formalised concept of"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a similar to the similarity of two sentences that is based", "cit": "[CLS] in fact, the only work that addresses exactly the same task is that of #refr, as far as we"}
{"pre": "[CLS] the first is the task of identifying the textual entailment (te) of identifying the same textual entailment #otherefr; #refr.", "cit": "[CLS] some work has thus focused on a re-ranking strategies (see geffet and #refr and geffet and dagan, 2005, who"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for dependency parsing #refr. [SEP] grammars", "cit": "[CLS] while this ?surface dependency approximation? #refr may be acceptable for certain applications of syntactic parsing, it is clearly not"}
{"pre": "[CLS] the most popular approach to translation lexicons from the target language model is the target side of the source", "cit": "[CLS] for example, #refr for a joint phrase based model, #otherefr for a complex model of insertion, deletion and head-word"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] event and semantic role", "cit": "[CLS] systems such as #refr and #otherefr [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? 2008. [SEP]", "cit": "[CLS] these supervised methods are limited by their reliance on the manually roletagged corpora of framenet #refr or propbank #otherefr"}
{"pre": "[CLS] the approach of #refr is based on the assumption that the assumption of the assumption that the assumption of", "cit": "[CLS] in natural-language processing #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of identifying the sentence pairs in a text is to be extracted from text #refr. [SEP] text", "cit": "[CLS] close to the problem studied here is jing and mckeown?s #refr cut-and-paste method founded on endres- niggemeyer?s observations. [SEP]"}
{"pre": "[CLS] #refr proposed a method for identifying the polarity of a document by a document into a document into a", "cit": "[CLS] textrank #refr and lexpager- ank #otherefr use algorithms similar to pagerank and hits to compute sentence importance. [SEP] [PAD]"}
{"pre": "[CLS] we use the stanford ner tagger #refr to identify the text in the text and extract the text and", "cit": "[CLS] for unsupervised methods, sentence importance can be estimated by calculating topic signature words #otherefr, combining query similarity and document"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with a probability distribution", "cit": "[CLS] the parser consists of a state (or a configuration) #refr have a forthcoming paper based on a similar idea"}
{"pre": "[CLS] the parser is trained on the penn treebank corpus and tested on the penn treebank wall street journal, parser", "cit": "[CLS] in light of these new results, we train the pcfg parser of #refr on ccgbank and achieve state of"}
{"pre": "[CLS] the grammar is a grammar that is a grammar that is a parsing system for the grammar that is", "cit": "[CLS] 4 i #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] previous challenges included the individual aspects of such a system, including event extraction, timex extraction, and event/time ordering #refr."}
{"pre": "[CLS] the first is the case for the syntactic parsing model of the semantic role labeling task of converting the", "cit": "[CLS] #refr and li and abe #otherefr studied how to find an optimal abstraction level of an argnment noun in"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is not to be useful for the purpose", "cit": "[CLS] state-of-the-art lexicon-building segmentation algorithms are typically reported to yield better performance than word boundary detection algorithms #otherefr; #refr. [SEP]"}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders and predict the polarity of words in a sentence. [SEP]", "cit": "[CLS] in terms of sentiment analysis, the work done by #refr in using part-of-speech and dependency structures to identify polarities"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be np-hard #refr. [SEP] s", "cit": "[CLS] it occurs because many systems, such as the ones proposed by #otherefr, and #refr represent their result space in"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a text that are semantically similar to the", "cit": "[CLS] the items mentioned in each unit were ranked according to thematic roles, using the ranking {agent > patient >"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] besides, there are various shared tasks connected to the sentiment analysis like #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of event extraction was first introduced by #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] in recent empirical work, these two subtasks have been tackled separately: #otherefr; #refr; markert et al., 2003; lassalle and"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] zollmann and venugopal #otherefr and #refr used broken syntactic fragments to augment their grammars to increase the rule coverage;"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a text that are semantically", "cit": "[CLS] used tools are the geniatagger #otherefr for tokenization and lemmatization, and the c&c parser #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the automatic metric of mt evaluation metrics such as bleu #refr, meteor #otherefr, and ter #otherefr.", "cit": "[CLS] subjects, scenarios, instructions data collection will proceed as described in #refr \\[16\\] with the following exceptions: (1) updated versions"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] 127) #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the em algorithm #refr with the", "cit": "[CLS] 8the n-best feature extraction already uses relative counts #otherefr 89.7 #refr 90.1 charniak and johnson #otherefr 92.1 [SEP] [PAD]"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] this setup presents unique challenges, since it typically involves non-professional translators whose language skills are varied, and since it"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr.", "cit": "[CLS] mst #refr 87.07 89.95 f . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of meaning that are not", "cit": "[CLS] hence, many nlp applications, such as keyword extraction #refr, social tag suggestion #otherefr, may also potentially benefit from distributed"}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders and predict the polarity of words in a sentence and", "cit": "[CLS] these hand-written summaries were then used to identify most relevant posts in a discussion thread in a manner similar"}
{"pre": "[CLS] we use the stanford named entity recognizer #refr to extract named entities from the text and extract named entities", "cit": "[CLS] more recently, #refr found that people?s ratings of interest for fairy tales can be successfully predicted using token-level scores"}
{"pre": "[CLS] the system used in this paper is based on the stanford ner tagger #refr. [SEP] parser #otherefr shared task", "cit": "[CLS] the set of features that we use, listed in table 5, is an extension of the set by #refr."}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal (wsj) corpus #refr. [SEP] trees and the penn", "cit": "[CLS] specifically, parsers for well-nested grammar formalisms are not confronted with the ?crossing configurations? that make the universal recognition problem"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] since only projective sequences can be handled by the shift-reduce scheme, we apply the pseudoprojective transformation introduced by #refr"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a similar to the one used in the sense of the", "cit": "[CLS] for example, in the context of the air traveler information system (atis) for spoken dialogue, #refr computed the probability"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] score is a variant of the best performing", "cit": "[CLS] brutus uses the ccg parser of #otherefr, henceforth the c&c parser), charniak?s parser #refr for additional cfg-based features, and"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] here we adopt two lexical weights called noisy-or features #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr proposed a method for learning a word sense disambiguation which is a word sense disambiguation method to identify", "cit": "[CLS] methods such as those of #refr, liu et al #otherefr are unsupervised but they typically use many adjustable parameters"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr.", "cit": "[CLS] normalizing katakana spelling variations has been the subject of research by itself #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to identify the polarity of words in a sentence. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] several recent studies have attempted to predict the readability of documents #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in this paper, we present a state-of-the-art dependency parser #refr to generate the parse tree in which the sentence", "cit": "[CLS] for an overview of different types of features that have been used in parse reranking see #refr. [SEP] [PAD]"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to learn a translation models from a", "cit": "[CLS] there has been some previous work on accuracy-driven training techniques for smt, such as mert #refr and the simplex"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] it was then proved to be efficient in solving the ranking task in information retrieval, and in syntactic parsing"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] the parser is coupled with an on-line averaged perceptron #refr as the learning method. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the word alignment of word alignment of word alignment of word alignment of", "cit": "[CLS] next, there is work on minimally supervised methods that use available resources such as dictionaries, bitexts, and other additional"}
{"pre": "[CLS] #refr use a similar approach to compute similarities between words in a sentence. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] this issue was noted by #refr who used a list of known very short words to detect these cases."}
{"pre": "[CLS] the first is the automatic evaluation of discourse structure of discourse relations #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] #refr centering algorithm. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to word alignment is the hmm alignment model #refr. [SEP] model is based on the", "cit": "[CLS] #refr and taskar et al #otherefr represent alignments with several feature functions that are then combined in a weighted"}
{"pre": "[CLS] in the past decade, there has been a lot of work on learning semantic parsers #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] #refr provide a comparison of various active learning strategies for sequence labeling tasks. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for parsing #refr. [SEP] grammars with", "cit": "[CLS] we performed experiments using three training algorithms: the averaged perceptron #refr, log-linear training (via conjugate gradient descent), and max-margin"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] #refr proposed several approaches for cross lingual subjectivity analysis by directly applying the translations of opinion corpus in source"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] algorithm with the em algorithm #refr with the best", "cit": "[CLS] discriminative structured prediction algorithms such as conditional random fields #otherefr, structured perceptron #refr, maxmargin markov networks #otherefr lead to"}
{"pre": "[CLS] the first is the task of coreference resolution #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] ? the typed-similarity [SEP]", "cit": "[CLS] #refr show that bestfirst clustering performs similarly to bell-treebased clustering, but neither of these algorithms 5when applying closest-first and"}
{"pre": "[CLS] the most common approach to paraphrase acquisition is to use a lexicon to automatically extracted from corpora #otherefr; #refr.", "cit": "[CLS] sentence similarity [ss] is emerging as a crucial step in many nlp tasks that focus on sentence level semantics"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] most work has focused on automatically learning reordering rules #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the grammar is a grammar that is a grammar that is a parsing system for data-driven parsing and a", "cit": "[CLS] the abduction-based approach #refr has provided asimple and elegant way to realize such a task. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of coreference resolution #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] ? ? ? ? ?", "cit": "[CLS] recent work on coreference resolution has shown that a rich feature space that models lexical, syntactic, semantic, and discourse"}
{"pre": "[CLS] the most common approach to paraphrase acquisition is to use a lexicon to automatically extracted from parallel corpus data", "cit": "[CLS] several studies have focused on the extraction of bilingual lexicons from comparable corpora #otherefr; #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the second international chinese word segmentation bakeoff #otherefr; #refr and the best results are achieved by using the", "cit": "[CLS] history-based models have been a popular approach in a variety of natural language processing #otherefr; #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to parsing is to use a linear model and maximise the structure to parse tree", "cit": "[CLS] machine learning methods were employed by #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to translation lexicons from parallel corpora #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ?", "cit": "[CLS] other work has used decipherment techniques to learn translations from monolingual and comparable data #otherefr; #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the system is based on the inprotk toolkit for the best performing system described in #refr. [SEP] system, and", "cit": "[CLS] in recent years, there has been growing interest in metaphor and metonymy resolution that is either corpus-based or evaluated"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] the", "cit": "[CLS] we employ the method of #refr to tokenise messages, and use token unigrams as features, including any hashtags, but"}
{"pre": "[CLS] #refr use a similar approach to compute sentence importance. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] one solution to this problem is to augment the lexicon of the morphological analyzer by extracting unknown morphemes from"}
{"pre": "[CLS] the first is the automatic evaluation of discourse structure of discourse structure of discourse structure of discourse structure of", "cit": "[CLS] our goal is not to link a ce to an event, neither to fix it on a ?temporal line\","}
{"pre": "[CLS] the task of identifying the overall sentence pairs in the overall structure is a text #refr. [SEP] of the", "cit": "[CLS] in order to understand what part of the history of the dialogue is important for processing fu qs, significant"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] the focus of this paper is on sentence-level argument detection rather than document-level stance classification (e.g., #refr, #otherefr). [SEP]"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] while local reordering can be directly memorized in phrases, modeling reordering at a phrase level still remains a major"}
{"pre": "[CLS] the translation quality is measured by bleu #refr. [SEP] score #refr with the bleu score of the translation quality.", "cit": "[CLS] approaches in the past have suggested using stems or synonyms for oov words as replacements #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the system is based on the inprotk project #refr. [SEP] score for the number of the user is not", "cit": "[CLS] generic and dr-specific rules use the cmu question generation tool #refr in combination with syntactic and lexical manipulation rules."}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a semantic role labelling task", "cit": "[CLS] because lexical information is highly sensitive to domain variation, approaches that can identify vcs, scfs and sps in corpora"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a text with a text corpus to be", "cit": "[CLS] the variability of the generated texts ranges from a close similarity to slightly shorternot an uncommon #refr, but not"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] the common practice of plugging some aspect of a learned itg into either #otherefr; #refr; saers et al#otherefr?obscures the"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] incidentally, the above attributes are the same as those used by the conventional stochastic dependency parsing methods #refr. [SEP]"}
{"pre": "[CLS] the first is the same as #refr, which we will use the inside-outside algorithm for the same as the", "cit": "[CLS] the idea is well known from #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second approach is to use a word lattice as a sequence of a sequence of a sequence of", "cit": "[CLS] the second benchmark that we adopted is the sighan bakeoff-2005 dataset #refr for chinese word segmentation. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to parsing is to use a log-linear parsing model and maximise the conditional random field", "cit": "[CLS] in statistical dependency parsing, morphological information is mostly used as features in the statistical classifier that guides the search"}
{"pre": "[CLS] the most popular approach to paraphrase generation has been the extraction #otherefr; #refr. [SEP] input text is based on", "cit": "[CLS] #refr point out that, for example, a rule like ?x acquire y ?? ?x buy y ? may work"}
{"pre": "[CLS] the first is the task of learning a semantic parser that is a semantic role labeling task in the", "cit": "[CLS] more recently, several groundedlearning approaches have been proposed to alleviate the annotation burden #otherefr; bo?#refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the system is based on the inprotk project at the xerox implementation of the xerox implementation of the xerox", "cit": "[CLS] conventional approaches to subcategorization, such as definite clause grammar #otherefr, and lexicalized tag #refr all deal with complementation by"}
{"pre": "[CLS] the most popular approach to translation lexicons from parallel corpora #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] although hiring professional translators to create bilingual training data for machine translation systems has been deemed infeasible, mechanical turk"}
{"pre": "[CLS] the first is the polarity of a polarity of a polarity of a polarity of a polarity of a", "cit": "[CLS] for example, researchers in nlp have begun to develop lexicons of connotations #refr, i.e., words associated with polarities out"}
{"pre": "[CLS] the first is the case for the automatic acquisition of subcategorization frames from corpora #refr, which is the web", "cit": "[CLS] the automatic acquisition of specific types of mwe has attracted much interest #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of coreference resolution #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] ? the typed-similarity [SEP] ?", "cit": "[CLS] as another example, denis and baldridge #otherefr and #refr perform joint inference for anaphoricity determination and coreference resolution, by"}
{"pre": "[CLS] the most common approach to coreference resolution is to use a conditional random field #otherefr; #refr. [SEP] model to", "cit": "[CLS] see #refr and hoffmann et al #otherefr for examples of such models. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the semantic textual similarity of the words in the context of the context", "cit": "[CLS] although we propose to construct the system without an intermediate representation language, we show that our combinatory categorial grammar"}
{"pre": "[CLS] the task of identifying positive and negative sentiments and negative sentiments from a passage, #otherefr; #refr. [SEP] of the", "cit": "[CLS] figure 4: bootstrapping algorithm for responsebased life event identification. modeling #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] event and event and", "cit": "[CLS] these distributions bear similarities with the previous work #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the automatic evaluation of the automatic evaluation of the automatic evaluation of the", "cit": "[CLS] figure 1: nagao is approach to settle this problem, #refr proposed a method to extract only useful strings. [SEP]"}
{"pre": "[CLS] the most common approach to this problem is to use a dictionary to model the similarity score for each", "cit": "[CLS] #refr model wordnet as a bayesian network to solve the ?explain away? ambiguity. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the system uses a combination of the maximum entropy model of #refr to generate multiple reference sentences, and a", "cit": "[CLS] some authors have already designed similar matching techniques, such as the one described in #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of identifying the semantic relations between nominals of a text is also relevant to the target language", "cit": "[CLS] #refr achieves 0.89 accuracy and 0.88 cws on the combined rte-2 and rte-3 dataset. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the pos tagging task is to be able to efficiently perform well on a small number of tags and", "cit": "[CLS] many approaches for pos tagging have been developed in the past, including rule-based tagging #otherefr, hmm taggers #refr, maximum-entropy"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] data sets the arabic data set contains two treebanks derived from the ldc penn arabic treebanks #otherefr, a dependency"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal, parser #refr and tested on the penn treebank", "cit": "[CLS] a permanent forwarding link is the usual forwarding link found in other algorithms (\\[#refr\\], \\[wroblewski, 1987\\], etc). [SEP] [PAD]"}
{"pre": "[CLS] the task of event extraction has received a lot of attention in recent years #otherefr; #refr. [SEP] [SEP] [SEP]", "cit": "[CLS] for bag of words feature in web people search, #refr illustrated that noun phrase and n-grams longer than 2"}
{"pre": "[CLS] #refr proposed a method for learning a word clustering words from a target language model to target language to", "cit": "[CLS] besides all the multilingual topic modeling work discussed above, comparable corpora have also been studied extensively #otherefr; #refr), but"}
{"pre": "[CLS] in the literature, slight variations of this problem are also referred to as unsupervised and semi-supervised learning #otherefr, #refr,", "cit": "[CLS] more recent work has used a subset of the observed word-tag pairs and focused on generalizing dictionary entries #otherefr;"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be np-hard #refr. [SEP] s", "cit": "[CLS] for the hierarchical and syntactically-enriched smt models, e.g., #otherefr; #refr, this training data is used for extracting statistically weighted"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best derivation of the best", "cit": "[CLS] our model is hmm-based, which makes it possible to efficiently process large amounts of data, allowing an evaluation on"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within the meaning representation of the meaning of the", "cit": "[CLS] although less common in traditional historical linguistics, phonetic alignment plays a crucial role in automatic approaches, with alignment analyses"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] this is the best automatically learned part-of-speech tagging result known to us, representing an error reduction of 4.4% on"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the sentence in", "cit": "[CLS] this includes learning from question-answer pairs #otherefr, with distant supervision #refr, and from sentences paired with system behavior #otherefrb)."}
{"pre": "[CLS] the system is based on the moses toolkit #otherefr and the standard phrase-based system described in #refr. [SEP] test", "cit": "[CLS] we run giza++ #refr for bidirectional word alignment, and then obtain the lexical translation table and phrase table. ."}
{"pre": "[CLS] we use the averaged perceptron algorithm #refr to train the parameters of the weights for the weights for the", "cit": "[CLS] #refr). [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the training data is to", "cit": "[CLS] the projected trees, albeit noisy, can then constitute the training data for data-driven tl parsers #otherefr; #refr. [SEP] [PAD]"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with the one of", "cit": "[CLS] however, another approach is to train a separate out-of-domain parser, and use this to generate additional features on the"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best derivation of the best", "cit": "[CLS] #refr used bottom-up clustering techniques on discourse contexts, performing sentencelevel model interpolation with weights updated ynamically through an em-like"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] recently, more linguistically motivated constraints are introduced to improve phrase-based decoding. #refr and #otherefr further presents a bracketing model"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] the", "cit": "[CLS] miwa et al#otherefra) proposed a hybrid kernel, which combines the all-paths graph (apg) kernel #refr, the bag-of-words kernel, and"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a text with a text that are semantically", "cit": "[CLS] also, #refr introduce discourse obligations as an alternative to joint intentions and shared plans, to allow for models of"}
{"pre": "[CLS] the system is based on the inprotk project #refr. [SEP] score for the number of the number of the", "cit": "[CLS] given an input string, n-best list or lattice, the cle applies unification-based syntactic rules and their corresponding semantic rules"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] the past decade has witnessed the rapid development of phrase reordering models #otherefr; #refr, just to name a few)."}
{"pre": "[CLS] the second approach is to use a small number of manually annotated corpora such as the same data set", "cit": "[CLS] rule-based methods, e. g. #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be a given by using", "cit": "[CLS] on this issue, we speculate that one of the feasible approach to realize a robust system is to divide"}
{"pre": "[CLS] in this paper, we propose a method for learning semantic parsing by using a large corpus of #refr. [SEP]", "cit": "[CLS] the first is usually focus on exploiting automatic generated labeled data from the unlabeled data #otherefr, the second is"}
{"pre": "[CLS] the grammar we are extracted from the treebank using the stanford parser #refr. [SEP] parser #refr with the self-trained", "cit": "[CLS] the basic corpus used was a set of 16,000 utterances from the air travel planning (atis; #refr) domain. [SEP]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] oriented parsing (?all-subtrees?) approaches, and self-training techniques brought further improvements, and recent results are starting to level off at"}
{"pre": "[CLS] the grammar is a grammar that is a grammar that is a grammar that is a grammar that is", "cit": "[CLS] the problem of computing the best tree is np- complete #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] yuqing#otherefr extends #refr ?s approach for recovering english non-local dependencies and applies it to chinese. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be a difficult problem #otherefr;", "cit": "[CLS] tree alignment in a variety of forms has been extensively used in machine translation systems #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the annotation tool which we will use the annotation tool #refr. [SEP] project", "cit": "[CLS] 7this property turned out to be problematic for adapting the gf parallel resource grammar to russian #refr. [SEP] [PAD]"}
{"pre": "[CLS] the most common approach to this problem is to use a dictionary to model the decision to generate a", "cit": "[CLS] this formulation of the task provided the basis for the noisy-channel en decisiontree based algorithms presented in #otherefr; #refr"}
{"pre": "[CLS] the second approach is to use a small number of seed examples for example outputs #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] similarly to previous work in hedge cue detection #refr, we first convert the task into a sequential labeling task"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] finally, some work has looked at applying semantic parsing to answer queries against large knowledge bases, such as yago"}
{"pre": "[CLS] we use the stanford named entity recognizer #refr to extract named entities from the text and extract named entities", "cit": "[CLS] previous work on sentence-based classification of scientific literature according to categories of information structure has mostly used feature-based machine"}
{"pre": "[CLS] the translation quality is measured by bleu #refr. [SEP] score #refr with the bleu score of the translation quality.", "cit": "[CLS] in the recent years statistical machine translation #otherefr, #refr, #otherefr have been in focus. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the system used for the experiments in this paper is based on the senseval-3 english lexical sample task #refr.", "cit": "[CLS] 3.2.1 resources the swat-e system used the english web1t 5- gram corpus #otherefr, roget?s online thesaurus , nltk?s implementation"}
{"pre": "[CLS] the second approach is to use a small number of seed examples for example outputs #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] in order to perform error-detection, we chose to adapt the approach of #refr which resembles uncertainty based sampling for"}
{"pre": "[CLS] the most popular approach to word alignment is the ibm models #otherefr; #refr. [SEP] model 1 and the hmm", "cit": "[CLS] a discussion of these two contrasting approaches can be found in #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of converting natural language processing #otherefr; #refr. [SEP] [SEP] [SEP] to be a number", "cit": "[CLS] #refr and bikel #otherefr show that removing bilexical dependencies hardly hurts the performance of the collins model2 parser, although"}
{"pre": "[CLS] the wsd system used in the experiments was based on the senseval-3 english lexical sample task #refr. [SEP] test", "cit": "[CLS] this resource has already been used on word sense disambiguation #refr, but it has not made use of glosses"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] in addition to the standard features including the rule translation probabilities, we incorporate features that are found useful for"}
{"pre": "[CLS] the model is trained using the averaged perceptron algorithm #refr. [SEP] score #refr with the best results for the", "cit": "[CLS] for instance, #refr describes a method for inducing a multilingual lexicon from a group of related languages. [SEP] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the training data is to", "cit": "[CLS] traditional adaptation techniques like #otherefr; ming-wei chang and #refr need to retrain the model for every new domain. [SEP]"}
{"pre": "[CLS] #refr proposed a method for learning a word sense disambiguation which is a word sense disambiguation method to identify", "cit": "[CLS] in the recent wssim annotation study #refr, senses sentence 1 2 3 4 5 6 7 annotator this question"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] only few studies have specifically focused on the linguistic content analysis of tweets, e.g. #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the semantic relations between nominals of noun phrases and semantic relations #refr.", "cit": "[CLS] characterising the semantic behaviour of phrases in terms of compositionality has particularly attracted attention in recent years #otherefr; #refr."}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr.", "cit": "[CLS] compositional distributional models of meaning vary in sophistication, from simple element-wise operations between vectors such as addition and multiplication"}
{"pre": "[CLS] the most popular approach to word alignment is the ibm models #otherefr; #refr. [SEP] model 1 and the hmm", "cit": "[CLS] this provides a diverse collection of translation hypotheses for mt system combination #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the work of #refr is similar to ours in that it is possible to distinguish between the elements of", "cit": "[CLS] most current anaphora resolution systems implelnent a pipeline architecture with three modules #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? 2008.", "cit": "[CLS] settings the charniak-johnson reranking parser #refr, along with a self-trained biomedical parsing model #otherefr, has been used for tokenization,"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] in a recent work #refr, linguistic and contextual information was effectively used in the framework of a hierarchical machine"}
{"pre": "[CLS] the system is based on the inprotk project #refr. [SEP] score for the user is not a given sentence,", "cit": "[CLS] although we address these states within the tutoring domain, speech researchers from other domains and applications are also focusing"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a text with a text that are semantically", "cit": "[CLS] #refr devised guidelines for the manual annotation of sentences in learner texts, and evaluated a rule-based approach that classifies"}
{"pre": "[CLS] the feature weights were tuned on the wmt-10 devset using mert #refr with pro #otherefr. [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] online methods #refr, are recognized to be effective, but require substantial implementation efforts due to difficulties with parallelization. [SEP]"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] algorithm with the best derivation of the best derivation", "cit": "[CLS] however, existing models of disambiguation with lexicalized grammars are a mere extension of lexicalized probabilistic context-free grammars (lpcfg) #refr,"}
{"pre": "[CLS] the first is the case for the generation of the generation of the generation of referring expressions from the", "cit": "[CLS] it is a bottom up, tabular algorithm [#refr] optimised for tags. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for each sentence with dependency parsing. [SEP]", "cit": "[CLS] #refr utilized a multitask learner within their semi-supervised algorithm to learn feature representations which were useful across a large"}
{"pre": "[CLS] the first is the case for the generation of referring expressions from the utterances that is the language model", "cit": "[CLS] algorithmic solutions, that is morphology based applications, are the only way to solve the problem #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained on the penn treebank corpus and tested on the penn treebank wall street journal, parser", "cit": "[CLS] the arrows below the words correspond to its associated dependency graph. based: for example, those described by #refr, barbero"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] previous work on sentiment analysis has covered a wide range of tasks, including polarity classification #otherefr, and opinion source"}
{"pre": "[CLS] the first is the task of converting natural language processing #otherefr; #refr. [SEP] [SEP] [SEP] to be a given", "cit": "[CLS] combining treebank transformation techniques with a suffix analysis, #refr trained a probabilistic parser and reached a labelled f-score of"}
{"pre": "[CLS] in recent years, there has been a growing interest in the task of generating referring expressions, reflected in the", "cit": "[CLS] recently, research into nlg systems that generate text from georeferenced data has begun to emerge #otherefrb; #refr. [SEP] [PAD]"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with a probability distribution", "cit": "[CLS] therefore, high precision gr sets can be determined by thresholding on the gr weight #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the polarity of a text that is a text of a text", "cit": "[CLS] we pos-tagged the test data with the tnt tagger #refr and developed finite state automata to detect such constellations."}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] it is also possible for the word alignments leading to phrase-based smt models to be learned through transduction grammars"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? 2008. [SEP]", "cit": "[CLS] recently there have been some improvements to the charniak parser, use n-best re-ranking as reported in #otherefr and selftraining"}
{"pre": "[CLS] #refr use a similar approach to compute sentence importance. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] figure 3: graphical model of srt factors that partition words in a sentence into flow and inert groups; we"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to handle the hierarchical phrase-based model #refr", "cit": "[CLS] however, #refr reported that the computational complexity for decoding amounted to o(j3+3(n?1)) with n-gram even using a hook technique."}
{"pre": "[CLS] #refr proposed a method for automatically learning entailment rules from the web by using a large corpus of annotated", "cit": "[CLS] word compositions have long been a concern in lexicography#otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] these annotations are often used 1the approaches are too numerous to list; we refer the interested reader to the"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] in this and other areas of text analysis and classification, recent years have seen a rise in use of"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for each sentence with dependency trees for", "cit": "[CLS] unlike minimum error rate training #otherefr, our system is able to exploit large numbers of specific features in the"}
{"pre": "[CLS] the system is based on the inprotk toolkit for instance in the standard metrics such as bleu #refr and", "cit": "[CLS] consequently, there is a recent common trend towards enriching the current models with some extra knowledge as the new"}
{"pre": "[CLS] the most common approach to word alignment is to use a log-linear word alignment model #refr. [SEP] model to", "cit": "[CLS] for example, part-of-speech #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the generation of the grammar used by #refr, which is the grammar and", "cit": "[CLS] one exception to this is #refr, where the target function space are the subsequential transducers, for which a limit-identification"}
{"pre": "[CLS] the most popular approach to machine translation is the one proposed by #refr. [SEP] bleu score #refr and ter", "cit": "[CLS] word alignment is newer, found only in a few places #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] systems based on this paradigm have recently been among the topranked submissions to public evaluation campaigns #otherefr; #refr. [SEP]"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for parsing #refr. [SEP] grammars with", "cit": "[CLS] we part-of-speech tag our data using a perceptron tagger similar to the one described by #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] we use stanford corenlp #refra; klein and manning, 2003b) to obtain dependencies. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the automatic evaluation of the automatic evaluation of the automatic evaluation of the", "cit": "[CLS] the measure of success for the domain-action based interlingua (as described in #refra)) is that (1) it covers the"}
{"pre": "[CLS] the system uses a combination of the maximum entropy model of #refr to generate multiple reference translations for the", "cit": "[CLS] not surprisingly, much published research on grammatical error correction focuses on article and preposition errors #otherefr; #refr; dahlmeier and"}
{"pre": "[CLS] the second approach is to use a word lattice as a sequence of word segmentation and its context #otherefr;", "cit": "[CLS] their model achieves 86.8% tagging accuracy with sparse pos priors and outperforms 74.50% accuracy of the standard second order"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] model with the average number of the average", "cit": "[CLS] it should be noted that the results reported by #refr were obtained using a corpus containing 1.6 terawords, making"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] the", "cit": "[CLS] although a wealth of lexical and ontological resources covering anatomical entities are available #otherefr; #refrb). [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a text that is a", "cit": "[CLS] following siegel and mckeown #otherefr; #refrb). [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to paraphrase generation is to use a lexicon to paraphrase generation #otherefr; #refr. [SEP] words", "cit": "[CLS] for details on dps and distributional measures, see #refr and turney and pantel #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the penn treebank #otherefr. [SEP] s", "cit": "[CLS] #refr have suggested that there are substantial subcategorization differences between written corpora and spoken corpora. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the translation quality is evaluated by case-insensitive bleu-4 #refr. [SEP] quality measured by bleu score #refr with the bleu", "cit": "[CLS] #refr described an elegant way of integrating automatically acquired probabilistic templates into the translation process, and nie?en and ney"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] event and semantic role", "cit": "[CLS] temporal, contingency, comparison and expansion #refr), the task of identifying discourse relations that hold between spans of text has"}
{"pre": "[CLS] in this paper, we propose a method for extracting parallel bilingual news articles from monolingual corpora #refr. [SEP] [SEP]", "cit": "[CLS] when evaluating extractive methods, it suffices to have a single human gold extractive compression per source sentence, because it"}
{"pre": "[CLS] the first is the automatic evaluation of the automatic metric of the automatic evaluation of the automatic evaluation of", "cit": "[CLS] #refra), that is, the term support verb is a hypernym of light verb. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr proposed a method for learning a dependency tree based on a probabilistic model that is a probabilistic model", "cit": "[CLS] this observation has formed the basis for important work on syntax projection across languages #otherefr; #refr and unsupervised syntax"}
{"pre": "[CLS] the generation of referring expressions is a subject of research in natural language generation (nlg) in natural language generation", "cit": "[CLS] the generator is informed by a corpus study of embedded discourse units on two discourse annotated corpora: the rst"}
{"pre": "[CLS] the wsd system is based on the standard approach of #refr and the best performing wsd system for wsd", "cit": "[CLS] note that senseval-3 also defined a translation or multilingual lexical sample task #refr, which is just like the english"}
{"pre": "[CLS] #refr used a similar method to extract word pairs from the mpqa corpus #otherefr. [SEP] corpus #otherefr. [SEP] #refr.", "cit": "[CLS] popular approaches for cross-lingual sentiment analysis #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the context of sentiment analysis #refr used a word based on the word based on the word sense", "cit": "[CLS] as a countermeasure, in #refr, we showed that non-expert annotations collected through amazon mechanical turk (mturk) can replace expert"}
{"pre": "[CLS] the most common approach to paraphrase generation is to use a lexicon to paraphrase database into a target language", "cit": "[CLS] from sentence-to-sentence aligned corpora, symbolic #otherefr, statistical #refr, or hybrid techniques #otherefr are used for word and expression alignments."}
{"pre": "[CLS] the first is the one of the most popular approach to use crowdsourcing, which has been to use a", "cit": "[CLS] to use guitar, we first parsed the texts using charniak?s parser #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the input is to be", "cit": "[CLS] correct results are obtained for swts with an accuracy of about 80% for the top 10-20 proposed candidates using"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best f1 score calculation of", "cit": "[CLS] we filter arcs by simply adapting the sieves method proposed in #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a semantic role labelling task", "cit": "[CLS] to reduce these costs, many methods have been proposed for automatically building thesauri #otherefr; #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] as proposed by #refr, bleu is approximately computed in the local batch, since bleu is not linearly decomposed into"}
{"pre": "[CLS] the model is trained using the moses decoder #refr. [SEP] score #refr with the decoder that the best alignment", "cit": "[CLS] #refr present a method for predicting the inflection of russian and arabic sentences aligned to english sentences. [SEP] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for each sentence in the training corpus.", "cit": "[CLS] this approach is related to transition-based dependency parsing such as #refr or dependency tree revision#otherefr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to word segmentation is to use a word segmentation and a dictionary as a sequence", "cit": "[CLS] this has been applied to bootstrapping syntactic parsers #otherefr, morphology #refr, tense #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr. [SEP]", "cit": "[CLS] reranking of n-best lists has recently become popular in several natural language problems, including parsing #otherefr, machine translation #refr"}
{"pre": "[CLS] the task of event extraction has received a lot of attention in recent years #otherefr; #refr. [SEP] [SEP] [SEP]", "cit": "[CLS] while weak supervision works well when the textual corpus is tightly aligned to the database contents (e.g., matching wikipedia"}
{"pre": "[CLS] #refr use a similar approach to identify the polarity of words in a sentence and their polarity in a", "cit": "[CLS] this has been shown in other work too #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the feature weights were tuned using minimum error rate training (mert) #refr on the development set using minimum error", "cit": "[CLS] in #refr a standard phrase-based model is augmented with more than a million features whose weights are trained discriminatively"}
{"pre": "[CLS] the second approach is to use a small number of manually annotated corpora such as the same data set", "cit": "[CLS] and ambiguity classes have been shown to be successfully employed, in a variety of ways, to improve pos tagging"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] this would allow to better exploit syntactic and shallow semantic structures, e.g. as in #otherefrb; #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for parsing #refr. [SEP] grammars with", "cit": "[CLS] various methods have been proposed to address the disadvantages of greedy local parsing, among which a framework of beam-search"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a text that is a", "cit": "[CLS] examples include the use of decision trees for syntactic analysis #refr, coreference #otherefra; daelemans et al, submitted). [SEP] [PAD]"}
{"pre": "[CLS] the system is based on the inprotk project #refr that the number of possible antecedents of possible antecedents of", "cit": "[CLS] in \\[pustejovsky, 1989\\] and \\[#refr\\], i suggest that there is a system of relations that characterizes the semantics of"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best results for the best", "cit": "[CLS] #refr). [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the literature, supervised approaches have been applied to unsupervised learning of natural language processing #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] to avoid this requirement, #refr suggested a ?multi-view? learning scheme based on re-ranking. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] these data-driven parsing approaches obtain state-of-the-art results on the de facto standard wall street journal data set #otherefrb; #refr,"}
{"pre": "[CLS] the second approach is to use a small number of seed examples for example outputs #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] there are accurate parsers available such as chaniak parser #otherefr, stanford parser #refr and berkeley parser #otherefr, among which"}
{"pre": "[CLS] the first is the task of identifying the semantic textual similarity of a text of a text in a", "cit": "[CLS] several studies have shown that extracting modifier-head pairs from text and including these as compound indexing terms can improve"}
{"pre": "[CLS] the work of #refr is similar to ours in that it is possible to use a more complex linguistic", "cit": "[CLS] text segmentation procedures (more with an information retrieval motivation, rather than being related to reference resolution tasks) have also"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with a probability distribution", "cit": "[CLS] the best system #otherefr, a variant of the mst algorithm, obtained 89.54 uas, while the second system #refr obtained"}
{"pre": "[CLS] the first is the task of identifying the temporal relation of the events and the events #refr. [SEP] event", "cit": "[CLS] since temporal anchors are not annotated in the timex2 standard, our system uses a simple heuristic method for temporal"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] previous approaches to wfst-based reordering #otherefr; #refr constructed permutation acceptors whose state spaces grow exponentially with the length of"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the input is to be", "cit": "[CLS] semantic parsers can be trained using sentences annotated with their formal representation #otherefr; #refr or various less restrictive annotations"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] we selected the standard german tiger treebank #otherefr; ku?#refr.8 this format allows a compact representation of the syntactic structure."}
{"pre": "[CLS] the grammar is a synchronous grammar formalism which is a synchronous grammar (scfg) used to reorder the grammar (scfg)", "cit": "[CLS] in the context of the np-hardness of decoding in statistical machine translation #otherefr; #refr, it is natural to ask"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal (wsj) treebank #otherefr; #refr. [SEP] trees with the", "cit": "[CLS] earlier studies by #refr and dubey #otherefr reports that lexicalization of pcfgs decrease the parsing accuracy when parsing negra?s"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] the resource presented in #refr uses a similar binomial annotation for single words; another interesting resource is wordnetaffect #otherefr"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the entire sequence of the probability", "cit": "[CLS] explicitly specifying the causal and computational structure of a model enables the investigation of hypotheses such as the feasibility"}
{"pre": "[CLS] the dependency parser is trained on the penn treebank #refr and the penn treebank #otherefr. [SEP] score #otherefr. [SEP]", "cit": "[CLS] broadly speaking, we can classify the methods to incorporate semantic information into parsers in two: systems using static lexical"}
{"pre": "[CLS] the second approach is to use a maximum entropy model #otherefr; #refr. [SEP] model to predict the probability of", "cit": "[CLS] work by #refr first used this framework for chinese word segmentation by treating it as a binary decision task,"}
{"pre": "[CLS] in the case of dependency parsing, previous work has shown that it is possible to improve the performance of", "cit": "[CLS] in order to make the search tractable, the feature set needs to be restricted to features over single edges"}
{"pre": "[CLS] the learning algorithm used in this paper is a variation of the winnow update rule extraction algorithm #otherefr; #refr.", "cit": "[CLS] generation is evaluated using bleu score #refr between generated sentences and reference nl sentences in the test set. [SEP]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for each sentence in the training corpus.", "cit": "[CLS] for example, incorporating long-distance dependencies and syntactic structure can help the lm better predict words by complementing the predictive"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to translate the phrases and the", "cit": "[CLS] they were estimated using the standard tools giza++ #refr and moses #otherefr applying default settings and lowercased training data."}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] model with the average number of the average", "cit": "[CLS] to improve their estimates, factored language models (flms) are used along with generalized parallel backoff #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to compute sentence importance. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] #refr presented a method for guessing pos tags of pre-segmented unknown words that took into consideration all the occurrences"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be np-hard #refr. [SEP] s", "cit": "[CLS] another source of data sparsity that occurs in all languages is proper names, which have been handled by using"}
{"pre": "[CLS] the task of identifying the semantic textual similarity of a text is a document #otherefr; #refr. [SEP] of a", "cit": "[CLS] in addition, we also compared it to a supervised approach #refr, that we regarded as an upper bound, obtaining"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] while there has been substantial previous work on the task of sf acquisition from corpora #otherefr; #refr; briscoe and"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with a probability distribution", "cit": "[CLS] prior work in this area has considered the effect of splitting and merging these states #refr, as well as"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a text with a text corpus of english", "cit": "[CLS] this basic idea has been exploited by many proposals for distributional similarity and clustering, e.g., #otherefr; #refr. [SEP] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to compute sentence importance. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] this restriction imposes major difficulties on authorship attribution systems, since authorship attribution methods that work well on long texts"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best results for the best", "cit": "[CLS] to obtain their corresponding weights, we adapted the minimum-error-rate training algorithm #refr to train the outside-layer model. [SEP] [PAD]"}
{"pre": "[CLS] the most common approach to coreference resolution is to use a separate task #otherefr; #refr. [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] it is worth noting that researchers typically adopt a pipeline coreference architecture, performing discourse-new detection prior to coreference resolution"}
{"pre": "[CLS] the second approach is to use a small number of manually annotated corpora such as the penn treebank #otherefr,", "cit": "[CLS] similarly to the more recent work by koo et al. #otherefr or #refr, he addresses the question of how"}
{"pre": "[CLS] the grammar we are extracted from the treebank using the berkeley parser #refr and the charniak parser #otherefr. [SEP]", "cit": "[CLS] furthermore, they can be used to parse dependency structures (kuhlmann and #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the dialogue manager (dm) is the one of the most common approach to nlg systems that of the dialogue", "cit": "[CLS] commandtalk #otherefr and trains-96 #refr are spoken language systems but they interface to simulation or help facilities rather than"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for learning and beam-search decoding #otherefr;", "cit": "[CLS] over the past decade, there has been tremendous progress on learning parsing models from treebank data #otherefr; #refr. [SEP]"}
{"pre": "[CLS] the system is based on the inprotk toolkit for dialogue systems that participated in the system described in #refr", "cit": "[CLS] for example, a dialog system can be evaluated by measuring the system is ability to help users achieve their"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with a probability distribution", "cit": "[CLS] similar ideas have been explored in #refr, collins and duffy #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second approach is to use a small number of seed examples for example outputs #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] though multilingual word embeddings have been employed in the literature, they are developed for other nlp tasks such as"}
{"pre": "[CLS] the similarity between two sentences is computed using the wordnet #otherefr and the wordnet similarity measure described in #refr.", "cit": "[CLS] the input phrases are passed on one hand directly to the n-gram similarity module, and on the other they"}
{"pre": "[CLS] in the second international chinese word segmentation bakeoff #otherefr; #refr and the best results are achieved by the best", "cit": "[CLS] dependency parsing has been intensively studied in recent years #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the automatic evaluation of discourse coherence models in the natural language processing (nlp) tasks such as", "cit": "[CLS] in our experiments, we focus on neutrality and style flaws, since they are of particular high importance within the"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be a given by using", "cit": "[CLS] language models constitute an important feature for assessing readability #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the overall sentence pairs from the target sentence using the same text", "cit": "[CLS] in answer typing phase, td-qa assigns the most possible answer type t? to a given question q based on:"}
{"pre": "[CLS] in this paper, we propose a method for learning a sequence of word alignment model #refr. [SEP] [SEP] [SEP]", "cit": "[CLS] this has been shown both in supervised settings #otherefr; #refr in which constraints are used to bootstrap the model."}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to learn a translation probabilities from", "cit": "[CLS] recently, #refr tried the marcu and wong model constrained by a word alignment and also found that koehn, et"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? ?", "cit": "[CLS] as expected, each semantic resource has different volume and accuracy figures when evaluated in a common and controlled framework"}
{"pre": "[CLS] the generation of referring expressions is a subject of research in natural language generation (nlg) in natural language generation", "cit": "[CLS] as the referents in #refr domain are all points on a map distinguishable only by their spatial relations to"}
{"pre": "[CLS] the first is the discourse structure of discourse structure of discourse structure of discourse structure in the discourse structure", "cit": "[CLS] moreover, selecting alternatives on the basis of their syntactic, semantic, and pragmatic ontributions to the sentence using tag allows"}
{"pre": "[CLS] the most popular approach to translation lexicons from parallel corpora #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] we have evaluated the one-translation-per-discourse feature using the cdecmt system #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to this task is to use a lexicon to model that of a sentence level", "cit": "[CLS] even though some works try to address the problem of summarizing multiparty written conversions #otherefr; #refr), they do so"}
{"pre": "[CLS] the first is the stanford ner tagger #refr, which is based on the dependency parser #otherefr and the dependency", "cit": "[CLS] for syntactic information, we used the stanford parser #refr and the stanford dependency representation #otherefr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a text with a text that are semantically", "cit": "[CLS] semantic parsers can be trained using sentences annotated with their formal representation #otherefr; #refr or various less restrictive annotations"}
{"pre": "[CLS] in this paper, we propose a discriminative model for dependency parsing by #refr which is used to extract the", "cit": "[CLS] finally, #refr use features, somewhat like qg configurations, on the shift-reduce actions in a monolingual, targetlanguage parser. [SEP] [PAD]"}
{"pre": "[CLS] the first is the automatic evaluation of discourse structure of discourse relations #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] in previous work, argumentation schemes have been classified in constrained domains, especially in legal argumentation #otherefr; #refr the araucaria"}
{"pre": "[CLS] the second approach is to use a small number of seed examples for example outputs #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] recently, #refr and chen et al #otherefr proposed more 4for these experiments, the same settings were used as in"}
{"pre": "[CLS] the most common sense (mfs) is to identify the words and their words in a text in a text", "cit": "[CLS] this requires part-of-speech tagging the glosses, for which we use the stanford maximum entropy tagger #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the learning algorithm used in this paper is a variation of the winnow update rule based statistical parsing algorithm", "cit": "[CLS] to provide a benchmark for comparison, we applied the pccg-based semantic parser called ubl, developed by #refr. [SEP] [PAD]"}
{"pre": "[CLS] the first is the task of coreference resolution #otherefr; ng and #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ?", "cit": "[CLS] prior work showed that models that jointly resolve mentions across multiple entities result in better performance than simply resolving"}
{"pre": "[CLS] we use the stanford named entity recognizer #refr to extract named entities from the text corpus. [SEP] [SEP] [SEP]", "cit": "[CLS] in recent years several techniques have been developed for semantic lexicon creation (e.g., #refr). [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the event extraction of event descriptions of event descriptions of event descriptions", "cit": "[CLS] 1http://cleartk.googlecode.com/ thus, each classifier in the cleartk-timeml pipeline uses only the features shared by successful models in previous work"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] as mentioned by (al-onaizan and #refr, it can be problematic that these deterministic choices are beyond the scope of"}
{"pre": "[CLS] the second approach is to use a small number of seed examples for example outputs #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] the fourth and final system is the mxpost system as described by #refr2; henceforth tagger e, for entropy). [SEP]"}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders and predict the polarity of words in a sentence and", "cit": "[CLS] although subjectivity tends to be preserved across languages ? see the manual study in #otherefr, #refr hypothesize that subjectivity"}
{"pre": "[CLS] in this paper, we present a bayesian approach to discriminative learning for probabilistic models #otherefr; #refr. [SEP] grammars with", "cit": "[CLS] probabilistic context-free grammars are an essential ingredient in many natural language processing models #otherefr; #refr; cohen and smith, 2009,"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to learn a translation models from a", "cit": "[CLS] in the derivation on the left, a memorized phrase pair captures the movement of the verb #refr. [SEP] [PAD]"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] #refr examine opinion holder extraction using crfs with various manually defined linguistic features and patterns automatically learnt by the"}
{"pre": "[CLS] the first is the automatic metric of discourse structure of discourse relations #refr. [SEP] quality assessment of the discourse", "cit": "[CLS] repetition in the input is often exploited as an indicator of importance by different summarization approaches #otherefr; #refr. [SEP]"}
{"pre": "[CLS] the most popular approach to word alignment is the one proposed by #refr. [SEP] model is to use a", "cit": "[CLS] for example, compound splitting enables smt systems to translate a compound on a word-by-word basis, even if the compound"}
{"pre": "[CLS] the translation model is a log-linear model #refr. [SEP] model with respect to the decoder that does not necessarily", "cit": "[CLS] the resulting weighted synchronous grammar represents, as in hiero, the ?parse forest? #otherefr; #refr, as is the case with"}
{"pre": "[CLS] the model is trained using the averaged perceptron algorithm #refr. [SEP] score #refr with the best results for the", "cit": "[CLS] f1 b r basic-unigram em ? 76.9 (0.1) feature-unigram em 0.2 84.5 (0.5) lbfgs 0.2 88.0 (0.1) #refr 87"}
{"pre": "[CLS] the system is based on the inprotk toolkit for the best performing system described in #refr. [SEP] score for", "cit": "[CLS] systems which produce short summaries of small amounts of data, such as weather-forecast generators #refr, have been one of"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] for example, in the domain of syntactic parsing, a range of recent work has exploited the mutual constraint between"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the input is to be", "cit": "[CLS] 8edge label frequencies for each pos were computed from the training data for the mst parser #refr. [SEP] [PAD]"}
{"pre": "[CLS] the second approach is to use a small number of seed examples for example outputs #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] to evaluate srl with automatic parsing, we use a state-of-the-art parser, bikel parser4 #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? 2008. [SEP] ?", "cit": "[CLS] table 1: sentence and apposition distribution apposition extraction is a common component in many nlp tasks: coreference resolution #otherefr;"}
{"pre": "[CLS] the first is the case for the semantic textual similarity of the words in the context of the context", "cit": "[CLS] it seems nevertheless that all 2#refr, smadja #otherefr use statistics in their algorithms to extract collocations from texts. [SEP]"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] our system is based on that of bjo?#refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best derivation of the best", "cit": "[CLS] #refr solve the same problem using a ? search. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the semantic relation between nominals in the verb classes were extracted from the wordnet sense tagged with the stanford", "cit": "[CLS] #refr present a lexical similarity model based on random walks on graphs derived from wordnet; rao et al #otherefr"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] every word w in such corpus is represented as one co-occurrence vector as in #otherefr with the setting discussed"}
{"pre": "[CLS] the first is the one of the most popular approach to use a word in which is to use", "cit": "[CLS] #refr present a knowledgefree unsupervised model in which orthographybased distributional cues are combined with semantic information automatically extracted from"}
{"pre": "[CLS] the system is based on the inprotk toolkit for the best performing system described in #refr. [SEP] system, which", "cit": "[CLS] the 5compare to #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to translation models #otherefr; #refr rely on parallel data #otherefr. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] nevertheless, the models as well as the pipeline have been optimized in several ways to achieve tasks such as"}
{"pre": "[CLS] we use the stanford corenlp suite to lemmatize and partof-speech tag each word #otherefr; #refr, and parse the input", "cit": "[CLS] recent work by #refr, by contrast, offers an analysis of zero anaphora in a cr architecture. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to parsing is to use a log-linear parsing model and maximise the conditional likelihood of", "cit": "[CLS] the actual implementation is derived from the decoder by #refr, which was shown to be efficient even for very"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] other works applied the reranking framework to different nlp tasks such as named entities extraction #otherefr, parsing (collins and"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a text that is a", "cit": "[CLS] this can complicate or even preclude a complete linguistic analysis, leading us to the following research question: which linguistic"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] because of its good tradeoff between efficiency and expressiveness, btg restriction is widely used for reordering in smt #refr."}
{"pre": "[CLS] the second approach is to use a word lattice as a sequence of a sequence of a sequence of", "cit": "[CLS] more recent approaches #otherefr; #refr used mdl in combination with existing algorithms, such as branching entropy #otherefr, to determine"}
{"pre": "[CLS] the first is the task of identifying the polarity of a text with a text that is a text", "cit": "[CLS] for example, #refr found that character bigrams are quite useful for nli, which led them to suggest that second"}
{"pre": "[CLS] #refr proposed a method for learning semantic parsers from a target language for a target language for a target", "cit": "[CLS] this is in line with the findings of #refr, who reports no benefit from manually corrected or unsupervised pos"}
{"pre": "[CLS] the most common approach to parsing is to use a linear model trained on a maximum entropy classifier trained", "cit": "[CLS] we present modifications to an existing system, mstparser #refr, to incorporate a very simple model of morphological agreement. [SEP]"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with a probability distribution", "cit": "[CLS] finding the optimal tree in the set of projective trees can be done efficiently #otherefr; #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to compute sentence importance. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] most attempts to perform this task have engineered various feature sets, augmenting words with topic or content models #otherefr,"}
{"pre": "[CLS] the most common approach to parsing is to use a linear model trained on a maximum spanning tree (mst)", "cit": "[CLS] stochastic parsers for english trained on the penn treebank have peaked their performance around 90% #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the second approach is to use a word similarity measure and the similarity measure proposed by #refr. [SEP] score", "cit": "[CLS] from a large corpus of japanese web documents #refr (100 million documents), where each sentence has a dependency parse,"}
{"pre": "[CLS] the most common approach to this problem is to use a word sense disambiguation algorithm #otherefr; #refr. [SEP] model", "cit": "[CLS] in this paper, we expand queries using the hinoki ontology #refr, which includes related words extracted from the definition"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a task of identifying the degree of semantic relations between words", "cit": "[CLS] in the last 4-5 years, researchers have begun to introduce compositional operations on distributional semantic representations, for instance to"}
{"pre": "[CLS] in the literature, slight variations of this problem are also referred to as unsupervised learning #otherefr, decision tree learning", "cit": "[CLS] #refr present an algorithm for learning with constraints, but this method requires users to set weights by hand. [SEP]"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] algorithm with the best derivation of the best derivation", "cit": "[CLS] we achieve 73.2/76.5% lp/lr on section 23 of the penn treebank, compared to 82.9/82.4% lp/lr of roark #otherefr and"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to learn a translation models from a", "cit": "[CLS] however, with the algorithms proposed in #refr, it is possible to develop a general-purpose decoder that can be used"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] we also implemented a method based on distributional similarity: using lin?s proximity-based thesaurus #refr trained on our in-house essay"}
{"pre": "[CLS] we use the moses toolkit #refr to train a 5-gram language model with kenlm and then extract the srilm", "cit": "[CLS] many statistical translation models #otherefr; #refrb) try to model word-to-word correspondences between source and target words. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the first is the annotation tool which was the annotation tool for the annotation tool #refr. [SEP] project at", "cit": "[CLS] early systems investigated rule-based approaches to parsing the durations and orderings of events from the tenses and aspects of"}
{"pre": "[CLS] the system is based on the inprotk project #refr. [SEP] score for the user is not a given input", "cit": "[CLS] new ways of mapping computer vision to generated language have emerged in the past few years, with a focus"}
{"pre": "[CLS] #refr use a similar approach to identify the topics of discourse relations in terms of discourse relations in terms", "cit": "[CLS] a number of token-based approaches have been discussed in the literature, both supervised #otherefr), weakly supervised #refr) and unsupervised"}
{"pre": "[CLS] the system uses the stanford ner tagger #refr to extract the stanford parser for the best parse the stanford", "cit": "[CLS] to this day, there have been only a few systems reported that work on multiple languages #otherefr; #refr. [SEP]"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a text that is a", "cit": "[CLS] menestrina et al#otherefr, v- measure #refr, normalized mutual information #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the polarity of a text with a text that is a text", "cit": "[CLS] various statistical approaches, e.g., a maximum entropy model #refr, hmms and svms #otherefr, have been used with various feature"}
{"pre": "[CLS] the segmentation model is trained with the perceptron algorithm described by #refr. [SEP] score is a word segmentation algorithm", "cit": "[CLS] morphological segmentation has been shown to be beneficial to a number of nlp tasks such as machine translation #refr,"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] while there has been some work in the last few years on enriching the output of state-of-the-art parsers that"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for learning and beam-search decoding #otherefr;", "cit": "[CLS] #refr and zhang and clark #otherefr proposed stacking methods to combine graph-based parsers with transition-based parsers. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the task of segmenting utterances into words, for example, johnson and colleagues have been shown to be effective in", "cit": "[CLS] the relevant algorithms include maximum entropy #otherefr, robust risk minimization (rrm) classification method #refr, etc. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second approach is to use a word sense disambiguation algorithm to find the most probable word sense disambiguation", "cit": "[CLS] an alternate way to optimize weights over translation features is described in #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the one of the most popular approach to use a word in which is to use", "cit": "[CLS] the reduction in keystrokes also translates into a lower degree of fatigue from typing all day #refr. [SEP] [PAD]"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] we carry out translation experiments on the test set by hierarchical phrase-based #otherefr system to demonstrate the utility of"}
{"pre": "[CLS] the most common approach to this problem is to use a word representation of a word or word or", "cit": "[CLS] measuring the similarity between sentences is an important problem that is relevant to many areas of language processing, including"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal, parser #refr and tested on the penn treebank", "cit": "[CLS] #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the generation system is based on the inprotk project #refr. [SEP] score for the user is a given input", "cit": "[CLS] #refr proposed an analysis of such systems in terms of a simple three stage pipeline. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the shared task included three supporting tasks: coreference #otherefr, the bionlp?09 shared task on event extraction #refr, and the", "cit": "[CLS] we used the stanford lexicalized parser #refr to extract word-related information, as well as for dependency parsing. [SEP] [PAD]"}
{"pre": "[CLS] we use the stanford ner tagger #refr to identify the text in the text and extract the text and", "cit": "[CLS] although coherence has been studied widely in a field of multi-document summarization #refr, it has not been studied enough"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a text that is a", "cit": "[CLS] recently, there has been an increased interest in approaches to automatically learning to recognize shallow linguistic patterns in text"}
{"pre": "[CLS] the first is the one of the most popular approaches to automatically identify the text #otherefr; #refr. [SEP] words", "cit": "[CLS] in addition, we have incorporated the cybertrans embedded machine translation system which ?wraps? available machine translation engines to make"}
{"pre": "[CLS] the system is based on the inprotk toolkit for the best system described in #refr, a system that uses", "cit": "[CLS] in an attempt to relate existing nlg systems to the rags framework [#refr], tg/2 was among the systems to"}
{"pre": "[CLS] the most popular approach to word alignment is the ibm models #otherefr; #refr. [SEP] model 1 and the hmm", "cit": "[CLS] there has been a long line of research on learning translation pairs from non-parallel corpora #otherefr; #refr. [SEP] [PAD]"}
{"pre": "[CLS] the second approach is to use a maximum entropy model #otherefr; #refr. [SEP] model for the probability distribution over", "cit": "[CLS] if the information contributed by the features is too specific to the training data, overfitting becomes a problem #otherefr;"}
{"pre": "[CLS] the second approach is to use a maximum entropy model #otherefr, which is based on the conditional probability of", "cit": "[CLS] note that, while we restrict ore: discussion to analysis of jat)anese senl;(;nc(;s in this t)~l)er, what we present l)elow"}
{"pre": "[CLS] the first step is to use a dialogue system that is based on the one described in #refr. [SEP]", "cit": "[CLS] an incremental method by devault, sagae, and #refr found possible points that a system could interrupt without loss of"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr.", "cit": "[CLS] the model bears resemblance to scott miller is novel work in the air traffic information system (atis) task, as"}
{"pre": "[CLS] the first is the case for the syntactic analysis of the english genitive, the penn treebank #refr, which we", "cit": "[CLS] they include such topics as: work with various corpus manipulation and annotation tools, use of various pos taggers and"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be np-hard #refr. [SEP] s", "cit": "[CLS] further, existence of efficient learning algorithms #otherefr; #refr that make no language specific assumptions, make inversion transduction grammars a"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] we use bleu #refr, ter #otherefr, which are the most-widely used mt evaluation metrics. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to learn a translation models from a", "cit": "[CLS] second, obtaining simplified sentences by word deletion is a well-studied issue #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the generation of referring expressions is a subject of research in natural language generation (nlg) in natural language generation", "cit": "[CLS] indicates equal author contribution. al., 2010; kulkarni et al., 2011; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a task of semantic relations between words and phrases and logical", "cit": "[CLS] metonymy is a pervasive phenomenon in language and the interpretation of metonymic expressions can impact tasks from semantic parsing"}
{"pre": "[CLS] the second approach is to use a discriminative model #otherefr; #refr. [SEP] model to predict the probability of the", "cit": "[CLS] previous joint models mainly focus on word segmentation and pos tagging task, such as the virtual nodes method #otherefra),"}
{"pre": "[CLS] the second approach is to use a semantic parser to generate the parse tree and the sentence in a", "cit": "[CLS] we further apply a semantic kernel (sk), namely the smoothed partial tree kernel #refr, which uses the lexical similarity"}
{"pre": "[CLS] in the future, we plan to investigate the use of a bayesian model for learning from decision trees #otherefr;", "cit": "[CLS] this technique has previously been used not only for part-of-speech tagging #otherefr, but also for prepositional phrase attachment disambiguation"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be np-hard #refr. [SEP] s", "cit": "[CLS] inside-out alignments #otherefr and generalized multitext grammars #refr, which are all way more complex than itg, stsg and (2,2)-brcg."}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] sentence and thus a", "cit": "[CLS] while the work by kleinbauer et al is among the earliest research on abstracting multi-party dialogues, much attention in"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] only n-ary rules n>2 are factored. perceptron-trained tagger, using the tagger documented in #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the grammar is a grammar that is a parsing model that of #refr and a probability distribution over a", "cit": "[CLS] for example, algorithms for restricted cases are being studied, e.g., by go?mez-rodr??#refr, as well as rank reduction, primarily in"}
{"pre": "[CLS] the first is the case for the automatic acquisition of subcategorization frames from the web #otherefr; #refr. [SEP] test", "cit": "[CLS] we do not specifically discuss here multilingual issues in collocation extraction; these are dealt with in a separate paper"}
{"pre": "[CLS] the system is based on the inprotk toolkit for the best performing system described in #refr. [SEP] system, which", "cit": "[CLS] at least two methods for finding bitext maps have been described in tile literature #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to learn a translation probabilities from", "cit": "[CLS] note that it is a subsection of the whole cn that is converted from the word lattice in figure"}
{"pre": "[CLS] the first is the one of the most popular methods for automatic acquisition of lexical acquisition of paraphrase patterns", "cit": "[CLS] for example, #refr say that ?to gain a real insight into the abilities of dsms (distributional semantic models, a/n)"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal (wsj) treebank #otherefr; #refr. [SEP] trees and the", "cit": "[CLS] this interest is motivated by two observations: first, well-nestedness is interesting as a generalization of projectivity #otherefr are non-projective,"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be np-hard #refr. [SEP] s", "cit": "[CLS] #refr presented such a system for transforming semantic-level dependecy trees into syntactic-level dependency trees for text generation. [SEP] [PAD]"}
{"pre": "[CLS] the task of semantic relation extraction has received a lot of attention in recent years #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] but whereas cdp?s perit remains very difficult to prevent semantic drift #refr from occurring. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the grammar we are extracted from the treebank using the berkeley parser #refr and the charniak parser #otherefr. [SEP]", "cit": "[CLS] this makes it possible to use lcfrs as a syntactic backbone with which various formalisms can be parsed by"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] the", "cit": "[CLS] focusing more specifically on the molecular interactions between genes and proteins, the bionlp shared task on event extraction #refr"}
{"pre": "[CLS] the grammar is a grammar that is a parsing model that of #refr and a probability distribution over a", "cit": "[CLS] the moves made by the parser can be explained by an automaton which is weakly equivalent to tags called"}
{"pre": "[CLS] #refr used a similar method to extract dependency relations. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] while the examination of adjectives is highly important for sentiment analysis (as shown by #refr who were able to"}
{"pre": "[CLS] the first is the automatic evaluation of discourse structure of discourse relations #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] some of the data comes from the parsed files 2-21 of the wall street journal penn treebank corpus #otherefr"}
{"pre": "[CLS] the learning algorithm we use is a variant of the structured perceptron #refr. [SEP] algorithm #refr with the exact", "cit": "[CLS] and previous research on english srl shows that combination is a robust and effective method to alleviate srl?s dependency"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] methods have been proposed for automatically identifying rationales #otherefr, #refr, zhao et al. #otherefr). [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the longrange reordering problem as a sequence of words", "cit": "[CLS] the external function f (x,y) returns a vector (called the global feature vector in #refr) of the number of"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] the", "cit": "[CLS] table 1: f-value from our bioevent system compared to turku09 (bjo#refr results, using approximate span/approximate recursive matching based on"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] they typically work by traversing the syntactic structure either bottom-up #refr or topdown #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] shared", "cit": "[CLS] we represent the data into a columns format, following the standard format of the conll shared task 2006 #refr,"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the penn treebank #otherefr. [SEP] s", "cit": "[CLS] for work on stochastic ltags see #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with a probability distribution", "cit": "[CLS] for german, our parser achieves an f1 of 79.8% compared to 81.5% by the state-of-the-art and substantially more complex"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal (wsj) corpus #refr. [SEP] test sets, with the", "cit": "[CLS] the full system can be extended in a variety of ways ? for example, by pruning pos tags but"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] for more information about the setup, see #refr in this paper, i will summarize the main findings from the"}
{"pre": "[CLS] the first is the annotation tool which was the annotation tool for the annotation tool #refr. [SEP] project at", "cit": "[CLS] although related, this problem is distinct from tense and aspect interpretation i discourse #otherefr, and #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the unsupervised approach has been shown to be effective in many natural language processing tasks, including information extraction #otherefr;", "cit": "[CLS] our bootstrapping model can be viewed as a form of self-training #otherefr; mue#refr). [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to translate the phrases as the", "cit": "[CLS] systems were tuned on newstest2012 with mert #otherefr and then binarized by using kenlm toolkit #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for learning and beam-search decoding #otherefr;", "cit": "[CLS] the output is support set s|t | and coefficients ?|t | (optionally, w?), to which the efficient classification techniques"}
{"pre": "[CLS] in the second international chinese word segmentation bakeoff #otherefr; #refr and the best results are achieved by the best", "cit": "[CLS] reranking approaches have given improvements in accuracy on a number of nlp problems including parsing #otherefr; #refr, machine translation"}
{"pre": "[CLS] the system is based on the inprotk toolkit for the paradise evaluation of #refr, which uses a combination of", "cit": "[CLS] sphinx2?s best hypothesis is then sent to why2-atlas for syntactic and semantic analysis #refr, discourse and domain processing #otherefr."}
{"pre": "[CLS] the task of identifying the overall sentiment in text has been the focus of the focus of the focus", "cit": "[CLS] to address these issues we propose a distantly supervised approach which applies labeledlda #refr to leverage large amounts of"}
{"pre": "[CLS] we use the moses toolkit #refr to train the weights for the feature weights for each system. [SEP] [SEP]", "cit": "[CLS] hypergraphs have been successfully used in parsing #otherefr; #refr and machine translation #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford ner tagger #refr to train the training data with the training data and test data", "cit": "[CLS] #refr applied online learning on domain adaptation and proposed to combine multiple similar source domains to perform online learning"}
{"pre": "[CLS] the most common approach to deriving selectional preference acquisition is to use a set of seed words #otherefr; #refr.", "cit": "[CLS] the microsoft video description corpus #refr, msvd) is a resource providing textual descriptions of videos. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr used a supervised method to detect sentiment in the sentiment in the sentiment in the sentiment in the", "cit": "[CLS] in the following sections, to check if there was a statistically significant difference in the results, we used student?s"}
{"pre": "[CLS] in the context of sentiment analysis and opinion mining has been used to improve the performance of sentiment analysis", "cit": "[CLS] when used as an additional feature with word based language models, it has been shown to improve the system"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking approaches #otherefr; #refr to learning for dependency", "cit": "[CLS] we discriminatively trained our parser in an on-line fashion using a variant of the voted perceptron #refr. [SEP] [PAD]"}
{"pre": "[CLS] we use the stanford named entity recognizer #refr to extract named entities from the text to identify named entities", "cit": "[CLS] bayessum (daume? and #refr and the special words and background model #otherefr are very similar to topicsum. [SEP] [PAD]"}
{"pre": "[CLS] the most popular approach to smt is the one proposed by #refr, who use a large set of features", "cit": "[CLS] #refr use a noisy channel model to achieve whole-sentence grammar correction; they learn a noise model from a dataset"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to learn a translation probabilities from", "cit": "[CLS] today there are more than 6000 living languages spoken in the world #otherefr, and most of them have little"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the sentence in", "cit": "[CLS] this was shown to yield higher correlation with human judgments #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to paraphrase acquisition is to use a lexicon to automatically extracted from corpora #otherefr; #refr.", "cit": "[CLS] it provides substitutions for more than 30,000 words of running text from two domains of masc #otherefr; #refr, a"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking approaches #otherefr; #refr to learning for dependency", "cit": "[CLS] following #otherefr, we present an application of structural correspondence learning to non-projective dependency parsing #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to identify the polarity of words in a sentence. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] the language identification of tweets was performed by using the langid.py python library #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford ner tagger #refr to generate the sentences and the sentences in the text with the", "cit": "[CLS] an alternative are coverage-based models (?2.1; #refr, which seek a set of sentences that covers as many diverse ?concepts?"}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders and predict the polarity of words in a sentence and", "cit": "[CLS] this amounts to performing binary text categorization under categories objective and subjective #refr; 2. determining document orientation #otherefr. [SEP]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] reranking has previously been applied to semantic role labeling by #refr, from which we use several features. [SEP] [PAD]"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? ? ? ?", "cit": "[CLS] previous works #otherefr; #refr usually leveraged mappings between nl phrases and logical predicates as lexical triggers to perform transformation"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] more recently developed hierarchical systems (e.g., #refr) may be better equipped to deal with reordering of this type; however,"}
{"pre": "[CLS] the second dataset is the use of a word sense disambiguation algorithm which is based on the most widely", "cit": "[CLS] additionally, pedersen #refr has pursued the approach of using simple word bigrams and other linguistically impoverished feature sets for"}
{"pre": "[CLS] in the second international chinese word segmentation bakeoff #otherefr; #refr and the segmentation bakeoff #otherefr are two of the", "cit": "[CLS] another thread of relevant research has explored the use of features in unsupervised pos induction #otherefr; #refr. [SEP] [PAD]"}
{"pre": "[CLS] the most common approach to paraphrase generation is to use a lexicon to automatically extracted from corpora #otherefr; #refr.", "cit": "[CLS] examples include mining the world-wide web for parallel text #refr and building parallel corpora from comparable corpora such as"}
{"pre": "[CLS] the most popular approach to translation lexicons from parallel corpora #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ?", "cit": "[CLS] then, the two bi-texts and their word alignments would be concatenated and used to build a phrase table #refr."}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] following previous work #otherefr use label propagation to analyze other semi-supervised algorithms such as the #refr algorithm. responding to"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] to train the proposed predicate translation model and argument reordering model, we first parsed all source sentences using the"}
{"pre": "[CLS] the similarity between two sentences is computed using the wordnet #otherefr and the wordnet similarity measure proposed by #refr.", "cit": "[CLS] since judging the correctness of these paraphrases ?out-of-context? is rather difficult we limit ourselves to giving examples and analyzing"}
{"pre": "[CLS] the most common approach to selectional preference acquisition is to use a candidate set #otherefr; #refr. [SEP] is to", "cit": "[CLS] lapata et al investigate the correlations between the co-occurrence counts #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to word alignment is the ibm models #otherefr; #refr. [SEP] model 1 and the hmm", "cit": "[CLS] since the bilingual corpus is a valuable resource for training statistical language models [dagon, 91; su et al, 95;"}
{"pre": "[CLS] the most popular approach to translation lexicons from parallel corpora #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ?", "cit": "[CLS] generally speaking, grammatical cohesion is less common and harder to identify than lexical cohesion #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] the other type of highprecision parser, which is based on dependency analysis was introduced by collins #refr. [SEP] [PAD]"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] algorithm with respect to search space is a probability", "cit": "[CLS] recently, specific probabilistic tree-based models have been proposed not only for machine translation #otherefr; #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a text with a text that are semantically", "cit": "[CLS] #refr named their hashing trick random feature mixing and empirically supported it by experimenting on nlp tasks. [SEP] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] event and semantic relations", "cit": "[CLS] the state-of-the-art resources on verb semantics, such as wordnet, verbnet, prop- bank, framenet, etc. #otherefr; #refr, provide information about"}
{"pre": "[CLS] we use the moses decoder docent #otherefr; #refr to generate the weight for each of the weight for each", "cit": "[CLS] syntax-based approaches, on the other hand, model the hierarchical structure of natural languages #otherefr; #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the berkeley parser #refr to obtain the syntactic parse trees for each sentence and the english and", "cit": "[CLS] further work in this direction employed lexi- ?this work has been done while the first author was visiting microsoft"}
{"pre": "[CLS] the shared task included three supporting tasks: coreference #otherefr, the bionlp?09 shared task on event extraction #refr, and the", "cit": "[CLS] finally, dependency relations based on the stanford parser provided better performance in our case, in contrast to general consensus"}
{"pre": "[CLS] we use the stanford named entity recognizer #refr to extract named entities from the text and extract named entities", "cit": "[CLS] some other approaches are focused on generating a structured representation of events #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal (wsj) treebank #otherefr; #refr. [SEP] trees and the", "cit": "[CLS] probabilistic models where probabilities are assigned to the cfg backbone of the unification-based grammar have been developed #otherefr; #refr,"}
{"pre": "[CLS] the system is based on the inprotk toolkit for the best system described in #refr, which uses a combination", "cit": "[CLS] bot4 #refr profile. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of relation extraction has received much attention in recent years #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] previous work has noted that distinguished relations, such as hypernymy (is-a) and meronymy (part-whole), are often expressed using a"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] we use kenlm because it has been shown #refr to be faster and use less memory than srilm #otherefr."}
{"pre": "[CLS] the second approach is to use a discriminative model #otherefr; #refr. [SEP] model to predict the probability of the", "cit": "[CLS] we use linear chain conditional random fields #otherefr to train the word segmentation model and pos tagging model, and"}
{"pre": "[CLS] the translation quality is measured by bleu #refr. [SEP] [SEP] = 4. [SEP] = 4. [SEP] = ? =", "cit": "[CLS] following the guidelines of the workshop we built baseline systems, using the lower-cased europarl parallel corpus (restricting sentence length"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] a number of recent studies rely on this notion of parallelism and demonstrate that annotations can be adequately projected"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] event", "cit": "[CLS] finally, our previous work #refr proposed composing new relations out of chains of previously extracted relations. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to word alignment is the hmm model #refr. [SEP] model is based on the one", "cit": "[CLS] co-occurrence information between eighboring words and words in the same sentence has been used in phrase extraction #otherefr; #refr,"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] results: table 2 shows uncased bleu scores #refr on the test set. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with a probability distribution", "cit": "[CLS] the approach in this paper is reminiscent of co-training #otherefrb) and up-training #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a bootstrapping method to identify the polarity of words in a sentence. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] motivated by #refr, we complement 3-grams with additional features that capture longer spans of text and generalize using part"}
{"pre": "[CLS] the task of identifying the overall sentiment in text is a text #otherefr; #refr. [SEP] of the same event", "cit": "[CLS] textrank and its variants #otherefr; #refr are graph-based text ranking models, which are derived from google?s pagerank algorithm #otherefr."}
{"pre": "[CLS] in this paper, we present a bayesian approach to learning based on a discriminative model of #refr. [SEP] [SEP]", "cit": "[CLS] for example, #refr demonstrated a convex training method for semi-supervised dependency parsing; lashkari and golland #otherefr 26.5 10.9 33.3"}
{"pre": "[CLS] the grammar is a grammar that is a synchronous grammar (stsg) #refr. [SEP] grammars are capable of translation for", "cit": "[CLS] the result could be derived from the findings in #refr that synchronous rewriting systems as scfgs are related to"}
{"pre": "[CLS] the most common approach to paraphrase extraction is to use a lexicon based on bilingual parallel corpus or comparable", "cit": "[CLS] in a multilingual formulation, this hypothesis states that the translations of a word are likely to appear in similar"}
{"pre": "[CLS] the first is the process of annotating the word in a language #refr. [SEP] model with the process of", "cit": "[CLS] the search is based on the property that when computing sim(wl, w2), words that have high mutual information values"}
{"pre": "[CLS] the model is based on a probabilistic model of #refr. [SEP] model that is a probability distribution over a", "cit": "[CLS] coherence relations have been used in generation \\[mekeown, 1985, #refr, moore and paris, 1988, horacek, 1992\\] but none of"}
{"pre": "[CLS] we use the stanford ner tagger #refr to train the feature weights. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] choi et al. #otherefr extended the sequential prediction approach to jointly identify opinion holders; #refr jointly detected polarity and"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be a difficult problem #otherefr;", "cit": "[CLS] the corpora are the tanaka corpus #otherefr), the japanese wikipedia corpus (7,949,605),3 and the kyoto university text corpus with"}
{"pre": "[CLS] the task of event extraction has received a lot of attention in recent years #otherefr; #refr. [SEP] [SEP] [SEP]", "cit": "[CLS] in order to alleviate this bottleneck, a possible solution is to exploit global knowledge from the related documents and"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] of a text in", "cit": "[CLS] although the vast majority of work on reference resolution has been with monologic text, some recent research has dealt"}
{"pre": "[CLS] in the context of machine translation, itg has been explored for paraphrase generation #otherefr; #refr. [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] previous methods for paraphrase #otherefr; #refr and a parallel corpus based approach #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best results for the best", "cit": "[CLS] character-based feature templates we adopt the ?non-lexical-target? feature templates in #refra). [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal (wsj) treebank #otherefr; #refr. [SEP] test sets, with", "cit": "[CLS] this was done to some extent in #refr to automatically generate training data for the log-linear disambiguation component of"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal, parser #refr and tested on the penn treebank", "cit": "[CLS] as #refr point out, the inherently top-down character of goal freezing interpreters may occasionally cause serious troubles during execution"}
{"pre": "[CLS] #refr proposed a method for learning a word sense disambiguation which uses a word to identify the same word", "cit": "[CLS] #refr addressed the phonetic substitution problem by extending the initial letter-to-phone model. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the last decade, there has been a lot of work on joint inference in nlp #otherefr; #refr. [SEP]", "cit": "[CLS] several bayesian inference approaches have also been proposed #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the context of machine translation, itg has been explored for paraphrase generation #otherefr; #refr. [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] pg shows its importance in many areas, such as question expansion in question answering #otherefr, and sentence similarity computation"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for dependency parse trees for dependency parse", "cit": "[CLS] in the future, it will be very interesting to use syntactic/semantic kernels, as for example in #refr; bloehdorn and"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a text that is a", "cit": "[CLS] a number of semi-supervised learning methods have been proposed in the literature for reducing annotation effort, such as active"}
{"pre": "[CLS] the system is based on the inprotk project #refr. [SEP] score for the number of the number of the", "cit": "[CLS] as #refr have shown however, sloppy identity is not necessarily linked to vp-ellipsis. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to compute the polarity of the polarity of the polarity of the polarity of", "cit": "[CLS] the most commonly used datasets include: the mpqa corpus of news documents #refr, web customer review data #otherefr, etc."}
{"pre": "[CLS] we use the stanford chinese segmenter #refr to tokenize the dependency parser for chinese sentences. [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] joint n-gram models #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the giza++ implementation of ibm model 4 alignments #refr. [SEP] model 4 alignments #otherefr", "cit": "[CLS] nova?k #otherefr obtained an improvement of 0.22 bleu with no distortion penalty; whereas #refr enhanced by 0.5 points using"}
{"pre": "[CLS] the second approach is to use a small number of seed examples for training data #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] because it is expensive to construct sensetagged corpus or bilingual corpus, many researchers tried to reduce the number of"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] the word alignment is computed using giza++ for the selected 73,597 sentence pairs in the fbis corpus in both"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] rnn utilizes the recursive structure of text, and it has achieved state-of-the-art sentiment analysis results for movie review dataset"}
{"pre": "[CLS] the grammar we are extracted from the parallel corpus using the berkeley parser #refr and the berkeley parser #otherefr.", "cit": "[CLS] different from synchronous grammars #otherefr; #refr is that occurrences of terminal symbols are also coupled. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the one of the most popular approaches to automatically identify the words in a text that", "cit": "[CLS] in the arc-eager approach introduced by #refr the possible actions are as follows, with s0 being the token on"}
{"pre": "[CLS] in this paper, we propose a method for extracting parallel bilingual news articles from monolingual news articles from monolingual", "cit": "[CLS] one pair (2%) of our sample had a ?compression? that was identical to the input. is the author of"}
{"pre": "[CLS] in the literature, supervised learning methods have been applied to unsupervised learning of natural language processing #otherefr; #refr. [SEP]", "cit": "[CLS] given large amounts of labeled sentences, supervised methods are able to achieve good performance #otherefr; #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] #refr proposed a method for learning semantic parsers from a target language for a target language for a target", "cit": "[CLS] syntactic category refinement our work also relates to work in syntactic category refinement in which pos categories and parse"}
{"pre": "[CLS] the first is the automatic evaluation of the automatic metric of the automatic evaluation of the automatic evaluation of", "cit": "[CLS] because learner writing is generally highly erroneous, an emerging research trend has focused on automated error detection in l2"}
{"pre": "[CLS] the first is the automatic metric of discourse structure of discourse structure of text #refr. [SEP] is the discourse", "cit": "[CLS] like #refr, we generate lexical variants of the target automatically by replacing either the verb or the noun constituent"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to learn a translation rules for", "cit": "[CLS] we split the resulting list into training, development and testing parts and we trained and tuned a character-level macedonian-"}
{"pre": "[CLS] we use the berkeley parser #refr to obtain the syntactic parse trees for each sentence and the english and", "cit": "[CLS] these include phrase-based smt #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser used in this paper is the conll shared task on dependency parsing #refr. [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] #refr developed a system to predict semantic roles #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the automatic evaluation of the automatic metric of the automatic evaluation of the automatic evaluation of", "cit": "[CLS] recently, lau et al. #otherefra,b) found this method to give the overall best performance on two wsi shared tasks"}
{"pre": "[CLS] the task of identifying the semantic textual similarity of a document is a text #refr. [SEP] of a text", "cit": "[CLS] one way to do this grounding in the context of distributional semantics is to obtain representations that combine information"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? 2008.", "cit": "[CLS] #refr adapted an entity-based coreference resolution model to extend automatically the training corpus. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal (wsj) section of the penn treebank #otherefr; #refr.", "cit": "[CLS] for instance, the crisp generation algorithm #refr, while specified for tag, could be generalized to arbitrary grammar formalisms that"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] our system was developed using version 2.0 of the mpqa corpus #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] probability estimates of the rhs given the lhs are often smoothed by making a markov assumption regarding the conditional"}
{"pre": "[CLS] #refr use a supervised sentiment classifier to detect the polarity of words in a target words and their polarity", "cit": "[CLS] to learn word vectors for sentiment analysis, maas et al. #otherefr, #refr re-embed words from existing word embeddings and"}
{"pre": "[CLS] the first is the automatic evaluation of discourse structure of discourse structure of discourse structure of discourse structure of", "cit": "[CLS] czesl #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second approach is to use a word lattice as a sequence of a sequence of a sequence of", "cit": "[CLS] typically, the local context around the word to be sense-tagged is used to disambiguate the sense #otherefr; #refr, or"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] sentence pair of a", "cit": "[CLS] there has been much research effort on automatic disfluency detection in recent years #otherefr; #refr, particularly from the darpa"}
{"pre": "[CLS] the first is the automatic classification of discourse structure of discourse relations #refr. [SEP] is the discourse structure of", "cit": "[CLS] these agreement results are in line with the agreement noticed in previous studies on agreement/disagreement annotations in online interactions"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] this approach was pioneered by #refr, and there has been a lot of research since, usually referred to as"}
{"pre": "[CLS] in this paper, we propose a new model to use a discriminative model to generate multiple translation of word", "cit": "[CLS] domain adaptation #otherefr; #refr often distinguishes general-domain data from in-domain data. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of semantic similarity to be", "cit": "[CLS] several paradigms proposed for a variety of notions, such as word sense #otherefr or frame semantics #refr, have given"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] the first two were annotated as part of the pennbioie project #refr and consist of sentences drawn from either"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the entire sequence of the entire", "cit": "[CLS] when data have distinct sub-structures, models exploiting latent variables are advantageous in learning #otherefr; #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] this idea was first explored for weakly supervised learning #refr and recently by naseem et al#otherefr for multisource cross-lingual"}
{"pre": "[CLS] in this paper, we present a system that uses a combination of the semantic parser trained on the penn", "cit": "[CLS] this includes linguistics studies of relational nouns #otherefr, parsing as abduction #refr, and other more general theories for lexicons"}
{"pre": "[CLS] the first is the task of converting natural language processing #otherefr; #refr. [SEP] [SEP] [SEP] to be a given", "cit": "[CLS] our system is a bottom-up projective dependency parser, based on the cubic-time algorithm by #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the sentence planning problem that the user is a text generation of", "cit": "[CLS] naturally, this problem extends to the acquisition of language, where approaches such as #otherefr; #refr have focused on basing"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to learn a translation models from a", "cit": "[CLS] we performed language model interpolation and batch-mira tuning #refr using newstest2010 (2,849 sentence pairs). [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford ner tagger #refr to train the feature weights. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] we also investigate dynamic mappings (search orders) with an easy-first policy #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second approach is to use a word sense disambiguation algorithm to find the most probable word as described", "cit": "[CLS] in fact, supervised word-based wsd systems are very dependent of the corpora used for training and testing the system"}
{"pre": "[CLS] the model is trained using the moses decoder #refr. [SEP] score #refr with the decoder that the best alignment", "cit": "[CLS] #refr apply this method to generate inflections after translation in two different ways: by rescoring inflected n-best outputs or"}
{"pre": "[CLS] the first is the automatic metric of text with human annotation of text segments as shown in #refr. [SEP]", "cit": "[CLS] it was also used to compute the distributional similarity between words c#refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr proposed a method for learning a word segmentation and a word segmentation model that uses a word segmentation", "cit": "[CLS] among the various models for tagging, there are maximum entropy models #otherefr, and other succesful approaches #refr. [SEP] [PAD]"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] examples include information extraction #refr, question answering #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first group is the user to use of dialogue acts from the dialogue acts to the dialogue acts", "cit": "[CLS] in the implemented voice dialogue system \"the circuit fix-it shop\" #refr, the dialogue fragment given in figure 1 occurs"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] an improvement of up to +1.35 was observed on the english-to-german and up to +0.63 bleu points on the"}
{"pre": "[CLS] the wsd system used for wsd in the experiments was performed on the experiments described in #refr for the", "cit": "[CLS] the goal of clwsd is to predict semantically correct translations for ambiguous words in context #otherefr; #refr. [SEP] [PAD]"}
{"pre": "[CLS] the second approach is to use a maximum entropy model #otherefr; #refr. [SEP] model to predict the probability distribution", "cit": "[CLS] we also use baseline feature templates include the features described in previous works #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford ner tagger #refr to train the english and the chinese sentences. [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] for example, transductive approaches have used automatic translations of monolingual corpora for self-training modules of the generative smt pipeline"}
{"pre": "[CLS] #refr use a similar approach to compute the polarity of words in a sentence. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] commonly used to compress sentences, tree-based approaches #otherefr; #refr compress a sentence by editing the syntactic tree of the"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a similar to the one used in the sense of the", "cit": "[CLS] framenet #refr, propbank #otherefr ? that convey information about lexical predicates and their arguments. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in recent years, there has been a growing interest in the task of generating referring expressions, reflected in the", "cit": "[CLS] this is by no means a trivial problem ? this sort of aggregation interferes with referring and coherence planning"}
{"pre": "[CLS] the parser is trained on the penn treebank corpus and tested on the penn treebank wall street journal, parser", "cit": "[CLS] previous studies #otherefr; #refr defined a probabilistic model of unification-based grammars as a log-linear model or maximum entropy model"}
{"pre": "[CLS] the translation quality is measured by bleu #refr. [SEP] [SEP] = 4. [SEP] = 4. [SEP] = ? =", "cit": "[CLS] we used a multi-threaded version of the giza++ tool #refr.2 this speeds up the process and corrects an error"}
{"pre": "[CLS] the first is the automatic evaluation of discourse relations #refr. [SEP] of the discourse markers in the discourse structure", "cit": "[CLS] edits to capture a fine-grained picture of changes to wikipedia article pages, we rely on the notion of edits"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to translate the phrases as the", "cit": "[CLS] to obtain part of speech (pos) tags we used an open source stanford pos tagger for english #refr and"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with the one of", "cit": "[CLS] #refr describe a generative model for ccg, which only requires a non-iterative counting process for training, but it is"}
{"pre": "[CLS] the system is based on the c&c parser #refr and the collins parser #otherefr. [SEP] shared task on dependency", "cit": "[CLS] the evaluation results were provided by the organizers using their evaluation script in python #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best f1 score calculation of", "cit": "[CLS] hence, we use a structure learning approach that has been successfully applied to many similar structure finding nlp tasks"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] shared", "cit": "[CLS] an overall notable trend was the use of full dependency parsing #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] remedies have been suggested, typically involving additional search directions and experiment replicates #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the berkeley parser #refr to obtain the syntactic parse trees for each sentence and the english and", "cit": "[CLS] at each step, it chooses one of the three actions #refr to extend a state: 1. shift (s): move"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] for instance, models addressing the acquisition of grammatical constructions and their meaning #refr typically learn from symbolic input. [SEP]"}
{"pre": "[CLS] the first is the automatic metric of text with human annotation of text #refr. [SEP] quality estimation (qe) and", "cit": "[CLS] the md task has close ties to named-entity recognition, which has been the focus of much recent research #otherefr;"}
{"pre": "[CLS] in the literature, discriminative reranking has been widely used in nlp tasks such as pos tagging #otherefr, parsing #refr,", "cit": "[CLS] the sequential classification approach can handle many correlated features, as demonstrated in work on maximum-entropy #otherefr, adaboost #refr, and"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with a beam search", "cit": "[CLS] greedy local search #otherefr has typically been used for decoding in shift-reduce parsers, while beam-search has recently been applied"}
{"pre": "[CLS] the first is the annotation tool which was the annotation tool for the annotation tool #refr. [SEP] project in", "cit": "[CLS] rather than using )~-reduction to simplify meanings, we rely on deduction, as advocated by #refr; 1991\\]. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] the same technique is frequently adopted for the averaged perceptron #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a text with a text that are semantically", "cit": "[CLS] another approach is to apply the pagerank algorithm to determine the most important keywords based on their co-occurrence linkstructure"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for each sentence in the training corpus.", "cit": "[CLS] there is also another related direction of research in parsing of parallel texts, which is targeted on parsing under-resourced"}
{"pre": "[CLS] in the context of machine translation, itg has been explored for statistical word sense disambiguation #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] examples of groundings include pictures #otherefr, or even paraphrases of the same sentence #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the annotation of the annotation tool which has been shown to be useful in various nlp", "cit": "[CLS] in particular, we explicitly represent many semantic relations expressed in a sentence that are not captured by contemporary proposition-directed"}
{"pre": "[CLS] the system is based on the inprotk toolkit for the best system described in #refr. [SEP] evaluation of the", "cit": "[CLS] #refr) for unification-based gra.mmars. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? 2008.", "cit": "[CLS] traditionally, semantic role labeling (srl) systems have focused in searching the fillers of those explicit roles appearing within sentence"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] however, such methods require the existence of either a parallel corpus/machine translation engine for projecting/translating annotations/lexica from a resource-rich"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] model with the average number of the average", "cit": "[CLS] a number of statistical surface realizers have been described, notably the fergus #otherefr, as well as experiments in #refr."}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the sentence in", "cit": "[CLS] therefore, it makes intuitive sense that verb semantics can be useful in determining these relations.2 in #refr, 1the corpus"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr. [SEP]", "cit": "[CLS] although some work has been done on syllabifying orthographic forms #otherefr; #refr, syllables are, technically speaking, phonological entities that"}
{"pre": "[CLS] the most common approach to phrase-based translation is to phrase-based translation #otherefr; #refr. [SEP] s ? the reordering model", "cit": "[CLS] additionally, we will compare two decision rules, the common maximum a-posteriori (map) decision rule and the minimum bayes risk"}
{"pre": "[CLS] the first is the task of sentence compression #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] , s ? ,", "cit": "[CLS] this algorithm and its many variants are widely used in the computational linguistics community #otherefr; #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the grammar is a grammar that is a synchronous grammar (stsg) #refr. [SEP] grammars are capable of a synchronous", "cit": "[CLS] for arbitrary scfgs, this is typically accomplished via the synchronous binarization technique of #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the sentences and the syntactic parse trees of the sentences in", "cit": "[CLS] to train and evaluate our summarizer, we used a corpus of extractive summaries produced at the university of edinburgh"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] shared", "cit": "[CLS] for example, stanford #refr has relatively low performance for simple events but achieves the highest result for process, while"}
{"pre": "[CLS] we use the stanford ner tagger #refr to identify the text in the text and extract the text and", "cit": "[CLS] for instance, some approaches coarsely discriminate between biographical and non-biographical information #otherefr; #refr, while others go beyond binary distinction"}
{"pre": "[CLS] #refr proposed a method for learning semantic inference based on a set of sentences paired with their meaning representation", "cit": "[CLS] we recently showed how such models can be evaluated for their ability to support semantic inference for use in"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for parse trees for parse trees for", "cit": "[CLS] this includes work on phrasestructure parsing #otherefr, dependency parsing #refr as well as a number of other formalisms #otherefr."}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the penn treebank #otherefr. [SEP] score", "cit": "[CLS] it is more similar to the approach described in #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr proposed a method for learning a word sense disambiguation which is a word sense disambiguation which is a", "cit": "[CLS] for example, #refr demonstrated good results for mapping text to underspecified semantics via dependencies without resorting to gold tags."}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a text that are semantically", "cit": "[CLS] the syntactic relations were extracted using the minipar parser #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] the topic was ?evolution?, with sides ?yes, i believe? vs. ?no, i dont believe?. bates #otherefr; #refr. [SEP] [PAD]"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of meaning that are not", "cit": "[CLS] as such a matrix reduction, we utilized a learning method developed by hnc software #refr. 1 [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the feature weights were tuned using mert #refr on the development set. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] in recent work, dual decomposition?a special case of lagrangian relaxation, where the linear constraints enforce agreement between two or"}
{"pre": "[CLS] the first is the process of annotating the word segmentation and then the word segmentation model of a word", "cit": "[CLS] to attempt to overcome this issue, both #refr and hearst #otherefr conflated multiple manual segmentations into one that contained"}
{"pre": "[CLS] the second approach is to use a word lattice as a word alignment model that is based on the", "cit": "[CLS] whereas dependency based semantic spaces have been shown to surpass other word space models for a number of problems"}
{"pre": "[CLS] #refr use a supervised sentiment classifier to detect the polarity of words in a target words in a target", "cit": "[CLS] #refr bootstrap subjectivity lexicons for japanese by generating subjectivity candidates based on word co-occurrence patterns. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the semantic textual similarity of a text of a text of a", "cit": "[CLS] analysis of the semantic correlation between the constituent parts and whole of an mwe is perhaps more commonly discussed"}
{"pre": "[CLS] the second approach is to use a semantic parser to generate the parse tree from the sentence in a", "cit": "[CLS] we opt for a mistake-driven training strategy based on the structured averaged perceptron #refr, which directly employs shortest-path inference"}
{"pre": "[CLS] in recent years, supervised learning methods have been applied to natural language processing #otherefr; #refr. [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] in the repubblica-deppath model, we preserve the paths as part of the features (so that subj-uccidere ?subj-kill? and objuccidere"}
{"pre": "[CLS] in the context of machine translation, itg has been explored for a variety of nlp tasks such as word", "cit": "[CLS] sf is a similarity function (e.g., cosine, the kullback-leibler divergence, the jaccard index) operating on the context vectors #refr."}
{"pre": "[CLS] the most common approach to paraphrase generation is to use a lexicon to automatically extracted from corpora #otherefr; #refr.", "cit": "[CLS] a recent result that the svs model builds on is that selectional preferences can be represented as prototype vectors"}
{"pre": "[CLS] #refr and liang et al. #otherefr proposed joint models for learning a variety of nlp tasks including word segmentation", "cit": "[CLS] we compared the gibbs sampling compressor (gs) against a version of maximum a posteriori em (with dirichlet parameter greater"}
{"pre": "[CLS] the most common approach to word alignment is to use a word alignment model that is to predict the", "cit": "[CLS] some typical tasks are document information retrieval #otherefr; #refr, query expansions #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the longrange reordering problem as a sequence of words", "cit": "[CLS] in particular, knowing a little about the structure of a language can help in developing annotated corpora and tools,"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the input is to be", "cit": "[CLS] the representations used by danlos #otherefr, or #refr are similar, but do not (always) explicitly represent the clause combining"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a text that are semantically similar to the", "cit": "[CLS] we then apply brill?s rule-based tagger #otherefr and basenp noun phrase chunker #refr to extract noun phrases from these"}
{"pre": "[CLS] we use the berkeley parser #refr to obtain the syntactic parse trees for each sentence and the english and", "cit": "[CLS] even though phrase-based machine translation #otherefr; #refr systems have achieved great success, many problems remain for distinct language pairs,"}
{"pre": "[CLS] the model is trained using the giza++ implementation of ibm model 4 alignments #refr. [SEP] model 4 alignments #otherefr", "cit": "[CLS] modern statistical machine translation #otherefr; #refr, typically using a deterministic heuristic to convert these to phrase alignments #otherefr. [SEP]"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] #refr applied this to german using a lattice encoding different segmentations of german words. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of coreference resolution #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] ? the typed-similarity [SEP] ?", "cit": "[CLS] our baseline coreference system implements the standard machine learning approach to coreference resolution #otherefr, #refr, for instance), which consists"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] similar but more complex models were used in the pcfg-sem model of toutanova et al #otherefr and using wordnet"}
{"pre": "[CLS] #refr use a graph to identify and their polarity of words in their polarity in a sentence. [SEP] [SEP]", "cit": "[CLS] ng #otherefr used an expectation-maximization (em) algorithm, and #refr applied markov logic network (mln). [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders and opinion holders in opinion holders in opinion holders in", "cit": "[CLS] we experiment with two classification approaches: #otherefr; #refr 10a set-count feature is a count of the number of instances"}
{"pre": "[CLS] the first is the task of identifying the event descriptions of the bionlp?09 shared task on event extraction #refr.", "cit": "[CLS] paralleling work in summarization, it is hypothesized that the quality of a rewritten story can be defined by the"}
{"pre": "[CLS] the second dataset is the use of a word sense disambiguation task as a sense disambiguation task #refr, which", "cit": "[CLS] this process is thoroughly explained in #refrb). [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the semantic textual similarity of a text of a text of a", "cit": "[CLS] tive properties for the algorithm discussed in section 4.1, we derived our descriptive properties using the output of the"}
{"pre": "[CLS] #refr use a bootstrapping method to identify opinion holders and extract opinion holders and extract opinion expressions from documents", "cit": "[CLS] one outputs lists of sentences #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] model with the em algorithm #refr with the best", "cit": "[CLS] #refr have proposed an efficient method based on bloom filters. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? ? ?", "cit": "[CLS] there has also been work on extracting semantic class members from the web #otherefr; #refr). [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the idea of using cross-lingual lexical similarity #otherefr and the distributional similarity measure proposed by #refr. [SEP] model word", "cit": "[CLS] such crosslingual semantic spaces are typically spanned by: (1) bilingual lexicon entries #refr, or #otherefr; daume. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be a phrase-based system #otherefr;", "cit": "[CLS] a phrase-based translation model is one of the modern approaches which exploits a phrase, a contiguous sequence of words,"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] shared", "cit": "[CLS] in particular, previous research #otherefr; #refr focused on verb-argument structures, i.e., relations between a verb and its syntactic arguments."}
{"pre": "[CLS] the system is based on the inprotk project #refr. [SEP] system, which was developed at the level of the", "cit": "[CLS] for more information on ilex, see knott et al #otherefr and #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the annotation tool which was the annotation tool for the annotation tool #refr. [SEP] project in", "cit": "[CLS] table 3: performance on geoquery. available): scissor #otherefr, an integrated syntactic-semantic parser; krisp #refr, an svm-based parser using string"}
{"pre": "[CLS] #refr use a similar approach to compute similarities between words by training and their translations in the same sentence", "cit": "[CLS] the average accuracy is about 82%, which is consistent with prior work on monolingual sentiment analysis #refr. [SEP] [PAD]"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] dependency parsers have been tested on parsing sentences in english #otherefr; #refr as well as many other languages #otherefra)."}
{"pre": "[CLS] #refr used a supervised method to detect sentiment in the sentiment in the sentiment words in the context of", "cit": "[CLS] several other approaches focus on the subjectivity classification of sentences #otherefr; riloff and #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] we employ two different approaches to inducing sparsity: dirichlet priors over grammar rule probabilities and an approach based on"}
{"pre": "[CLS] the first is the task of identifying the event extraction of event descriptions of event descriptions of coreference in", "cit": "[CLS] to obtain this semantic information, previous work on reference resolution usually leverages a semantic lexicon like wordnet #otherefr; ng"}
{"pre": "[CLS] we use the stanford ner tagger #refr to train the training data with the training data and test data", "cit": "[CLS] these belong to two main categories based on machine learning #refr and language or domain specific rules #otherefr. [SEP]"}
{"pre": "[CLS] the system used in the conll shared task #refr is based on coreference and the genia corpus #otherefr. [SEP]", "cit": "[CLS] corry has only participated in the ?open? setting, as it has already a number of preprocessing modules integrated into"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? 2008. [SEP]", "cit": "[CLS] motivated by this issue, several research groups have reported their experiments on automatic acquisition of causal, temporal and entailment"}
{"pre": "[CLS] we use the stanford named entity recognizer #refr to extract named entities from the stanford named entities from the", "cit": "[CLS] models of coherence have been used to impose an order on sentences for multidocument summarization #otherefr, and to insert"}
{"pre": "[CLS] the most common approach to word sense disambiguation is to use a word similarity measure which is to measure", "cit": "[CLS] two datasets are made publicly available by #refr: the development set, which consists of 162 questions, and the test"}
{"pre": "[CLS] the system is based on the moses toolkit #otherefr and the best system described in #refr. [SEP] test set", "cit": "[CLS] carpuat and wu #otherefr into a chinese- english word-based statistical mt system using the isi rewrite decoder #refr. [SEP]"}
{"pre": "[CLS] the most common approach to this problem is to use a dependency parser #refr and the dependency parser #otherefr.", "cit": "[CLS] the features are the same as those in #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] there are also pre-reordering methods for longdistance reordering in svo-to-sov translations using heuristics designed based on source-side syntactic structures"}
{"pre": "[CLS] the grammar is a grammar that is a grammar that is a grammar that is a grammar that is", "cit": "[CLS] at the same time, these notions are a way of relaxing the primitive adjunetion operation in order to capture"}
{"pre": "[CLS] the second method is based on the word similarity score #refr and the similarity between two words in a", "cit": "[CLS] applications of word clustering include language modeling #otherefr, thesaurus construction #refr and so on. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the second international chinese word segmentation bakeoff #otherefr; #refr and the same task as the best results are", "cit": "[CLS] unsupervised dependency parsing has seen rapid progress recently, with error reductions on english #otherefr of about 15% in six"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] event and semantic role", "cit": "[CLS] for example, the tempeval challenge series #otherefr; #refr provided a number of tasks that have resulted in several temporal"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] it differs from the many approaches where #otherefr; #refr and from transfer-based systems defined by context-free grammars #otherefr. [SEP]"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of meaning that are not", "cit": "[CLS] the evaluation is based on the word disambiguation task developed by #refr for intransitive sentences, and on a similar"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] there are also some interesting approaches to dynamic domain adaptation mainly using flexible mixture models or techniques for the"}
{"pre": "[CLS] we use the stanford chinese word segmenter #refr to tokenize the english side of the parallel corpus #otherefr. [SEP]", "cit": "[CLS] a k-best list with higher diversity can better represent the entire decoding space, and thus tuning on such a"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for parsing #otherefr; #refr for parsing", "cit": "[CLS] the other is a graph-based approach that searches for a global optimum (highest scoring tree) from a complete graph"}
{"pre": "[CLS] the similarity between two words was computed using the wordnet similarity measure proposed by #refr. [SEP] bleu score #refr", "cit": "[CLS] they are also used for modeling the meaning of a phrase or a sentence #refr; wartena, 2013; mitchell, 2011;"}
{"pre": "[CLS] in the literature, supervised learning methods have been applied to part-of-speech tagging and unsupervised learning of part-of-speech tagging #otherefr;", "cit": "[CLS] #refr reported positive results with little labeled training data but negative results when the amount of labeled training data"}
{"pre": "[CLS] the most popular approach to translation lexicons from parallel corpora #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] several authors considered as well the translation of new words by relying on distributional collocational properties computed from a"}
{"pre": "[CLS] in the literature, supervised learning methods have been applied to part-of-speech tagging and unsupervised learning for part-of-speech tagging #otherefr;", "cit": "[CLS] thus, recent studies on pos tagging focus on semi-supervised #otherefr or unsupervised approaches #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained with the averaged perceptron algorithm #refr. [SEP] model with the entire sequence of the entire", "cit": "[CLS] one key problem with typical models of phrase alignment is that they choose a single (latent) segmentation, giving rise"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to translate the words and the", "cit": "[CLS] furthermore the new features on the edges are introduced by formula (6), which is then tuned on the development"}
{"pre": "[CLS] we use the stanford corenlp suite to lemmatize and partof-speech tag each word #otherefr; #refr, and parse the input", "cit": "[CLS] also before running our supervised classifier, we run a named entity recognizer #refr and assign the label ?c np"}
{"pre": "[CLS] we use the stanford ner tagger #refr to extract named entities from the target domain and find the text", "cit": "[CLS] more recently, kernel functions, which implicitly represent data in some high dimensional space, have been employed to study and"}
{"pre": "[CLS] the first is the task of identifying the semantic relations between entities #otherefr; #refr. [SEP] of a single document", "cit": "[CLS] along this line, recent work by #refr and liang et al #otherefr relax supervision to require only annotated answers"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] another alternative, which we do not explore in this work, is to use cascaded translation using a pivot language"}
{"pre": "[CLS] the second approach is to use a word lattice to improve the performance of a state-of-the-art wsd system #refr.", "cit": "[CLS] riv stands for the recall rate of the iv words, and nc stands for nchanges (insertion+deletion+substitution) #refr. [SEP] [PAD]"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a task of semantic relations between words and phrases and the", "cit": "[CLS] it was parsed using minipar #refr, which is considerably faster than the collins parser but failed to parse about"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] we show that enhancing the parser with external lexical information is greatly beneficial, both in an artificial scenario where"}
{"pre": "[CLS] the most popular approach to translation lexicons from parallel corpora #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ?", "cit": "[CLS] in this paper, a pagerank-style random walk algorithm #otherefr; #refr is conducted to iteratively compute the importance score of"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with the one of", "cit": "[CLS] one can either select a segmentation path prior to parsing, or, as has been recently argued, one can let"}
{"pre": "[CLS] the first is the task of converting natural language processing #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] to be", "cit": "[CLS] #refr acquired subtrees derived from dependency trees as extraction rules for ie in general domains. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] there are also alternatives to mst which allow imposing additional constraints on the dependency structure, e.g. that at most"}
{"pre": "[CLS] the translation model is a log-linear model #refr. [SEP] model with the decoder that we use the decoder that", "cit": "[CLS] synchronization of tree adjoining grammars (tags) #refr are even more powerful than the previous formalisms, and have been applied"}
{"pre": "[CLS] the system is based on the moses toolkit #otherefr and the moses toolkit #refr. [SEP] system, pharaoh decoder with", "cit": "[CLS] one uses confusion network decoding to combine translation systems as described in #otherefr and #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to identify the topics of discourse relations in terms of discourse relations in a", "cit": "[CLS] examples include genre classification #otherefr, sentiment analysis #refr, readability classification #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; yu and", "cit": "[CLS] #refr worked on a 2829 sentence citation corpus using a 12-class classification scheme. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for each sentence in the sentence with", "cit": "[CLS] we explored a range of data settings, but there are many others where tree kernels have been proven useful,"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] shared", "cit": "[CLS] from these articles, we have then calculated a semantic space using latent semantic analysis (lsa) as implemented by the"}
{"pre": "[CLS] the task of identifying the semantic textual similarity of a text is a text of a text #otherefr; #refr.", "cit": "[CLS] among the machine learning algorithms studied, rule based systems have proven effective on many natural language processing tasks, including"}
{"pre": "[CLS] we use the minimum error rate training #refr to tune the feature weights for the feature weights of the", "cit": "[CLS] log-linear weights are estimated with minimum error rate training #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] automatic acquisition of sentiment expressions have also been studied #refr, but limited to adjectives, and only one sentiment could"}
{"pre": "[CLS] the system is trained on the penn treebank #refr and the wsj treebank #otherefr. [SEP] score for the same", "cit": "[CLS] 2.1.1 mst training #refr present a technique for training discriminative models for dependency parsing. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the system is trained using the moses toolkit #refr. [SEP] training data for the translation quality measured by bleu", "cit": "[CLS] for english, we also took advantage of our inhouse text processing tools for the tokenization and detokenization steps (d#refr"}
{"pre": "[CLS] the system used in this paper is based on the c&c parser #otherefr shared task #refr. [SEP] [SEP] [SEP]", "cit": "[CLS] this architecture is similar to the one presented by the same authors in the past edition, with the extension"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a text that are semantically", "cit": "[CLS] for example, in summarization, #refr and lin and hovy #otherefr focus on multiword noun phrases. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for parsing #otherefr; #refr for parsing", "cit": "[CLS] we use the tnt tagger #refr in our experiments, because of its efficiency and ease of use. [SEP] [PAD]"}
{"pre": "[CLS] #refr used a supervised sentiment classification approach to classify reviews as well as features for sentiment classification and classify", "cit": "[CLS] recently, several methods have been proposed to cope with the problem #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained with the averaged perceptron algorithm #refr. [SEP] score = ? ? ? ? ? ?", "cit": "[CLS] we present a unified view of two state-of-theart non-projective dependency parsers, both approximate: the loopy belief propagation parser of"}
{"pre": "[CLS] we use the averaged perceptron algorithm #refr to train the parameters of the model for the model parameters. [SEP]", "cit": "[CLS] generating a simplified version of a text traditionally has many applications in question answering #otherefr and speech synthesis #refr."}
{"pre": "[CLS] the first is the one of the most popular approach to use a word in which is to use", "cit": "[CLS] recently, promising computat ional methods have been suggested \\[lesk, 1987; mcdonald et al., 1990; wilks et al, 1990; #refr;"}
{"pre": "[CLS] the system is based on the inprotk toolkit for dialogue systems that participated in the system described in #refr.", "cit": "[CLS] while our approach is related to belief tracking, we focus here on spoken language understanding under uncertainty rather than"}
{"pre": "[CLS] the work of #refr is similar to ours in that the context of discourse relations are used in the", "cit": "[CLS] recent studies have shown that surprisal can successfully account for a range of psycholinguistic effects #otherefr; #refr. : what"}
{"pre": "[CLS] the grammar is a grammar that is a synchronous grammar (stsg) grammar (stsg) grammar (stsg) for synchronous grammar formalisms", "cit": "[CLS] efficient recognition and parsing algorithms for mcfg have been described in nakanishi et al #otherefr and #refr. [SEP] [PAD]"}
{"pre": "[CLS] the first is the process of annotating the predicate-argument structure in text with se- ?this research was partially ordered", "cit": "[CLS] in addition, #refr found that a vanilla space of this sort performed best in their composition experiments, when compared"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the sentence in", "cit": "[CLS] for example, syntactic features like dominance sets #refr are extremely useful for sentence-level parsing, but are not even applicable"}
{"pre": "[CLS] the first is the task of coreference resolution #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] ? ? ? ?", "cit": "[CLS] #refr have proposed an unsupervised approach which also incorporates cluster information into consideration. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to identify the polarity of words in a sentence. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] #refr gave a streaming variant of the earlier perfect hashing language model of talbot and brants #otherefr, which operated"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] the tempeval shared tasks #refr have been one of the key venues for researchers to compare methods for temporal"}
{"pre": "[CLS] the first is the task of coreference resolution #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? the typed-similarity", "cit": "[CLS] one of the earliest kinds of semantic knowledge employed for coreference resolution is perhaps selectional preference #otherefr; #refr: given"}
{"pre": "[CLS] the parser is trained on the penn treebank corpus and tested on the penn treebank corpus #refr. [SEP] test", "cit": "[CLS] that analysis, rule filtering #refr, reduces parse times by filtering out mother-daughter unifications that can be determined to fail"}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders and predict the polarity of words in a sentence and", "cit": "[CLS] #refr used a random walk model built on top of a word relatedness network to predict the semantic orientation"}
{"pre": "[CLS] the most common approach to this task is to use a lexicon to identify the one proposed by #refr.", "cit": "[CLS] clusters of noun phrases #otherefr; #refr and are labeled by applying ?such-as? surface patterns to raw web text #otherefr."}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders and predict the polarity of words in a sentence. [SEP]", "cit": "[CLS] previous research on sentiment prediction for online discussions, however, focuses on turn-level predictions #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the automatic acquisition of subcategorization frames from the penn treebank #otherefr; #refr. [SEP]", "cit": "[CLS] to handle such situations, it is useflfl to view the l)ti) is as open-ended objecls 1o which new semantic"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] future works aim at transforming such non-projectable trees into projectable form #refr, driven by translation rules from aligned data#otherefr"}
{"pre": "[CLS] the first is the case for the automatic acquisition of subcategorization frames from the web #otherefr; #refr. [SEP] test", "cit": "[CLS] in the case that the object is pronominal, however, the vpc must occur in split word order #otherefr; #refr."}
{"pre": "[CLS] #refr use a similar approach to compute sentence importance. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] some researchers #refr have developed simple statistical models for aligning documents and headlines. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to paraphrase generation has been the source sentences into either by using a monolingual parallel", "cit": "[CLS] due to the small size of our crowdsourced corpus, we will use it in the mert tuning #refr, and"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] other work has used regression to train models for adjectives in adjective-noun phrases #otherefr; #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] shared", "cit": "[CLS] among the tasks for the new series is the infection disease task, proposed and investigated by #otherefr; #refr. [SEP]"}
{"pre": "[CLS] the system is based on the inprotk toolkit for the best system described in #refr. [SEP] system, which is", "cit": "[CLS] we provide an extensive evaluation methodology to assess such efficiency by improving on our previous work in this area"}
{"pre": "[CLS] in the context of discourse relations are used to identify the discourse relations and discourse relations and discourse relations", "cit": "[CLS] previous research has addressed revision in single-document summaries [jing & mckeown, 2000] [#refr] and has suggested that revising summaries"}
{"pre": "[CLS] the first is the task of identifying the event extraction of event descriptions of event descriptions of event extraction", "cit": "[CLS] clearly, promoting competitive evaluation of pronoun resolution scoring systems by giving competitors equivalent real-world preprocessing output along the lines"}
{"pre": "[CLS] the task of identifying the event descriptions of event extraction and event extraction of event descriptions has been well", "cit": "[CLS] following lin #otherefr, #refr, and pantel et al #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? 2008. [SEP] ?", "cit": "[CLS] these three of types of information have proved useful for natural language processing #otherefr; #refr; o. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] in recent years, many methods have been proposed to solve the problem of learning problem #otherefr; #refr. [SEP] grammars", "cit": "[CLS] on the contrary, bilingual projection #otherefr; #refr seems a promising substitute for languages with a large amount of bilingual"}
{"pre": "[CLS] the task of paraphrase extraction is similar to the one described by #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] the lexical substitution #refra) can be regarded as a subtask of the lexical entailment, in which for a given"}
{"pre": "[CLS] the most popular approach to smt is the one proposed by #refr, who use a large set of features", "cit": "[CLS] we perform bootstrap resampling with bounds estimation as described by #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr. [SEP]", "cit": "[CLS] a number of researchers have investigated a semisupervised pos induction task in which a tag dictionary or similar data"}
{"pre": "[CLS] the system is based on the moses toolkit #otherefr and the moses toolkit #refr. [SEP] decoder with the best", "cit": "[CLS] as chiang #otherefr and #refr note, purely lexical ?phrase-based? translation models suffer from sparse data effects when translating conceptual"}
{"pre": "[CLS] the grammar we use is a standard grammar development set for the grammar development and test and test and", "cit": "[CLS] #refr and graehl et al #otherefra) show that the additional expressive power of treesequences helps the translation process. [SEP]"}
{"pre": "[CLS] the most common approach to machine translation is to use a log-linear combination of statistical machine translation #otherefr; #refr.", "cit": "[CLS] semantic roles as features in aggregate metrics gime?nez and ma`#refr introduced ulc, an automatic mt evaluation metric that aggregates"}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders and predict the polarity of words in a sentence and", "cit": "[CLS] with respect to sentiment, related approaches analyze discourse relations #otherefr, identify the different aspects mentioned in a text #refr,"}
{"pre": "[CLS] we use the stanford ner tagger #refr to train the training data with the training data and test data", "cit": "[CLS] examples include question answering #otherefrb; #refr, dialog systems #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] algorithm with respect to search algorithm with the entire", "cit": "[CLS] both #refr and chiang #otherefr use scfgs as the underlying model, so their translation schemata are syntax-directed as in"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] in both cases, we use an in-house implementation of english pos tagger #otherefr and a japanese morphological analyzer #refr"}
{"pre": "[CLS] the first is the case for the automatic evaluation of the automatic evaluation of translation quality measured by bleu", "cit": "[CLS] the increasing availability of mt engines and the need for better quality has motivated considerable efforts to combine multiple"}
{"pre": "[CLS] the feature weights were tuned using mert #refr on the development set. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] in #refr the downhill simplex method is used to estimate the weights; around 200 iterations are required for convergence"}
{"pre": "[CLS] #refr use a similar approach to identify the polarity of words in a sentence. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] wiebe et al #otherefr, #refr), automatic analysis of them largely remains as a big challenge. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the polarity of a sentence, a document #otherefr; #refr. [SEP] of a", "cit": "[CLS] the most popular formalism today is markov logic, which has already been used for natural language processing tasks such"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] finding an appropriate mechanism to inform phrasebased translation models and their hierarchical variants with linguistic syntax is a difficult"}
{"pre": "[CLS] the first is the automatic evaluation of discourse structure of discourse relations #refr. [SEP] discourse markers are the discourse", "cit": "[CLS] since it correlates with word order and pitch accent #otherefr; #refr, for instance, incorporating knowledge on information status would"}
{"pre": "[CLS] the task of identifying the event descriptions of event extraction and event extraction of event descriptions has been well", "cit": "[CLS] since the message understanding conferences in the 1990s #refr, information extraction (ie) and named entity recognition (ner) approaches have"}
{"pre": "[CLS] the first is the automatic metric of text with human annotation of text segments as shown in #refr. [SEP]", "cit": "[CLS] those two measures have been previously shown to give promising performance for the task of estimating the frequencies of"}
{"pre": "[CLS] #refr use a similar approach to compute similarities between words in a sentence. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] in early pioneering work, #refr use a flat feature vector (e.g., a bag-of-words) to represent the documents. [SEP] [PAD]"}
{"pre": "[CLS] the most popular approach to word alignment is the ibm models #otherefr; #refr. [SEP] model 1 and the hmm", "cit": "[CLS] the performance of pb-smt system is measured with bleu score #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the one of the most popular method for building word sense disambiguation in the wsd task", "cit": "[CLS] we also produced best and oot baselines using the distributional similarity measures l1, jaccard, cosine, lin #otherefr and ?sd"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of words in the same", "cit": "[CLS] first, in parameter estimation, we find values of the parameters that explain a labeled corpus, such as semcor #refr."}
{"pre": "[CLS] the most common approach to machine translation is to use a log-linear combination of the feature weights for the", "cit": "[CLS] english: nlparser6 (92%) #refr . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] shared", "cit": "[CLS] lexical polarity values are largely derived from a polarity lexicon #refr and extended by us- trigger pos semantic type"}
{"pre": "[CLS] the system is based on the inprotk project at the xerox implementation of the xerox implementation of the xerox", "cit": "[CLS] the key functions of ete are described below: manage test resources: ete provides an graphical interface to manage various"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within the source sentence into a target sentence into", "cit": "[CLS] dependency parsing is accomplished with mica, a tag-based parser #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking approaches #otherefr; #refr to learning for parsing", "cit": "[CLS] we used the limited memory variable metric optimization algorithm from the petsc/tao optimization toolkit #otherefr to find the optimal"}
{"pre": "[CLS] #refr use a similar approach to identify stances in which a coherent discourse. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] this approach has several advantages: markers are frequently found across genres #refr, they exist in many languages #otherefr. [SEP]"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] if the orthography of a language is strongly phonetic, as is the case for korean, then one may use"}
{"pre": "[CLS] #refr use a similar approach to identify the text in which a text in which a sentence and then", "cit": "[CLS] some research has already established a connection between neural and distributional semantic vector spaces #otherefr; #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with a probability distribution", "cit": "[CLS] while this technique has proven quite effective in parsing #otherefr; #refr as well as machine translation #otherefr, we show"}
{"pre": "[CLS] in the second international chinese word segmentation bakeoff #otherefr; #refr and the best results are achieved by the best", "cit": "[CLS] more recently, em has been used to learn hidden variables in parse trees; these can be head-child annotations #otherefr,"}
{"pre": "[CLS] the wsd system used for wsd in the experiments was based on the senseval-3 english lexical sample task #refr.", "cit": "[CLS] moreover, those relations have been shown to be very effective knowledge sources for wsd #otherefr and interpretation f noun"}
{"pre": "[CLS] the first is the case for the generation of referring expressions that we will use the same type of", "cit": "[CLS] each of these functions has been treated separately in various systems (see, e.g., \\[#refr\\] for contrastive focus in a"}
{"pre": "[CLS] the similarity between two sentences is computed using the wordnet #otherefr and the wordnet similarity measure proposed by #refr.", "cit": "[CLS] the best results were obtained by the vector metric #refr, which exploits the lexical information that is included in"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal (wsj) corpus #refr. [SEP] trees and the penn", "cit": "[CLS] 1992\\] adapted to the atis domain for a speech-translation task \\[#refr\\] and large corpora of real user data collected"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] one of the most popular instantiations of loglinear models in smt are phrase-based #otherefr; #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] for example, #refr incorporated bilexical preferences as features via self-training to improve the alpino parser for dutch. [SEP] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the semantic textual similarity of a sentence #refr. [SEP] task is a", "cit": "[CLS] the idea of a self-customizing ie system emerged recently with the improvement of pattern acquisition techniques #refrb), where the"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for parsing #refr. [SEP] grammars with", "cit": "[CLS] beam search incremental parsers #otherefr; #refr provide very competitive parsing accuracies for various grammar formalisms (cfg, ccg, and dependency"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr.", "cit": "[CLS] examples of meaning representations considered in prior research include logical forms based on database query #refr, semantic frames #otherefr."}
{"pre": "[CLS] the task of segmenting utterances into words, for example, johnson and colleagues have been shown to be useful in", "cit": "[CLS] recently, relative research has received considerable interest in the biomedical nlp community, including detecting hedges and their in-sentence scope"}
{"pre": "[CLS] the first is the one of the most popular approach to use a word in which is to use", "cit": "[CLS] however, most of these methods address only the sub-problem of alignment #otherefr, #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] this notion can be applied to any projective tree structure, but we use dependency trees, which have been shown"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the elements is to use", "cit": "[CLS] headline generators are noisy-channel probabilistic systems that are trained on large corpora of headline, text pairs #otherefr; #refr. [SEP]"}
{"pre": "[CLS] #refr use a similar approach to compute similarities between words by extracting phrases and their polarity and their polarity", "cit": "[CLS] the most common approaches #otherefr; #refr, or to cluster the aspect terms using #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to identify the text in which a text in which a sentence and then", "cit": "[CLS] although we are not bound to a specific composition model, throughout this paper we use the method proposed by"}
{"pre": "[CLS] the task of identifying the semantic textual similarity of a document is a document level #refr. [SEP] of a", "cit": "[CLS] we use the hierarchical directed acyclic graph (hdag) kernel #refr, which is suited to handle structured natural language data."}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be np-hard #refr. [SEP] s", "cit": "[CLS] these methods also try to leverage syntax, typically by applying hand-coded or automatically induced reordering rules to a constituency"}
{"pre": "[CLS] the task of identifying the overall event descriptions of a single document is a text #refr. [SEP] set of", "cit": "[CLS] commonly used ie techniques follow two main assumptions: #otherefr; (2) extraction processes are bootstrapped with some pre-existing knowledge of"}
{"pre": "[CLS] we use the stanford ner tagger #refr to train the models for each word and the training and the", "cit": "[CLS] sentence boundary detection is a problem that has received limited attention in the text-based computational linguistics community #otherefr; #refr,"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] also, syntactic features such as the dependency relationship of words and subtrees have been shown to effectively improve the"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] about 70% of web queries are nps #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to identify the relations in which words of words in a window of words", "cit": "[CLS] the sspace package of #refr also overlaps to some degree with dissect in terms of its intended use, however,"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for each sentence in the training corpus.", "cit": "[CLS] #refr use similar subtree constraints to improve parser accuracy in a dependency scenario. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the bionlp 2011 shared task #refr focuses on event extraction and event extraction and event extraction and event extraction", "cit": "[CLS] some event extraction data sets only include documents that describe relevant events #otherefr; gu and #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of possible target word in", "cit": "[CLS] at the other extreme, #refr developed a customized, explicit wsd algorithm as part of their decision tree system. [SEP]"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] the systems that we compared with are: the syn0, syn20 and goldsyn systems by ge and mooney #otherefr, the"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] mwer can be seen as a sequence labelling task (like chunking) by using an iob-like annotation scheme #refr. [SEP]"}
{"pre": "[CLS] the second dataset is the use of a word sense disambiguation algorithm which is based on the most widely", "cit": "[CLS] in senseval-2, the best performing system #otherefr in the english all-words task achieved an accuracy of 69.0%, while in"}
{"pre": "[CLS] the first is the one of the most popular approach to use crowdsourcing, which has been to use a", "cit": "[CLS] unfortuantely, the approach used to build catvar cannot be adopted: it builds on a collection of high-quality lexical-semantic resources"}
{"pre": "[CLS] the most popular approach to translation lexicons from parallel corpora #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] the vast majority of prior work on low resource mt has focused on spanish-english #otherefr; #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] we use the minimum error rate training #refr to tune the feature weights for the feature weights of the", "cit": "[CLS] recent work have shown that smt benefits a lot from exploiting large amount of features #otherefr; #refr. [SEP] [PAD]"}
{"pre": "[CLS] the task of identifying the semantic relations between nominals of relations and relations and relations between nominals #otherefr #refr.", "cit": "[CLS] instead, expressions populate a continuum between two extremes: idioms and free word combinations #otherefr; #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to learn a translation models from a", "cit": "[CLS] as an alternative option to our verb-modifier experiments, structured language models #otherefr, #refr, chiang #otherefr among many others) are"}
{"pre": "[CLS] the most popular approach to translation lexicons from parallel corpora #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] to translate across languages with different alphabets approaches such as #otherefr; #refr use transliteration techniques to tackle proper nouns"}
{"pre": "[CLS] the most popular approach to word alignment is the ibm models #otherefr; #refr. [SEP] model 1 and the hmm", "cit": "[CLS] besides, #refr and zhou et al #otherefr proposed the phrase-based translation models for question and answer retrieval. [SEP] [PAD]"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] treating the new lm as an additional feature function has the advantage that its weight can be directly optimized"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the words is to the", "cit": "[CLS] many methods exist for clustering, e.g., #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the mentions of a text with a text at the sentence level", "cit": "[CLS] ceaf #refr scores are obtained by computing the best one-to-one mapping between the system/true partitions, which is equivalent to"}
{"pre": "[CLS] in the literature, slight variations of this problem are also referred to as unsupervised learning #otherefr, and semi-supervised learning", "cit": "[CLS] machine learning-based approaches are taken in #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of semantic similarity to be", "cit": "[CLS] in #otherefr and anaphora resolution #refr. 1(1) was actually uttered by a flight attendant on a plane. [SEP] [PAD]"}
{"pre": "[CLS] the second approach is to use a word segmentation and pos tagger to tag dictionary to tag each word", "cit": "[CLS] our word segmenter uses a maximum entropy framework #otherefr; #refr and is trained on manually segmented sentences. [SEP] [PAD]"}
{"pre": "[CLS] in the context of machine translation, itg has been explored for mt #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] at the same time, the task has seen applications such as machine translation #otherefr; #refr, mt evaluation #otherefr. [SEP]"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] breck et al #otherefr introduced a sequence model to extract opinions and we took this one step further by"}
{"pre": "[CLS] the task of event extraction has received a lot of attention in recent years #otherefr; #refr. [SEP] [SEP] [SEP]", "cit": "[CLS] #refr formulated global inference using a bayesian network, where they captured the influence between a relation and a pair"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the sentence in", "cit": "[CLS] recently, methods for learning probabilistic semantic parsers have been shown to address such limitations #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for each sentence with dependency trees for", "cit": "[CLS] to better utilize the large training set, we propose to generalize from phrase-based mt to syntax-based mt, in particular"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] in order to study the behavior of both role sets in out?of?domain data, we made use of the prop-"}
{"pre": "[CLS] the data set contains 14,619 items and is available from the same corpus #refr. [SEP] test set of about", "cit": "[CLS] an exception to this trend is the work of #refr who reported non-significant correlation for the mean number of"}
{"pre": "[CLS] the system uses a combination of the maximum entropy model of #refr to generate a set of features from", "cit": "[CLS] one of the lessons learned from our previous experience at senseval-31 #refr is that the integration of different systems"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] 4http://trec.nist.gov/ 5we split the japanese articles into sentences by using simple heuristics and split the english articles into sentences"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] a representative set of similar systems includes: a set of hand-crafted reordering patterns for german-to-english #refr and chinese-english #otherefr."}
{"pre": "[CLS] the pos tagger was trained on the penn treebank #refr trained on the wall street journal section of the", "cit": "[CLS] megyesi #refr and kuba et al #otherefr produced results with tbl taggers that are given in table 1, in"}
{"pre": "[CLS] #refr proposed a method for learning a word clustering based on a probabilistic model that learns vector space representations", "cit": "[CLS] however, significant advances in event extraction have been achieved in the last decade as the result of standardization efforts"}
{"pre": "[CLS] the first is the case for the parsing of the english lexical substitution task #refr. [SEP] grammars in the", "cit": "[CLS] for example, statistical parsers from #refr on use features based on head-dependent relationships. #otherefr). [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the second international chinese word segmentation bakeoff #otherefr; #refr and the same task as the best results are", "cit": "[CLS] some previous multilingual research, such as bayesian parameter tying across languages #otherefr or models of parameter 1although the stated"}
{"pre": "[CLS] the second approach is to use a word sense disambiguation algorithm to find the most probable word sense disambiguation", "cit": "[CLS] to date, a thorough study of the domain dependence of wsd - - in the style of other studies"}
{"pre": "[CLS] in recent years, there has been a growing interest in the task of identifying the problem of identifying paraphrases", "cit": "[CLS] we employ integer linear programming #otherefr, #refr). [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr proposed a method for sentiment classification that is a given sentence in the sentiment classification of movie reviews.", "cit": "[CLS] some researchers also proposed to use topic modeling to identify implicit topics and sentiment words #otherefr; #refr; li et"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is not to be used in the context", "cit": "[CLS] one is to find unknown words from corpora and put them into a dictionary (e.g., #refr), and the other"}
{"pre": "[CLS] in the context of machine translation, itg has been explored for statistical word alignment in both unsupervised #otherefr; #refr", "cit": "[CLS] finally, crowd-sourcing techniques to obtain translations have been previously studied and applied to build datasets for casual domains #refr."}
{"pre": "[CLS] the first is the automatic evaluation of the automatic metric of the automatic evaluation of the automatic evaluation of", "cit": "[CLS] there have been a number of approaches proposed for the use of multilingual resources for mwe identification #refr. [SEP]"}
{"pre": "[CLS] the first is the automatic evaluation of the automatic metric of the automatic evaluation of the automatic evaluation of", "cit": "[CLS] for these reasons, mt has recently been used in a number of language experiments (e.g., #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford chinese word segmenter #refr to tokenize the english side of the parallel corpus #otherefr. [SEP]", "cit": "[CLS] various online em algorithms have been investigated (see #refr for an overview) but our focus is on the stepwise"}
{"pre": "[CLS] the most popular approach to word alignment is the hmm model #refr. [SEP] model is based on the one", "cit": "[CLS] more recent approaches to compression introduce reordering and paraphrase operations #otherefr while there are over 50 stanford dependencies (de"}
{"pre": "[CLS] the first is the task of identifying the semantic textual similarity of a sentence #refr. [SEP] of a single", "cit": "[CLS] among these types of unknown words, #refr pointed out that compound words constitute the most productive type of unknown"}
{"pre": "[CLS] the first is the automatic evaluation of the automatic metric of the automatic evaluation of the automatic evaluation of", "cit": "[CLS] as part of this step, sentences were preprocessed: tokenized #otherefr and then parsed with collins? statistical parser #refr. [SEP]"}
{"pre": "[CLS] the generation of referring expressions is a subject of research in natural language generation (nlg) in natural language generation", "cit": "[CLS] the task of natural anguage generation #otherefr; #refr, and surface generation, in which the high-level propositions are converted into"}
{"pre": "[CLS] the system is based on the inprotk toolkit for the best performing system described in #refr, a system that", "cit": "[CLS] in generation, a lexical entry that lists a word stem and a corresponding set of linguistic and semantic features"}
{"pre": "[CLS] the first is the automatic evaluation of discourse structure of discourse structure of discourse structure of discourse relations #refr.", "cit": "[CLS] previously #refr performed 17the number of sentences in this table are a subset of the ones in the table"}
{"pre": "[CLS] the task of identifying the overall sentiment in text has been the focus of the focus of the focus", "cit": "[CLS] #refr cast the problem as a sequence labeling task and show that performance is highly domain dependent and requires"}
{"pre": "[CLS] in the context of machine translation, itg has been explored for statistical word sense disambiguation #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] most of the applications of comparable corpora focus on discovering translation equivalents to support machine translation, such as bilingual"}
{"pre": "[CLS] the first is the task of identifying the word sense disambiguation of word sense disambiguation of word sense disambiguation", "cit": "[CLS] \\[li and abe, 1996\\] compared clustering methods proposed so far \\[#refr\\] \\[brown et ?1., 1992\\ ] \\[pereira et al,"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? ? ? ?", "cit": "[CLS] more specifically, an entailment rule is defined #refr as a directional relation between two sides of a pattern, corresponding"}
{"pre": "[CLS] the first is the task of identifying the event extraction of event descriptions of event descriptions of event descriptions", "cit": "[CLS] the results show that this extra information has a significant (p<0.01, using a randomization test #refr) impact on test"}
{"pre": "[CLS] #refr proposed a method for automatically learning entailment rules from the web by using a bilingual corpora. [SEP] [SEP]", "cit": "[CLS] the model cleanly incorporates both syntax and lexical semantics using quasi-synchronous dependency grammars #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford named entity recognizer #refr to identify named entities and extract named entities from the text", "cit": "[CLS] rao et al#otherefr, #refr). [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] the rules for tree transformation can be manually written #refr or automatically learned #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to semi-supervised learning algorithms use a small set of seed examples #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] the kernelbased classification is, however, known to be very slow in nlp tasks, so efficient classifiers should sum up"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] for system combination, we followed a minimum bayes-risk algorithm, as introduced in #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the system is based on the inprotk project #refr. [SEP] system, which was developed at the level of the", "cit": "[CLS] in contrast to the results of #refr, who found users preferred speech even when less efficient, our subjects generally"}
{"pre": "[CLS] the first is the case for the generation of referring expressions from the linguistic phenomena of referring expressions and", "cit": "[CLS] these are the 'quasi logical forms' #otherefr, differing from aresolved logical forms' (rlf) in several respects: they contain 'a_terms'"}
{"pre": "[CLS] #refr use a similar approach to compute sentence importance. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] a number of token-based approaches have been proposed: supervised #refr, weakly supervised #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to paraphrase generation is to use a lexicon to paraphrase database into a target language", "cit": "[CLS] commonly used (candidate) comparable corpora are news articles written by different news agencies within a limited time window #refr."}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] 4significance testing was carried out using the bootstrap resampling technique advocated by #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford named entity recognizer #refr to identify named entities and extract named entities from the polarity", "cit": "[CLS] #refr and anand et al#otherefr were interested in ideological content in debates, relying on discourse structure and leveraging sentiment"}
{"pre": "[CLS] the pos tagging task is to pos tagging and tagging and tagging and tagging in the first order of", "cit": "[CLS] khoja #otherefr first introduced a tagger for arabic, which has 131 tags, but subsequent work has collapsed the tagset"}
{"pre": "[CLS] the first is the task of identifying the semantic relations between nominals of noun phrases and the verb classes", "cit": "[CLS] it has been thoroughly studied in both the linguistics literature #refr and more recently within the field of computational"}
{"pre": "[CLS] the first is the process of annotating the word segmentation and then the word segmentation model of a word", "cit": "[CLS] because of its value to natural language processing, various text segmentation tasks have been automated such as topical segmentation?"}
{"pre": "[CLS] the model is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the em algorithm #refr with the", "cit": "[CLS] in order to speed up our sampling procedure, we used a parallel sampling strategy similar to that of blunsom"}
{"pre": "[CLS] the first is the case for the automatic evaluation of translation quality measured by bleu #refr, which is the", "cit": "[CLS] callison-burch et al #otherefr improved english?spanish and english?french smt using source-language paraphrases extracted with the pivoting technique of #refr"}
{"pre": "[CLS] the most popular approach to paraphrase generation has been the source sentences into either by using a monolingual target", "cit": "[CLS] a set of 100 test phrases were selected and for each test phrase, five distinct sentences were randomly sampled"}
{"pre": "[CLS] the first is the task of identifying the polarity of a text that is a text of a text", "cit": "[CLS] however, it would be interesting to see if parsing is necessary or if we can get equivalent or nearly-equivalent"}
{"pre": "[CLS] the most common approach to paraphrase generation is to paraphrase generation from corpora #otherefr; #refr. [SEP] words to a", "cit": "[CLS] several approaches rely on bilingual parallel data #otherefr; #refr, while others leverage distributional methods on monolingual text corpora #otherefr."}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a similar to the one used in the same textual entailment", "cit": "[CLS] the obtained scfs comprise the total 163 scf types which are originally based on the scfs in the anlt"}
{"pre": "[CLS] the first is the task of identifying the semantic relations between nominals of noun phrases and semantic relations #otherefr;", "cit": "[CLS] following the method for discovering patterns automatically #otherefr, #refr apply the same method to extract hypernyms of entities in"}
{"pre": "[CLS] the task of identifying the semantic relations between nominals of noun phrases and their arguments are also key to", "cit": "[CLS] a similar approach has been proposed in #refr for pos disambiguation. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] with different training corpus sizes, they focus on translation into english from arabic #otherefr; #refr, czech #otherefr. [SEP] [PAD]"}
{"pre": "[CLS] the task of semantic textual similarity (sts) task #refr is a similar to the system that uses a semantic", "cit": "[CLS] and #refr present a model for compositionality based on recursive neural networks. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking approaches #otherefr; #refr to learning for dependency", "cit": "[CLS] #refr use a large unlabeled corpus to estimate cluster features which help the parser generalize with fewer examples. [SEP]"}
{"pre": "[CLS] in this paper, we present an unsupervised learning approach for semantic parsing that has been applied successfully in the", "cit": "[CLS] another set of approaches have investigated the case where no logical forms are provided, but instead some form of"}
{"pre": "[CLS] in the context of machine translation, there is a large body of research in determining the polarity of words", "cit": "[CLS] for instance, galley et al. #otherefr use the list of subjective adjectives compiled by #refr to assign a positive"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? 2008.", "cit": "[CLS] generalizing textual patterns (both manually and automatically) for the identification of relationships has been proposed since the early nineties"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] f(o) and we seek e? = arg max e max f ??f(o) pr(e, f ?|o) (2) = arg max"}
{"pre": "[CLS] the first is the task of identifying the event and event extraction of event descriptions of event descriptions of", "cit": "[CLS] as a foundational task in text understanding #refr, contradiction detection has many possible applications. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a small set of sentences of sentences of", "cit": "[CLS] while previous work has combined web-scale features with other features in specific classification problems #otherefr; #refrb), we provide a"}
{"pre": "[CLS] the first is the case for the syntactic analysis of the english genitive, the english lexical substitution task #refr.", "cit": "[CLS] for the first, we build directly on the work of #refr (see ?2.2). [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with the one of", "cit": "[CLS] although wordbased parsers are proposed in #otherefr; #refr, they do not build bunsetsus and are not compatible with other"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best results for the best", "cit": "[CLS] our work leverages the idea that sharing information between local probabilities in a structured unsupervised model can lead to"}
{"pre": "[CLS] the first is the case for the generation of referring expressions from the linguistic phenomena of referring expressions and", "cit": "[CLS] the realization module works in the opposite direction, verbalizing the results of the execution of the command from the"}
{"pre": "[CLS] the bionlp 2011 shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP]", "cit": "[CLS] currently there are five metrics in wider use: muc #otherefr, the two ceaf metrics #refr and blanc #otherefr. [SEP]"}
{"pre": "[CLS] the first is the task of identifying the polarity of a text that is a text of a text", "cit": "[CLS] researchers have focused on the automated extraction of semantic lexicons #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to learn a translation hypotheses and a", "cit": "[CLS] we used mxpost #refr, and in order to discover more general patterns, we map the tag set down after"}
{"pre": "[CLS] the first is the process of annotating the correct grammatical errors in a text with a text with a", "cit": "[CLS] some statistical nlg research has looked at subproblems of language generation, such as ordering of np premodifiers [shaw and"}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders and opinion holders in opinion holders in opinion holders in", "cit": "[CLS] classifying stance involves identifying a holistic subjective disposition, beyond the word or sentence #otherefr; #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained with a maximum entropy model #otherefr, which is trained on the penn treebank wall street", "cit": "[CLS] one way to solve the mixing problem is for the sampler to make more global moves, e.g., with table"}
{"pre": "[CLS] the task of semantic relation between nominals (gi#refr. [SEP] task 4 and the two entities in the two entities", "cit": "[CLS] maximum entropy classifiers have proven effective for a variety of nlp problems including word sense disambiguation #otherefr; ye and"}
{"pre": "[CLS] the most common approach to this problem is to use syntactic information to learn semantic parsers #otherefr; #refr. [SEP]", "cit": "[CLS] we tested this hypothesis by combining our outputs, which are the most precise, with the outputs of the system"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] b??? #otherefr; #refr, it remains the standard in the statistical mt literature. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford word segmenter #refr to extract the word pairs from the target side of the parallel", "cit": "[CLS] previous work on generative approaches uses hidden markov models #otherefr, finite state automata #refr and bayesian learning #otherefr to"}
{"pre": "[CLS] in the context of machine learning techniques have been applied to many natural language processing tasks with promising results,", "cit": "[CLS] the unsupervised approaches for keyphrase extraction proposed so far have involved a number of techniques, including language modeling #otherefr,"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal (wsj) corpus #refr. [SEP] trees and the penn", "cit": "[CLS] viterbi-em has been used in various nlp tasks before and often performs better than classic em #refr. [SEP] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] given a word-aligned parallel corpus, phrasebased systems #otherefr; #refr work with #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the generation of referring expressions is a subject of research in natural language generation (nlg) in natural language generation", "cit": "[CLS] the lingo grammar matrix #refr facilitates formal modeling of syntax by generating basic hpsg ?starter grammars? for languages from"}
{"pre": "[CLS] the most common approach to paraphrase extraction is to use a lexicon to extract paraphrases from comparable corpora #otherefr;", "cit": "[CLS] we are then able to rank the symmetric candidate translation pairs in the pool of candidates according to their"}
{"pre": "[CLS] we use the stanford word segmenter #refr to tokenize the source side of the target side of the source", "cit": "[CLS] some of these models are partially supervised, combining unlabeled parallel text with manually-aligned parallel text #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the first is the automatic evaluation of the automatic metric of the automatic evaluation of the automatic evaluation of", "cit": "[CLS] most closely related to our work is the study by #refr who predict code-switching in recorded english-spanish conversations. [SEP]"}
{"pre": "[CLS] the first is the automatic evaluation of discourse structure of discourse structure of discourse structure of discourse structure of", "cit": "[CLS] contributions like bootcat #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] in this paper, we focus on an approach to semantic representation that supports this strategy: robust minimal recursion semantics"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best results for the best", "cit": "[CLS] this is similar to how #refr use diagonal initialization in an hmm for field segmentation to encourage the model"}
{"pre": "[CLS] the first is the automatic evaluation of discourse coherence models in the natural language processing (nlp) tasks such as", "cit": "[CLS] so we chose the genetic algorithm based text planner described in #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to learn a translation models from a", "cit": "[CLS] c}, is the standard phrase-based model (see e.g. #refra)) and introduces a feature in the following form: h?map:s?tm (c?k,"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] algorithm with the best derivation of the best derivation", "cit": "[CLS] we used bitpar #refr for our unlexicalized experiments. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of identifying the overall event descriptions of event descriptions is a text in a document that is", "cit": "[CLS] names of these kinds, generalized names #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the generation of referring expressions is a subject of research in natural language generation (nlg) in the past #otherefr;", "cit": "[CLS] the interaction of morphological nalysis with spelling correction #refr is another possibly fruitful area of work. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the system uses a combination of the maximum entropy model of #refr to generate multiple reference translations for the", "cit": "[CLS] sb-mmsystem: the approach #refr builds on the baseline definition of simplicity using word frequencies but attempt at defining a"}
{"pre": "[CLS] the system used in this paper is based on the conll shared task on dependency parsing #refr. [SEP] [SEP]", "cit": "[CLS] previous approaches to the srl task have made use of a full syntactic parse of the sentence in order"}
{"pre": "[CLS] #refr use a bootstrapping method to identify the polarity of words in a sentence. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] skipchain crf model is applied for entity extraction and meeting summarization #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the system uses a log-linear model #refr to extract the best parse the best parse the best parse the", "cit": "[CLS] this paper describes the university of illinois system that participated in the hoo 2012 shared task on error detection"}
{"pre": "[CLS] #refr proposed a method for extracting relations between entities from relations between entities and relations and relations between entities", "cit": "[CLS] there is growing research on relations between nominals #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] algorithm with the best derivation forest, and its default", "cit": "[CLS] there have been several attempts at japanese parsers #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained with the perceptron algorithm of #refr. [SEP] score entire sequence models and the entire sequence", "cit": "[CLS] the focus of learning-based coreference research has also shifted from the acquisition of a pairwise model that determines whether"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with a probability distribution", "cit": "[CLS] recently, many groups have had success using gibbs sampling to address the complexity issue and nonparametric priors to address"}
{"pre": "[CLS] the feature weights were tuned using minimum error rate training (mert) #refr on the development set using minimum error", "cit": "[CLS] in this work we treat the process of transliteration as a process of direct transduction from sequences of tokens"}
{"pre": "[CLS] the system is based on the moses toolkit #otherefr and the best system described in #refr. [SEP] test set", "cit": "[CLS] following #otherefr, we used the version 11a nist bleu script with its default settings to calculate the bleu scores"}
{"pre": "[CLS] the first is the process of word sense disambiguation (wsd) by using the wsd system of #refr. [SEP] [SEP]", "cit": "[CLS] however, due to a relative scarceness of parallel texts for many language pairs and domains, there has been a"}
{"pre": "[CLS] the system is based on the inprotk toolkit for the best performing system described in #refr, a system that", "cit": "[CLS] a different approach, based on statistical techniques was proposed in #otherefr. #refr presents a method of extracting answers as"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be a difficult problem #otherefr;", "cit": "[CLS] in particular, for mt, we expect it will benefit procedures which learn translation rules from syntactically analyzed parallel corpora,"}
{"pre": "[CLS] the second approach is to use a small number of seed examples for example outputs #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] in a similar vein, the work 3relative to the current word, whose tag is assigned a probability value by"}
{"pre": "[CLS] #refr use a similar approach to identify opinion polarity lexicons for opinion holders and their polarity in opinion expressions", "cit": "[CLS] mpqa05 : the mpqa subjectivity lexicon that is part of the opinionfinder system #refra; wilson et al 2005b). [SEP]"}
{"pre": "[CLS] #refr proposed a method for identifying word pairs of words in a target word in a target word in", "cit": "[CLS] in word sense disambiguation (wsd), an even narrower context is taken into consideration, for instance in graph based wsd"}
{"pre": "[CLS] the first step is to use a stochastic model that predicts a user to predict the sentence planner of", "cit": "[CLS] a condensed and compressed subset of the is ? the reinforcement learning state ? is used to learn which"}
{"pre": "[CLS] the english side of the parallel corpus was parsed using the berkeley parser #refr, and the german side of", "cit": "[CLS] theoretically, it is possible to describe the syntactic and semantic onstraints hat govern the acceptability of a structure in"}
{"pre": "[CLS] the system uses the stanford ner tagger #refr to extract the stanford parser for the best parse the stanford", "cit": "[CLS] the importance of a well-defined tagging scheme and consistent ita has been well recognized and studied in the past"}
{"pre": "[CLS] in this paper, we propose a method for extracting parallel bilingual parallel bilingual parallel bilingual parallel bilingual news articles", "cit": "[CLS] the translation quality is evaluated by case-insensitive bleu-4 metric #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best results for the best", "cit": "[CLS] hwa #otherefr used a variant of the inside-outside algorithm presented in #refr to exploit a partially labeled out-of-domain treebank,"}
{"pre": "[CLS] #refr use a graph to identify and their polarity of words in their polarity in a sentence. [SEP] [SEP]", "cit": "[CLS] researchers have studied the problem at the document level #otherefr; #refr, and attribute level #otherefr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained with a maximum entropy model #otherefr, which is trained on the penn treebank wall street", "cit": "[CLS] the model proposed in section 3 is generally applicable to any lexicalized grammars, and this section reports the evaluation"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to learn a translation probabilities from", "cit": "[CLS] several methods have been proposed to use syntactic information to handle the reordering problem, e.g. #otherefr; #refr. [SEP] [PAD]"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best results for the best", "cit": "[CLS] one answer was provided by #refr, who showed that already one hundred manually segmented words provide significant improvements to"}
{"pre": "[CLS] the language model is trained with the perceptron algorithm described in #refr. [SEP] model 2 (dtm2) #refr for the", "cit": "[CLS] under the provided scoring metrics, two consistently high-performing systems in the semeval 2007 evaluations are the ku #refr and"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to learn a translation probabilities from", "cit": "[CLS] our preprocessing steps include tokenization on the english side and for chinese: automatic word segmentation using the revised version"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the training data is to", "cit": "[CLS] a growing body of work in this field employs distinct techniques from a wide variety of perspectives from text-to-record"}
{"pre": "[CLS] #refr use a similar approach to identify the polarity of words in a sentence. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] in fact, moving from one (source) domain to a different (target) domain will often lead to severe performance drops"}
{"pre": "[CLS] the first is the task of coreference resolution #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? the typed-similarity", "cit": "[CLS] to address these problems, two coreference scoring programs have been developed: b3 #otherefr and ceaf #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] for czech, the adaptation by #refr culminated in an 80 f1-score. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to compute similarity between two words in a sentence. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] #refr present a fundamentally different approach to selectional preference induction which uses soft clustering to form classes for generalisation"}
{"pre": "[CLS] #refr use a morphological analyzer and morphological analyzer and morphological analysis to train a pos tagging method for arabic", "cit": "[CLS] #refr demonstrates convincingly that morphological disambiguation can be aided by a morphological analyzer, which, given a word without any"}
{"pre": "[CLS] the first is the polarity of a document #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] , s ,", "cit": "[CLS] all corpora were stemmed #otherefr and part-of-speech tagged #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a task of semantic relations between words and phrases and logical", "cit": "[CLS] task 4 at the 2007 semeval competition (gi#refr focused on the identification of semantic relations among nominals in text."}
{"pre": "[CLS] in the second international chinese word segmentation bakeoff #otherefr; #refr and the best results are achieved by the best", "cit": "[CLS] in addition, a number of different transition systems have been proposed, in particular for dealing with non-projective dependencies, which"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] for the chunk part of the code, we adopt the ?inside?, ?outside?, and ?between? (iob) encoding originating from #refr."}
{"pre": "[CLS] the first is the case for the word alignment of word alignment of word alignment of word alignment of", "cit": "[CLS] #refr showed how to adapt a bilingual method to align more than two c ? 2008. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for parsing #refr. [SEP] grammars with", "cit": "[CLS] an alternative to this approach is to use transitionbased parsing #otherefr; #refr, where there is an incremental processing of"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a similar to the one used in the sense of the", "cit": "[CLS] #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of identifying the sentence pairs in a text is to be extracted from text #refr. [SEP] text", "cit": "[CLS] one could rely on existing trainable sentence selection #otherefr strategies to pick up appropriate ? i ?s from the"}
{"pre": "[CLS] the first is the stanford ner tagger #refr and the stanford parser #otherefr. [SEP] shared task on coreference in", "cit": "[CLS] the stanford deterministic coreference resolution system #refr uses an unsupervised sieve-like approach to coreference resolution. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second approach is to use a word clusters as described in #refr. [SEP] [SEP] [SEP] [SEP] [SEP] =", "cit": "[CLS] it also contributes to word sense disambiguation #otherefr, named entity recognition #refr, part-of-speech tagging #otherefr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] recent adaptation techniques can be seen as mixture models, where two or more phrase tables, estimated from inand mix-domain"}
{"pre": "[CLS] the first is the task of identifying the semantic relations between nominals of noun phrases and semantic relations #refr.", "cit": "[CLS] further, the methods developed in this paper may be applicable to graph representations that occur in other problems such"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking approaches #otherefr; #refr to learning for dependency", "cit": "[CLS] a dependency graph can be modeled with the following nodes, as first proposed by #refr: . [SEP] [PAD] [PAD]"}
{"pre": "[CLS] #refr proposed a method for identifying paraphrases using a bilingual parallel corpus of news stories from comparable corpora. [SEP]", "cit": "[CLS] the work which best exemplifies this strand of research is found in the efforts of garrette et al#otherefr and,"}
{"pre": "[CLS] #refr use a supervised approach to identify the polarity of words in a sentence. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] recent studies #refr #otherefr have shown that more than 60% of word segmentation errors result from new words. [SEP]"}
{"pre": "[CLS] the most popular approach to word alignment is the ibm models #otherefr; #refr. [SEP] model 1 and the hmm", "cit": "[CLS] recently, there has been an increasing interest in decipherment work #otherefra; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the grammar is a grammar that is a parsing system for the grammar that is a grammar that is", "cit": "[CLS] within verbmobil, the generation component will also be used for text generation when producing protocols as described in #refr."}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for dependency parsing #refr. [SEP] grammars", "cit": "[CLS] clustering word clusters are regarded as lexical intermediaries for dependency parsing #otherefr and pos tagging #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to learn a translation probabilities from", "cit": "[CLS] och et al [1999]?s alignment template model can be reframed as a phrase translation system; #refr] use phrase translation"}
{"pre": "[CLS] the generation of referring expressions is a subject of research in natural language generation (nlg) in natural language generation", "cit": "[CLS] kibble and power [#refr] propose a system which uses centering theory [walker et al, 1998] for planning of coherent"}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders in which the polarity of words are represented as subjective", "cit": "[CLS] in the domain of synchronous conversations, prosodic features such as duration, speech rate and pause have been used for"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] event and semantic role", "cit": "[CLS] the work of #refr in linearizing kernel functions allows us to take a look at these tree fragments. [SEP]"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] we first aligned the articles using a method based on clir #refr and then aligned the sentences in these"}
{"pre": "[CLS] we use the averaged perceptron algorithm #refr to train the parameters of the model for the model parameters. [SEP]", "cit": "[CLS] the most notable of these include the trigram hmm tagger #otherefr, and cyclic dependency networks #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to this problem is to use a dictionary to model the decision to generate coherent", "cit": "[CLS] the problem of metaphor modeling is gaining interest within nlp, with a growing number of approaches exploiting statistical techniques"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to translate the phrases as the", "cit": "[CLS] lexicalized phrase orientation models #refr predict the orientation of a phrase with respect to the last translated one. [SEP]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] figure 1(a) shows the coordinate structure extracted from the output of #refr parser on the above example. [SEP] [PAD]"}
{"pre": "[CLS] the most common approach to deriving selectional preference acquisition is to use a lexicon to measure the similarity between", "cit": "[CLS] in this section, we describe our methodology for inducing conceptual domains for a linguistic metaphor by adapting techniques for"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] within the spectrum of appraoches to natural language parsing, xle can be considered a hybrid system combining a hand-crafted"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] event and semantic role", "cit": "[CLS] to create a kind of bird?s-eye view, #refr, statement map #otherefr identified various relations between statements including contradictory relations,"}
{"pre": "[CLS] in this paper, we propose a new approach to address the problem of learning from monolingual data by using", "cit": "[CLS] examples include improving post-editing efficiency by filtering out low quality segments which would require more effort or time to"}
{"pre": "[CLS] the feature weights were tuned using mert #refr on the development set. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] therefore decoding efficiency is influenced by the following combinatorial problem: given an input sentence of length n and a"}
{"pre": "[CLS] the most popular approach to translation models #otherefr; #refr rely on the source side and the target side #otherefr.", "cit": "[CLS] in addition, it is used in smt as a feature in minimum error training #refr and for rescoring lattices"}
{"pre": "[CLS] we use the stanford parser #refr to extract dependency relations for each word and parse trees from the sentence", "cit": "[CLS] conditional random fields #otherefr and shallow parsing #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to extract dependency relations for each sentence and parse trees from the sentence", "cit": "[CLS] part-of-speech tagging was performed using the stanford tagger #refr within lightside. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal (wsj) treebank #otherefr; #refr. [SEP] trees and the", "cit": "[CLS] #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the automatic metric of text with human annotation of text segments as described in #refr. [SEP]", "cit": "[CLS] in a sense this problem is analogous to kana?kanji conversion #otherefr and #refr), in that we seek to determine"}
{"pre": "[CLS] in the literature, supervised learning methods have been applied to relation extraction #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] many machine learning methods have been proposed to address this problem, e.g., supervised learning algorithms #refr, semi-supervised learning algorithms"}
{"pre": "[CLS] we use the berkeley parser #refr to obtain the syntactic parse trees for each sentence and the english and", "cit": "[CLS] the algorithm is similar to #refr, which binarize each constituent node to create some intermediate nodes that correspond to"}
{"pre": "[CLS] the task of identifying the semantic relations between nominals of noun phrases and their arguments are also key to", "cit": "[CLS] unfortunately, this number is often unavailable in information extraction tasks in general #refr, and attribute extraction in particular. [SEP]"}
{"pre": "[CLS] #refr proposed a method for learning semantic parsers from a target language for a target language for a target", "cit": "[CLS] #refr proposed an unlexicalized pcfg parser that modified pcfg probabilities to condition the existence of syntactic parallelism. [SEP] [PAD]"}
{"pre": "[CLS] the most common approach to this problem is to use a stochastic parse tree #otherefr; #refr. [SEP] model to", "cit": "[CLS] there are several related methods for 1-best outputs, such as revision learning #refr and transformation-based learning #otherefr for partof-speech"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] model with the average number of the average", "cit": "[CLS] o s?e#refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the syntactic dependency parsing of the english lexical substitution task #refr. [SEP] and", "cit": "[CLS] we apply an automatic conversion process using the gold-standard np data annotated by #refra). [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the words is to be represented by a word", "cit": "[CLS] parallel corpora are very valuable resources in various fields of multilingual natural language processing such as statistical machine translation"}
{"pre": "[CLS] the system is based on the moses toolkit #refr that we used for the best output of the english", "cit": "[CLS] the longest common substring (lcs) feature #refr is useful for finding proper name aliases. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be np-hard #refr. [SEP] [SEP]", "cit": "[CLS] a vital component of the realizer is the hypertagger #refr, which predicts lexical category assignments using a maxent model"}
{"pre": "[CLS] the first is the task of identifying the event descriptions of the bionlp?09 shared task on event extraction #refr.", "cit": "[CLS] zero anaphora resolution has remained a very active area of study for researchers working on japanese, because of the"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] many strategies of integrating morphology information into state-of-the-art smt systems in different stages have been proposed. #refr proposed a"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to learn a translation models from a", "cit": "[CLS] possibly the most remarkable evolution of recent years in statistical machine translation is the step from word-based models to"}
{"pre": "[CLS] the conll shared tasks on dependency parsing in 2006 and 2007 #refr have been shown to be a wide", "cit": "[CLS] use of global features for structured prediction problem has been explored by several nlp applications such as sequential labeling"}
{"pre": "[CLS] the most common approach to paraphrase extraction is to use a lexicon to extract paraphrases from comparable corpora #otherefr;", "cit": "[CLS] context matching at inference time was often approached in an application-specific manner #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to identify the polarity of words in a sentence. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] consequently, the goal of text mining (also known as literature mining) systems and algorithms is to assist users find"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for dependency parsing #refr. [SEP] grammars", "cit": "[CLS] then, when parsing, we visit each node n in the same bottomup order we would use for viterbi decoding,"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] many state-of-the-art parsers work with lexicalized parsing models that utilize the information and statistics of word tokens #refr. [SEP]"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] it is also possible to self-train a semantic parser without any labeled data #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] score #refr with the entire candidate set. [SEP]", "cit": "[CLS] linear programming helps to find an accurate approximated solution to this problem and became very popular in summarization field"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] only the statistical translation model was used in the evaluation. aligner #otherefr and the phrasal itg aligner (pialign) #refr."}
{"pre": "[CLS] the most common approach to machine translation is to use a log-linear combination of statistical machine translation #otherefr; #refr.", "cit": "[CLS] in #refr, different shortcomings of lexicon-based translation scheme was discussed for the more semantic-oriented task subjective analysis, instead the"}
{"pre": "[CLS] in the context of relation extraction #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] . [SEP] ?", "cit": "[CLS] this mapping technique is based on the many-to-one scheme used for evaluating unsupervised part-of-speech induction #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] shared", "cit": "[CLS] in 2010, we applied the turku event extraction system to detecting events in all 18 million pubmed abstracts, showing"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr.", "cit": "[CLS] srl methods critically depend on features of the syntactic structure, and consequently parsing mistakes can harm the quality of"}
{"pre": "[CLS] the second approach is to use a word lattice as a sequence of a sequence of a sequence of", "cit": "[CLS] recent work has started to address these shortcomings #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] score is a variant of the best performing", "cit": "[CLS] dependency-converted versions of the penn treebank, propbank and nombank were used in the conll-2008 shared task #refr, in which"}
{"pre": "[CLS] the first is the case for the generation of referring expressions from the dialogue manager sends and has been", "cit": "[CLS] in this work we have chosen to use patr-ii but in the future constructions from a more expressive formalism"}
{"pre": "[CLS] we use the moses toolkit #refr to train a 5-gram language model with kenlm and then filtered out the", "cit": "[CLS] other work has employed less common approximations to the model reducing its search space complexity #otherefr; #refr. [SEP] [PAD]"}
{"pre": "[CLS] in the literature, supervised learning methods have been applied to part-of-speech tagging and unsupervised #otherefr and unsupervised part-of-speech tagging", "cit": "[CLS] the realm system for sparse information extraction has also used unsupervised hmms to help determine whether the arguments of"}
{"pre": "[CLS] we use the stanford named entity recognizer #refr to extract named entities from the text and extract named entities", "cit": "[CLS] since this constructed tree structure represents semantic, discourse, and structural information extracted from the similar wikipedia paragraphs to each"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal (wsj) treebank #otherefr; #refr. [SEP] grammars with a", "cit": "[CLS] the discussion of the #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] completely unsupervised monolingual anaphora resolution has been approached using, e. g., markov logic #otherefr; #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the system is based on the standard approach of #refr and the best system for machine translation (mt) evaluation", "cit": "[CLS] other approaches using machine translation for error correction are not aimed at training smt systems but rather at using"}
{"pre": "[CLS] the first is the task of identifying the overall sentence pairs from the english text using the stanford parser", "cit": "[CLS] madnani et al#otherefr and #refr propose crowdsourcing to overcome the problem of annotator variability. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the input is to be", "cit": "[CLS] quite a different idea was suggested in #refr, of using the grammatical judgement of a parser to assess fluency,"}
{"pre": "[CLS] #refr use a similar method to identify the relations in a text in which words in a window of", "cit": "[CLS] these efforts have been met with some success in evaluations such as phrase similarity tasks #otherefr; #refr, sentiment prediction"}
{"pre": "[CLS] in the context of machine translation, itg has been explored for applications in machine translation #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] for example, #otherefr used random walks on wordnet graph to measure lexical semantic relatedness between words. #refr used graph-based"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? 2008. [SEP] ?", "cit": "[CLS] in addition to the experiments listed above, we also attempted to encode the output of a modern wsd engine"}
{"pre": "[CLS] the task of identifying the event descriptions of event extraction and event extraction of event descriptions has been well", "cit": "[CLS] examples include fact extraction systems such as #refr and entity extraction systems such as #otherefr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the averaged perceptron algorithm #refr to train the parameters of the averaged perceptron algorithm #otherefr. [SEP] [SEP]", "cit": "[CLS] the overall score of a hypothesis is the weighted sum of the scores from all the hypothesis evaluators: score(h,w)"}
{"pre": "[CLS] the system is based on the inprotk toolkit for the paradise system described in #refr. [SEP] system, which is", "cit": "[CLS] for example, the system described in #refr was designed on the basis of the principle that it was better"}
{"pre": "[CLS] the most common approach to translation is to use a log-linear translation model #refr and a log-linear translation model", "cit": "[CLS] for word alignments we use the maximum entropy aligner described in #refr that is trained using hand aligned training"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] the dominance of traditional phrase-based statistical machine translation (pbsmt) models #refr has recently been challenged by the development and"}
{"pre": "[CLS] the task of segmenting utterances into words, for example, johnson and colleagues have been shown to be useful in", "cit": "[CLS] we first define a new feature set to induce a new maxent model (maxent-b) which only uses lexical features"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a task of identifying the semantic relations between words in a", "cit": "[CLS] the semantic textual similarity #otherefr; #refr examines semantic similarity at a sentence-level. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the system used in this paper is based on the stanford parser #refr and the stanford parser #otherefr. [SEP]", "cit": "[CLS] significant improvements have been made in the field of language processing in general, and improved learning techniques have been"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] of a text with", "cit": "[CLS] there has been growing interest in characterizing social media users based on the content they generate; that is, automatically"}
{"pre": "[CLS] the grammar is a grammar that is a grammar that is a grammar that is a parsing model that", "cit": "[CLS] in paxticulax subset constraints #refr are employed to construct larger domains from smaller ones. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] sentence and thus a", "cit": "[CLS] thus, the current best accuracy performance on explicit pdtb relations are 94.15% on 4 relations #refr, and 86.77% on"}
{"pre": "[CLS] the most popular approach to translation models #otherefr; #refr rely on the source side and the target language to", "cit": "[CLS] later metrics that move beyond n-grams achieve higher accuracy and improved robustness from resources like wordnet synonyms #otherefr; #refr."}
{"pre": "[CLS] #refr use a bootstrapping method to identify opinion holders and extract opinion holders and opinion holders and opinion expressions", "cit": "[CLS] the advent of online social networks has produced a crescent interest on the task of sentiment analysis for short"}
{"pre": "[CLS] the task of identifying the semantic textual similarity of a text is a text of a text that of", "cit": "[CLS] we also plan to look at the helpfulness of argumentation schemes #refr, and other linguistic and essay features for"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the longrange reordering problem as a sequence of words", "cit": "[CLS] in nlp, such methods have been applied to tasks such as pos tagging #otherefr, parsing #refr, and machine translation"}
{"pre": "[CLS] the first is the case for the automatic acquisition of subcategorization frames from corpora #refr, which is the web", "cit": "[CLS] however, there has been a growing interest in idiom token identification in recent times #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most popular paraphrase collection has been used to paraphrase the source sentences into either from comparable corpora #otherefr;", "cit": "[CLS] finally, the developed paraphrase collection will be attested through applications, such as sentence compression #otherefr; #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the longrange reordering problem as a sequence of words", "cit": "[CLS] in this context, the most common formalization of a word?s information content is its surprisal #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the most common sense similarity to the sense of senses is to the senses in the senses of the", "cit": "[CLS] words appearing in similax grammatical contexts are assumed to be similar, and therefore classified into the same class #otherefr;"}
{"pre": "[CLS] the similarity between two sentences is computed using the wordnet #otherefr and the wordnet similarity measure proposed by #refr.", "cit": "[CLS] cross-lingual textual entailment (clte) has been recently proposed by #refr as an extension of the textual entailment task #otherefr."}
{"pre": "[CLS] the first is the task of identifying the semantic relations between nominals of noun phrases and the verb classes", "cit": "[CLS] using dictionaries as network of lexical items or senses has been quite popular for word sense disambiguation #otherefr before"}
{"pre": "[CLS] the most common approach to selectional preference acquisition is to use a similarity measure #otherefr; #refr. [SEP] model to", "cit": "[CLS] a number of works are based on a compositional semantics approach, where a prior representation of a target lexical"}
{"pre": "[CLS] the system is based on the inprotk toolkit for the best performing system described in #refr. [SEP] evaluation of", "cit": "[CLS] for instance, using linear combinations #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the sentence in", "cit": "[CLS] there are other approaches in which the generation grammars are extracted semiautomatically #refr or automatically #otherefr). [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the system is based on the inprotk project at the university of pennsylvania and the xerox project #refr. [SEP]", "cit": "[CLS] abductive inference has a long history in plan recognition, text understanding and discourse processing #otherefr; #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] sentence pair of a", "cit": "[CLS] while ner from formal texts has been well studied, relatively little work on ner for twitter was reported. #otherefr"}
{"pre": "[CLS] in this paper, we propose a method for learning semantic similarity based on the similarity of the similarity between", "cit": "[CLS] worse still, it is hard to write rules for some grammatical errors, for example, detecting errors concerning the articles"}
{"pre": "[CLS] the system is based on the moses toolkit #otherefr and the standard phrase-based system described in #refr. [SEP] test", "cit": "[CLS] supertagging #refrb): incorporating lexical syntactic descriptions, in the form of supertags, to the language model and target side of"}
{"pre": "[CLS] topic models have been successfully applied to a wide range of tasks including word sense disambiguation #otherefr, word sense", "cit": "[CLS] hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] sentiment analysis research has been performed to distinguish the authors? polarity #otherefr: #refr to sentence-level #otherefr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the overall sentence pairs from the same text with a text #refr.", "cit": "[CLS] there has been much research effort on automatic disfluency detection in recent years #otherefr; #refr, particularly from the darpa"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with the one of", "cit": "[CLS] #refrb) report that svm outperforms maxent models in chinese dependency parsing, using the algorithms of yamada and matsumoto #otherefr"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] early studies have suggested that lexical features, word pairs (crossproduct of the words in the first and second argument)"}
{"pre": "[CLS] in the second international chinese word segmentation bakeoff #otherefr; #refr and the segmentation bakeoff #otherefr, the segmentation bakeoff #otherefr,", "cit": "[CLS] #refr construct a graph to encourage similar n-grams to be tagged similarly, resulting in moderate gains in one domain,"}
{"pre": "[CLS] the first is the one of the most popular approach to use crowdsourcing, which has been to use a", "cit": "[CLS] furthermore, there have been some investigations on comma insertion into japanese written texts #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of identifying the semantic textual similarity of a semantic textual similarity of a semantic similarity of a", "cit": "[CLS] mutual information has been positively used in many nlp tasks such as collocation analysis #otherefr, and word sense disambiguation"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] we experimented with phrase-table interpolation using perplexity minimisation #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the word alignment of word alignment of word alignment of word alignment of", "cit": "[CLS] the interactive machine translation paradigm was first explored in the transtype and transtype2 projects #otherefra; #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to translation lexicons from parallel corpora #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] exploratory analysis of reference translations by carpuat #otherefr, which has proven to be effective in the context of wsd"}
{"pre": "[CLS] the second approach is to use a word segmentation and pos tagger to tag dictionary to tag each word", "cit": "[CLS] several other variants of crf model has been proposed in the machine learning literature, such as the generalized expectation"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of meaning that are not", "cit": "[CLS] a subset of twenty-five of the fifty verbs was used by #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second approach is to use a small number of approximate inference rules #otherefr; #refr. [SEP] model to handle", "cit": "[CLS] according to #refr, ?the two approaches [word-based and character-based approaches] are either based on a particular view of segmentation.."}
{"pre": "[CLS] #refr proposed a method for learning a probabilistic model that learns latent dirichlet allocation respectively, but the meaning of", "cit": "[CLS] for this reason, we chose to evaluate the systems using n-gram precision and recall (among other metrics), following #refr"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] qlf, #refr and mrs, copestake et al#otherefr) which are easier to produce from text without contextual inference but which"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking approaches #otherefr; #refr to learning for dependency", "cit": "[CLS] for example, several early works #otherefr; #refr demonstrate transfer of shallow processing tools such as part-of-speech taggers and noun-phrase"}
{"pre": "[CLS] the most popular approach to translation lexicons from parallel corpora #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] bayesian inference methods have become popular in natural language processing #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] the baseline parsing system comprises: ? an hmm part-of-speech tagger #refr, which produces either the single highest-ranked tag for"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? ? ?", "cit": "[CLS] and, there have been many extensions in machine learning models #otherefr; #refra; zhuang and zong, 2010b). [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the syntactic parsing of the english lexical substitution task #refr. [SEP] test set", "cit": "[CLS] the probability model we use can be found earlier in #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the automatic evaluation of discourse structure of discourse structure of discourse structure of discourse structure of", "cit": "[CLS] all domain-specific markup was removed, and the text was processed by the mxterminator sentence boundary detector \\[reynar and ratnaparkhi"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] we use the yamcha #refr chunker for our text chunking. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to tag the one of the source sentence in which the target language to", "cit": "[CLS] we evaluate on two tasks of differing granularity: the first, a coarse-grain classification, follows cherry and quirk #otherefr; the"}
{"pre": "[CLS] the second approach is to use a small number of manually annotated corpora such as the same data set", "cit": "[CLS] some systems even perform the pos tagging as part of a syntactic analysis process #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr proposed a method for identifying word pairs of words in a target word in a target word in", "cit": "[CLS] we trained their mixlda model on their corpus consisting of 3,361 bbc news documents and corresponding images #refr. [SEP]"}
{"pre": "[CLS] the first is the case for the automatic acquisition of subcategorization frames from corpora #refr, which is the penn", "cit": "[CLS] #refr introduced nitrogen, a system that implements a new style of generation in which corpus-based ngram statistics are used"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a word sense disambiguation task", "cit": "[CLS] we evaluated werdy on the semeval-2007 coarse-grained wsd task #refr, both with and without automatic recognition of entries. [SEP]"}
{"pre": "[CLS] in this paper, we present a discriminative model that uses the perceptron algorithm of #refr. [SEP] model to handle", "cit": "[CLS] #refr employed structured sparsity in computational sociolinguistics. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal (wsj) corpus #refr. [SEP] trees and the penn", "cit": "[CLS] this is essentially the observation of #refr, and her bottom-up factoring of n-gram computation is easily incorporated into our"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] [SEP] is a set of semantic similarity to", "cit": "[CLS] the idea of using supervised machine learning for wsd is not new and was used for example in (ng"}
{"pre": "[CLS] #refr use a similar approach to compute sentence importance. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] however, as demonstrated by #refr, pang and lee #otherefr, there are some nouns and verbs that are useful sentiment"}
{"pre": "[CLS] the most common approach to paraphrase generation is to use a lexicon to paraphrase generation #otherefr; #refr. [SEP] words", "cit": "[CLS] most of the previous research on sentence compression focuses on deletion using syntactic information, #otherefr, #refr). [SEP] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the sentence in", "cit": "[CLS] for example, nlp components such as parsers and co-reference resolution algorithms could be compared in terms of how much"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] a semantic similarity can be defined at structural level over a graph, e.g. #otherefr, as well as combining structural"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a noun phrases in a noun phrases in", "cit": "[CLS] pt (rn|rm) = count(rm, rn)? rn count(rm, rn) for the evaluation, we extracted triples by named entity recognition #refr,"}
{"pre": "[CLS] in the second international chinese word segmentation bakeoff #otherefr; #refr and the best results are achieved by the best", "cit": "[CLS] finally, as noted in the introduction, although joint tagging and parsing is rare in dependency parsing, most state-of-the-art parsers"}
{"pre": "[CLS] the similarity between two sentences is computed using the wordnet #otherefr and the wordnet similarity measure proposed by #refr.", "cit": "[CLS] algorithms for computing semantic textual similarity #otherefr and machine translation #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford named entity recognizer #refr to extract named entities from the named entities from the named", "cit": "[CLS] there are also models, with distant supervision (ds), utilizing reliable texts resources and existing kbs to predict relations for"}
{"pre": "[CLS] the first is the task of identifying the sentence planning problem that the sentence planning problem of identifying the", "cit": "[CLS] the paradise model #refr proposes instead to predict system performance, using parameters representing interaction costs and benefits between system"}
{"pre": "[CLS] the first is the task of identifying the event descriptions of event extraction of event descriptions of event descriptions", "cit": "[CLS] #refr tried to extract multiple relations by choosing entity types. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking approaches #otherefr; #refr to learning for dependency", "cit": "[CLS] since this achievement, the model has been used by many researchers #otherefr; and #refrb)). [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] topic models have been successfully applied to a wide range of tasks including word sense disambiguation #otherefr, and sentence", "cit": "[CLS] prior work has shown that pointwise mutual information (pmi) is the most consistent scoring method for evaluating topic model"}
{"pre": "[CLS] the task of relation extraction has received much attention in recent years #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] recent years have seen a surge of interest in distant supervision for relation extraction #otherefr; #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the second approach is to use a word lattice as a word segmentation and its context #otherefr; #refr. [SEP]", "cit": "[CLS] for english, a number of methods have been proposed to cope with real-word errors in spelling correction #refr. [SEP]"}
{"pre": "[CLS] the most common approach to paraphrase generation is to use a lexicon to paraphrase database into a single sentence", "cit": "[CLS] the techniques proposed have a strong relationship to the type of text corpus used 3this verse from apollinaire?s nuit"}
{"pre": "[CLS] the first is the task of coreference resolution #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? the typed-similarity", "cit": "[CLS] interestingly, recent results indicate that unsupervised approaches to coreference resolution #otherefr, #refr) rival their supervised counterparts, casting doubts on"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with a beam search", "cit": "[CLS] the performance of the parser has enabled large-scale logic-based distributional research #otherefr, and it is a key component of"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] in addition, as we showed earlier our method can use bayesian inference #otherefr; #refr) and still scale easily to"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] the solution is to tie together these features #refr or re-weight the input distribution #otherefr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] reordering rules are defined over this parse either through machine learning techniques #otherefr; #refr or linguistically motivated manual rules"}
{"pre": "[CLS] the system is based on the inprotk project #refr. [SEP] score for the number of the number of the", "cit": "[CLS] the key idea is to have a common representation for all the possible interpretations of an ambiguous expression, as"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the elements is to use", "cit": "[CLS] another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information #otherefrb;"}
{"pre": "[CLS] the first group (asr features) includes a variety of applications, such as question answering #refr, and dialogue act tagging", "cit": "[CLS] despite the fact that in these systems recognition and processing have become extremely difficult, the reliability thereof has been"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a text with a text corpus consisting of", "cit": "[CLS] a recent investigation on word acquisition from transcribed speech and eye gaze in human machine conversation was reported in"}
{"pre": "[CLS] the most popular approach to paraphrase generation is the source sentences into a target language to be a target", "cit": "[CLS] zhao et al #otherefra) enrich this approach by adding multiple resources (e.g., thesaurus) and further extend the method by"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the words is to be represented by a single", "cit": "[CLS] the use of sentence fusion in multidocument summarization has been extensively explored by barzilay in her thesis #otherefr; #refr,"}
{"pre": "[CLS] the most common approach to paraphrase generation is to use a lexicon to paraphrase database into a target language", "cit": "[CLS] semantically, the compositionality of mwes is gradual, ranging from fully compositional to idiomatic #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders and opinion holders in opinion holders in opinion holders in", "cit": "[CLS] recent work by #refr classifies methods for bootstrapping subjectivity lexicons into two types: corpus-based and dictionary-based. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the translation quality is measured by bleu #refr. [SEP] score #refr on the test set. [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] nowadays, most of the state-of-the-art smt systems are based on bilingual phrases #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the work of #refr is similar to ours in that it is possible to use a more complex linguistic", "cit": "[CLS] automatic text summarization approaches have offered reasonably well-performing approximations for identifiying important sentences #otherefr; #refr; daume. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] in the context of machine translation, itg has been explored for a variety of nlp tasks such as sentiment", "cit": "[CLS] to perform the generation task, we build a statistical response generator by following #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be np-hard #refr. [SEP] s", "cit": "[CLS] one convenient way to do this is to frame the enumeration problem as a search space, e.g. #refr analysis"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? 2008. [SEP] ?", "cit": "[CLS] semantic parsers typically map sentences to logical semantic representations #otherefr; #refr, with many systems using ccg as the parsing"}
{"pre": "[CLS] #refr use a similar approach to identify the topics of topic boundaries in which words in a sentence and", "cit": "[CLS] case frames are automatically constructed from web cooking texts (12 million sentences) by clustering similar verb usages #refr. [SEP]"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to learn a translation hypotheses and a", "cit": "[CLS] in this way we can take advantage of all its features, for instance using different optimisation algorithms such as"}
{"pre": "[CLS] we use the stanford named entity recognizer #refr to extract named entities from the text and extract named entities", "cit": "[CLS] recent work has looked at creating summaries of single and multiple documents #otherefr; #refr, query and topic biased summarization"}
{"pre": "[CLS] the first is the automatic evaluation of discourse relations #refr. [SEP] of the discourse markers in the discourse treebank", "cit": "[CLS] tempeval #otherefr, and more recently tempeval-2 #refr, in 2010, were concerned with this problem. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second approach is to use a small number of seed examples for example outputs #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] semi-supervised learning #refr 94.39 #otherefr 94.17 (daume. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the system is based on the inprotk project #refr. [SEP] system, which is a combination of the user to", "cit": "[CLS] further work based on hmms was presented by #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] one way to do this would be to ask the user to annotate a small number of reviews with"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] to this end, the translational correspondence is described within a translation rule, i.e., #refr (or a synchronous production), rather"}
{"pre": "[CLS] the translation quality is measured by bleu #refr. [SEP] [SEP] = 4. [SEP] = 4. [SEP] = ? i", "cit": "[CLS] in this paper, we show that both the network topology and parameters of a head transducer translation model #refrb)"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal (wsj) section of the penn treebank #otherefr; #refr.", "cit": "[CLS] #refr explore a parser stacking approach in which the output of one parser is fed as an input to"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr.", "cit": "[CLS] #refr show the effectiveness of using semantic information in multifaceted topic models for text categorization. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] recent papers provide mixed evidence as to whether techniques that increase statistical parsing performance for english also improve german"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] #refr introduced two types of semantic features for tree-to-string machine translation. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for each sentence in the training corpus.", "cit": "[CLS] tree and sequence kernels have been successfully used in many nlp applications, e.g.: parse reranking and adaptation #otherefr, chunking"}
{"pre": "[CLS] we use the stanford corenlp suite to lemmatize and partof-speech tag each word #otherefr; #refr, and parse the input", "cit": "[CLS] there has been a lot of recent work on anaphoricity determination (e.g., #refr, uryupina #otherefr). [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the entire sequence of the entire", "cit": "[CLS] the inclusion of entity-level features has been a driving force behind the development of many coreference resolution systems #otherefr;"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] score #refr with the entire candidate set. [SEP]", "cit": "[CLS] a lot of recent work in computer vision has been aimed at predicting these keywords #otherefr; #refr. [SEP] [PAD]"}
{"pre": "[CLS] the most common approach to statistical machine translation is to use a log-linear combination of statistical machine translation #otherefr;", "cit": "[CLS] for word alignment we used mgiza++ #otherefr, a multi-threaded implementation of giza++ #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] several wide-coverage statistical parsers have recently been developed for combinatory categorial grammar #otherefr; #refr; hockenmaier, 2003b; clark and curran,"}
{"pre": "[CLS] the task of event extraction has received a lot of attention in recent years #otherefr; #refr. [SEP] [SEP] [SEP]", "cit": "[CLS] over the years, researchers have successfully shown how to build ground facts #otherefr, semantic lexicons #refr, encyclopedic knowledge #otherefr."}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] some examples of the idealistic approach are the direct ibm word model #otherefr, the phrase-based approach of #refr, and"}
{"pre": "[CLS] the system uses the maximum entropy model described in #refr to generate a set of features from the best", "cit": "[CLS] the ner task for hindi has been explored by cucerzan and yarowsky in their language independent ner work which"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal (wsj) corpus #refr. [SEP] trees and the penn", "cit": "[CLS] our notation fol- 1other elevant parsers imultaneously consider two or more words that are not necessarily n a dependency"}
{"pre": "[CLS] the first is the case for the word in the context of a word is analyzed by #refr, which", "cit": "[CLS] as far as we know, all the other perform some sort of segmentation even when the goal is not"}
{"pre": "[CLS] the parser is trained on the penn treebank corpus and tested on the penn treebank wall street journal, parser", "cit": "[CLS] in other languages, numeral classifier constructions occur far more often (cf. #refr). [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the translation quality is measured by bleu #refr. [SEP] score #refr with the bleu score of the translation quality.", "cit": "[CLS] since its invention, bleu #refr has been the most widely used metric for both machine translation (mt) evaluation and"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the longrange reordering model is to use of the", "cit": "[CLS] more recently, #refra) considered syntactic features for this task, using logistic regression with features extracted from parse trees produced"}
{"pre": "[CLS] in the context of sentiment analysis and opinion mining has been used to determine if the word is to", "cit": "[CLS] several researchers #otherefr; #refr) work on reducing the granularity of sense inventories for wsd. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the system is based on the inprotk toolkit for the best performing system described in #refr. [SEP] score for", "cit": "[CLS] data-to-texts systems have been evaluated in a number of ways, including human ratings #otherefr, bleu-like scores against human texts"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be np-hard #refr. [SEP] s", "cit": "[CLS] much work has been spent on hmm-based formulations, focusing on the computationally tractable side #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to translation models #otherefr; #refr rely on parallel data #otherefr. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] we used two well studied unsupervised aligners, giza++ #otherefr and hmm #refr and one supervised aligner, itg #otherefr as"}
{"pre": "[CLS] the task of identifying the semantic textual similarity of a target word is a target word is a sentence", "cit": "[CLS] we aligned the untagged english with each of the target languages using the berkeley aligner #refr to get one-tomany"}
{"pre": "[CLS] the first is the task of learning a semantic parser that is a complete parse of a semantic role", "cit": "[CLS] we did this for three reasons: #otherefr data-set; (ii) we avoid the possibility of a better parser identifying a"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the longrange reordering problem as a sequence of the", "cit": "[CLS] recently, the desire to incorporate syntax-awareness into machine translation systems has generated interest in the application of synchronous tree-adjoining"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] hrm orientations are determined using an unrestricted shiftreduce parser #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] shared", "cit": "[CLS] the gdep dependency parser trained for the biomedical domain in the experiments of #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to syntax-based translation #otherefr; #refr is to use a log-linear combination of a log-linear model", "cit": "[CLS] mert #otherefr and lr-score #refr for evaluation. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the parsing of the penn treebank #refr, which we will use the penn", "cit": "[CLS] many authors #otherefr, and #refr incorporate ra into their parsing systems, yet none rely on it solely, integrating it"}
{"pre": "[CLS] the translation quality is measured by bleu #refr. [SEP] [SEP] [SEP] = 4. [SEP] = 4. [SEP] = ?", "cit": "[CLS] they are selected based on their distance to the clwsd trial and test sentences #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a similar to the similarity of words in the sentence is", "cit": "[CLS] a wide range of techniques has been proposed for relation extraction in general and synonym extraction in particular ?"}
{"pre": "[CLS] the first is the case for the semantic role labeling task of translating natural language processing #otherefr; #refr. [SEP]", "cit": "[CLS] the creation of the penn chinese treebank #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr proposed a method for identifying word pairs of sentences in a sentence by clustering words and their corresponding", "cit": "[CLS] a lot of research has been done on text segmentation #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second approach is to use a small number of manually annotated corpora such as the same data set", "cit": "[CLS] the attempts to model arabic morphology in a two-level system #otherefr finite state model, #refr two-level model and kiraz?s"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] recent research on statistical machine translation #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the em algorithm #refr with the", "cit": "[CLS] such a forest has a structure of a hypergraph #otherefr; #refr, where items like pp3,6 are called nodes, and"}
{"pre": "[CLS] we use the stanford ner tagger #refr to extract named entities from the text and extract named entities from", "cit": "[CLS] this theory is known by different names in many nlp applications: brown et al #otherefr used ?cautious? decision list"}
{"pre": "[CLS] the most popular approach to paraphrase generation has been the extraction #otherefr; #refr. [SEP] input text is based on", "cit": "[CLS] 1989; gale & church 1991; brown et al 1991; chen 1993), and coarser approaches when sentences are difficult to"}
{"pre": "[CLS] in this paper, we present a system that uses a combination of the grammar to model of #refr. [SEP]", "cit": "[CLS] semantichead-driven generation (shieber et al, forthcoming; #refr uses semantic heads and their complements as a locus of semantic locality."}
{"pre": "[CLS] we used the stanford parser #refr to extract the syntactic parse trees for the sentence in the sentence and", "cit": "[CLS] it uses existing nlg components, including the exemplars text planning framework #refr and the realpro syntactic realizer #otherefr. [SEP]"}
{"pre": "[CLS] the generation of referring expressions is a subject of research in natural language generation (nlg) and has been on", "cit": "[CLS] we evaluated the rdt generation model by comparing its performances with another system also competing in the give challenge"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be np-hard #refr. [SEP] s", "cit": "[CLS] to quantize, we use the binning method #refr that sorts values, divides into equally sized bins, and averages within"}
{"pre": "[CLS] #refr use a similar approach to identify the polarity of words in a sentence. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] #refr contribute an unrestricted metaphor corpus and propose a method based on tree kernels. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to paraphrase generation has been the source sentences into a target language syntax based on", "cit": "[CLS] using large, unrelated english and german corpora (with 163m and 135m words) and a small german-english bilingual dictionary (with"}
{"pre": "[CLS] the task of identifying the semantic textual similarity of a target word is a target word is a word", "cit": "[CLS] however, no explicit word sense inventory is required for lexical substitution #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be a difficult problem #otherefr;", "cit": "[CLS] better smt performance could be obtained with a system based on morphemes, see for instance #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the second approach is to use a maximum entropy model #otherefr; #refr. [SEP] model for the probability distribution over", "cit": "[CLS] in this work, we remain consistent with their work, using the head-finding rules of #refr, and the same binarization"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? 2008. [SEP]", "cit": "[CLS] the most notable work targeting implied predicate-argument relationships is the 2010 semeval task of linking events and their participants"}
{"pre": "[CLS] the system used in this paper is based on the stanford ner tagger #refr. [SEP] parser #otherefr shared task", "cit": "[CLS] the conll-2011 shared task #refr is concerned with intra-document coreference resolution in english, using ontonotes corpora. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] #refr proposed a method for learning a word sense disambiguation which is a word sense disambiguation method to extract", "cit": "[CLS] continuous semantic space representations have proven successful in a wide variety of nlp and ir applications, such as document"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of meaning that are not", "cit": "[CLS] recent work addresses this issue indirectly with the development of specialized models that represent word meaning in context #otherefr;"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a similar to the one used in the same textual entailment", "cit": "[CLS] to obtain the semantic distances between nouns and verbs for the calculation of the distance between exemplars (see equation"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within the source sentence into a target sentence into", "cit": "[CLS] analogical learning has been proven effective in translating unseen words #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the semantic textual similarity of the words in the context of a sentence.", "cit": "[CLS] the result is multiword terms #otherefr and endocentric, the compound is a hyponym of its head #refr. [SEP] [PAD]"}
{"pre": "[CLS] the semantic ontribution of a given input sentence, we have chosen to the semantic representation of the semantic representation", "cit": "[CLS] some of the simpler cases considered below (e.g. substituting a contextually-linked homophone) might fit naturally into a nlg system"}
{"pre": "[CLS] the unsupervised pos induction task has been extensively studied in the nlp community, and has been studied in the", "cit": "[CLS] there is a number of both unsupervised morphology learning systems that use ?raw? wordforms as training data #refr and"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? ? ? ?", "cit": "[CLS] structures the type of a target argument strongly depends on the type and number of the predicate?s arguments1 #otherefr;"}
{"pre": "[CLS] the model is trained using the giza++ implementation of ibm model 4 alignments #refr. [SEP] model 4 alignments were", "cit": "[CLS] brown et al #otherefra) stopped after only one iteration of em in using model 1 to initialize their model"}
{"pre": "[CLS] the second approach is to use a word sense disambiguation algorithm which is based on the most similar to", "cit": "[CLS] we use a sentence segmentation program #otherefr and a pos tagger #refr to segment the tokens surrounding into sentences"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking approaches #otherefr; #refr to learning for learning", "cit": "[CLS] thus, strong experimental results are often achieved by initialization techniques #otherefr; #refr, incremental dataset use #otherefr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the first is the one of the most popular methods for automatic acquisition of lexical acquisition #otherefr; #refr. [SEP]", "cit": "[CLS] it is well known that the different contexts that distributional semantics catches do not always directly refer to what"}
{"pre": "[CLS] #refr proposed a method for identifying word senses in a word sense disambiguation in a word sense disambiguation which", "cit": "[CLS] for example, in question answering, paraphrases from bilingual parallel corpus were used to expand the original questions #otherefr; in"}
{"pre": "[CLS] the task of identifying the semantic textual similarity of a semantic textual similarity of a text is a text", "cit": "[CLS] some authors have already designed similar matching techniques, such as the ones described in #refr and #otherefr. [SEP] [PAD]"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to learn a translation probabilities from", "cit": "[CLS] translation model parameters were chosen using mira #otherefr to optimize bleu #refr on a held-out development set. [SEP] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] producing a ?coarse? chart as efficiently as possible is thus crucial #otherefr; #refr, making these factorizations particularly useful. [SEP]"}
{"pre": "[CLS] the idea of using distributional semantics to measure the similarity between words in a target language is known as", "cit": "[CLS] guevara #otherefr and #refr explore a full form of the additive model (fulladd), where the two vectors entering a"}
{"pre": "[CLS] #refr proposed a method for identifying the polarity of a document by a phrase based on a phrase based", "cit": "[CLS] query logs have become an important resource for many nlp applications such as class and attribute extraction #otherefr, paraphrasing"}
{"pre": "[CLS] the model is trained with the averaged perceptron algorithm #refr. [SEP] model with the best results in the best", "cit": "[CLS] table 10: comparison of our best result #otherefr 91.5 #refr standard 92.02 huang and sagae #otherefr 93.79 our results."}
{"pre": "[CLS] the task of identifying the event descriptions of event extraction and event descriptions has been addressed in particular in", "cit": "[CLS] the most relevant research topic is the temporal information extraction #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to parsing is to use a linear model trained on a maximum spanning tree (mst)", "cit": "[CLS] generative models based on lexicalized pcfgs enjoyed great success as the machine learning framework for full parsing #otherefr; #refr,"}
{"pre": "[CLS] #refr use a similar approach to compute sentence importance. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] providing such fine-grained annotations would enrich information extraction, question answering, and corpus exploration applications by letting users see who"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for each sentence in the sentence with", "cit": "[CLS] the supervision was either given in the form of meaning representations aligned with sentences #otherefr; ge and #refr or"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a text with a text that are semantically", "cit": "[CLS] the features used in this work are typical for modern memm pos tagging and are mostly based on work"}
{"pre": "[CLS] the first is the task of identifying the semantic relations between nominals of noun phrases and the verb classes", "cit": "[CLS] distributed representations are useful in capturing such meaning for individual words #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of identifying the semantic relations between nominals of noun phrases and their arguments are also key to", "cit": "[CLS] automated part of speech tagging #refr is a useful technique in term extraction #otherefr, a domain closely related to"}
{"pre": "[CLS] the generation system is based on the inprotk project #refr. [SEP] system, which is a set of possible antecedents", "cit": "[CLS] a language-independent underlying knowledge representation: knowledge represented as ai plans [ro?sner and stede, 1994] [#refr], [paris and vander linden,"}
{"pre": "[CLS] we use the stanford pos tagger #refr to tokenize the english and then extract the parallel sentences and then", "cit": "[CLS] more recent domain adaptation methods employ corpus or instance weights to promote relevant training examples #refr or do more"}
{"pre": "[CLS] we use the berkeley parser #refr to obtain the syntactic parse trees for each sentence and the english and", "cit": "[CLS] we selected 580 short sentences of length at most 50 characters from the 2002 nist mt evaluation test set"}
{"pre": "[CLS] in the context of machine learning techniques have been applied to many natural language processing tasks, including machine translation", "cit": "[CLS] various machine learning strategies have been proposed to address this problem, including semi-supervised learning #otherefr; #refr, self-taught learning #otherefr,"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] in the work described in this paper, we employ the xle platform using the grammars available for english and"}
{"pre": "[CLS] in recent years, there has been a growing interest in the task of generating referring expressions, reflected in the", "cit": "[CLS] similar gender effects are also reported by other studies on multimodal output presentation, e.g. #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the translation model is a log-linear model #refr. [SEP] model weights ? = ? ? ? ? ? ?", "cit": "[CLS] the lmbr decision rule in #refr has the form e? = argmax e??e { ?0|e?|+ ? u?n ?u#u(e?)p(u|e) },"}
{"pre": "[CLS] we use the averaged perceptron algorithm #refr to train the parameters of the averaged perceptron algorithm #otherefr. [SEP] [SEP]", "cit": "[CLS] recently, the integer programming framework has been widely adopted by researchers to solve other nlp tasks besides pos tagging"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a text with a text corpus and then", "cit": "[CLS] we then parsed each sentence using the pcfg parser of huang and harper #otherefr; #refr, for the tree structure"}
{"pre": "[CLS] we use the averaged perceptron algorithm #refr to train the parameters of the averaged perceptron algorithm with the averaged", "cit": "[CLS] our lookahead method is orthogonal to more elaborate techniques for determining the order of actions such as easy-first tagging/parsing"}
{"pre": "[CLS] we use the berkeley parser #refr to obtain the syntactic parse trees for each sentence and the english and", "cit": "[CLS] we word-aligned the training data using giza++ with refinement option ?grow-diag-and? #otherefr. we applied the algorithm of #refr to"}
{"pre": "[CLS] the most common approach to paraphrase generation is to use a lexicon to paraphrase database into a target language", "cit": "[CLS] bilingual lexicon extraction from comparable corpora has received considerable attention since the 1990s #otherefr; #refr, among others). [SEP] [PAD]"}
{"pre": "[CLS] the first is the automatic evaluation of the automatic metric of the automatic evaluation of the automatic evaluation of", "cit": "[CLS] a thorough description of the task can be found in #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be a given by using", "cit": "[CLS] the improvements in end-to-end performance due to these additions in a french-to-english translation task are described elsewhere #refr. [SEP]"}
{"pre": "[CLS] the most common approach to paraphrase generation is to paraphrase generation from corpora #otherefr; #refr. [SEP] words to a", "cit": "[CLS] the work of #refr has shown how the monolingual context of a sentence to paraphrase can be used to"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to translate the phrases to be", "cit": "[CLS] hayashi et al#otherefr explored the word-based reordering model by #refr in hierarchical translation. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of event extraction was first introduced by #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] #refr define the trigger as the first new article that mentions an event, which is easier than to find"}
{"pre": "[CLS] the most popular approach to translation lexicons from the source language side #otherefr; #refr is to target language side", "cit": "[CLS] another line of work focuses on cache-based adaptive models #refr, which lets lexical choice in a sentence be informed"}
{"pre": "[CLS] #refr use a supervised classifier to detect sentiment words in a target words in a target word and use", "cit": "[CLS] sst-1: stanford sentiment treebank?an extension of mr but with train/dev/test splits provided and fine-grained labels (very positive, positive, neutral,"}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders and predict the polarity of words in a sentence and", "cit": "[CLS] graph-based methods #otherefr; #refrb) have also been proposed to rank sentences or passages based on the pagerank algorithm or"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] as shown by #refr, in many cases ccg is context-free, making it an ideal fit for our problem. [SEP]"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of possible senses in the", "cit": "[CLS] in this paper we use an unsupervised method to rank word senses from an inventory according to prevalence #refra),"}
{"pre": "[CLS] we use the stanford ner tagger #refr to extract named entities from the text and extract named entities from", "cit": "[CLS] for this, we construct feature sets similar to #refr and zhou #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] brown, lai and mercer #otherefr used word count as the sentence length, whereas gale and church #refr used character"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for learning and beam-search decoding for", "cit": "[CLS] jiang and #refr resort to a dynamic programming procedure to search for a completed projected tree. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a bootstrapping method to identify opinion holders and extract opinion expressions from documents in a target words", "cit": "[CLS] #refr also work with arguments. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to word alignment is the ibm models #otherefr; #refr. [SEP] model 1 and the hmm", "cit": "[CLS] in machine translation (mt), dictionaries can help in the domain adaptation setting (daume iii and #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] #refr proposed a method for learning semantic inference based on a set of sentences paired with their meaning representation", "cit": "[CLS] this model is very similar to the one used by #refr in the context of text summarization. w (the"}
{"pre": "[CLS] the model is trained using the averaged perceptron algorithm #refr. [SEP] [SEP] score #refr with the best alignment model", "cit": "[CLS] in most applications like #refr, a sequential model is used. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the second international chinese word segmentation bakeoff #otherefr; #refr and the best results are achieved by the best", "cit": "[CLS] more recently, em has been used to learn hidden variables in parse trees; these can be head-child annotations #refr,"}
{"pre": "[CLS] #refr proposed a method for learning semantic relations between pairs of sentences to identify the words in a sentence.", "cit": "[CLS] earlier work in this regard is a paper by #refr, who proposes that the semantics of a word is"}
{"pre": "[CLS] the task of identifying the semantic textual similarity of a text is a text of a text that of", "cit": "[CLS] although systems that can answer why-questions are emerging, they tend to have limitations in that they can answer questions"}
{"pre": "[CLS] the first is the case for the parsing of the english lexical substitution task #refr. [SEP] grammars in the", "cit": "[CLS] section 3 applies metarules #otherefra; #refr in recognizing these correspondences using standard context-free grammars. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to paraphrase generation is to use a lexicon to paraphrase generation #otherefr; #refr. [SEP] words", "cit": "[CLS] suppose that we have sample terms of a specific domain/topic, then the technical terms that are to be listed"}
{"pre": "[CLS] the first is the automatic evaluation of discourse structure of discourse structure of discourse structure of discourse structure of", "cit": "[CLS] early semantic parsing work made use of fully supervised training #otherefr; ge and #refr, but more recent work has"}
{"pre": "[CLS] the most popular approach to translation lexicons from the target language model is the target side of the target", "cit": "[CLS] possible applications for confidence measures include ? post-editing, where words with low confidence could be marked as potential errors,"}
{"pre": "[CLS] the first is the task of identifying the semantic textual similarity of a text of a text segments of", "cit": "[CLS] as our wsd system for this experiment, we used it makes sense (ims), a state-of-the-art supervised wsd system #refr."}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] since the original ibm paper, there has been a large amount of research exploring the original ibm models and"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of meaning that are not", "cit": "[CLS] the snapshot was tagged with the stanford part of speech tagger #refr and parsed with the malt parser #otherefr."}
{"pre": "[CLS] the task of relation extraction has received much attention in recent years #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] the first utilizes a set of carefully selected features obtained from different levels of text analysis, from part-of-speech (pos)"}
{"pre": "[CLS] we use the stanford dependency parser #refr to obtain the dependency parse trees for each sentence with dependency trees", "cit": "[CLS] taking account for the integration with smt systems, these methods can be divided into two different kinds of approaches"}
{"pre": "[CLS] the system used in this paper is based on the c&c parser #otherefr and the stanford parser #refr. [SEP]", "cit": "[CLS] dependencies in multiple languages #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] the large size of such grammars #otherefr; #refr ? which do not permit arbitrary probability distributions over the grammar"}
{"pre": "[CLS] in this paper, we propose a discriminative model for training using a discriminative model based on the averaged perceptron", "cit": "[CLS] in such cases, we must take steps to adapt a model trained on the source domain for use in"}
{"pre": "[CLS] the task of relation extraction has received much attention in recent years #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] sonex, ollie, patty, treekernel, swirl, lund and our two variants of our method, ex- emplar, explained in detail in"}
{"pre": "[CLS] the most popular approach to translation models #otherefr; #refr rely on parallel data #otherefr. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] word-aligned corpora have been found to be an excellent source for translation-related knowledge, not only for phrase-based models #otherefr;"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] our preprocessing pipeline utilized cleartk?s wrappers for clearnlp?s #refr tokenizer, lemmatizer, part-of-speech (pos) tagger, and dependency parser. [SEP] [PAD]"}
{"pre": "[CLS] the model is trained with the averaged perceptron algorithm #refr. [SEP] score = ? ? ? ? ? ?", "cit": "[CLS] then, the previous discriminative constituent parsing models #refr; henderson, 2004; taskar et al, 2004; petrov and klein, 2008a; ?the"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] statistical machine translation #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the system uses a log-linear model trained on the wall street journal (wsj) corpus of #refr to maximize the", "cit": "[CLS] it combines online peceptron learning #refr with a parsing model based on the eisner algorithm #otherefr, extended so as"}
{"pre": "[CLS] the model is trained using the giza++ implementation of ibm model 4 alignments #refr. [SEP] model 4 alignments #otherefr", "cit": "[CLS] these include hierarchical models #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the semantic role labeling of the predicate-argument structure of the input text with", "cit": "[CLS] to prepare the corpus for parsing with the tsg we therefore tagged it with #refr tagger and since this"}
{"pre": "[CLS] the task of identifying the semantic textual similarity of a target word is a given a semantic similarity of", "cit": "[CLS] the procedure uses a collection of web documents and applies some isa extraction patterns selected from #refr. [SEP] [PAD]"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr. [SEP]", "cit": "[CLS] it will also be relevant to apply advanced statistical models that can incorporate various useful information to this task,"}
{"pre": "[CLS] the bionlp 2011 shared task #refr focuses on event extraction and event extraction and event extraction and event extraction", "cit": "[CLS] #refr and ji and grishman #otherefr incorporate global information by enforcing event role or label consistency over a document"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] event and semantic relations", "cit": "[CLS] we use semgrex patterns to match the input text to dependencies #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the penn treebank #otherefr. [SEP] score", "cit": "[CLS] as #refr show, augmenting this knowledge with handcrafted prototype ?seeds? can bring strong improvements. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the semantic textual similarity of a text of a text of a", "cit": "[CLS] clarke et al #otherefr and #refr trained systems on question and answer pairs by automatically finding semantic interpretations of"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with a beam search", "cit": "[CLS] such models are commonly referred to as edge-factored since their parameters factor relative to individual edges of the graph"}
{"pre": "[CLS] the first is the case for the automatic acquisition of subcategorization frames described in #refr. [SEP] is the number", "cit": "[CLS] this built-in transcategoriality strongly facilitates applications such as interlingual mt, as it renders vacuous many problems connected with category"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for each sentence in the training corpus.", "cit": "[CLS] when dependencies between the tasks can be formulated in terms of constraints between their outputs, a simpler approach is"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a text with a text corpus consisting of", "cit": "[CLS] for example, the path (path) measure is the inverse of the shortest path length between two word senses in"}
{"pre": "[CLS] #refr proposed a method for learning a word sense disambiguation which is a word sense disambiguation method to identify", "cit": "[CLS] in the biomedical domain, ner has focused on identification of biological entities such as genes and proteins #refr. [SEP]"}
{"pre": "[CLS] #refr proposed a method for extracting relations from wikipedia articles from the web by using a large corpus of", "cit": "[CLS] meanwhile, submodular maximization has recently been applied to the text summarization task, and the methods thereof have performed very"}
{"pre": "[CLS] the task of semantic relation extraction has received a lot of attention in recent years #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] one school of extraction techniques concentrates on detecting the boundary of interesting entities in the text, #otherefr; #refr. [SEP]"}
{"pre": "[CLS] the most common approach to translation is to use a log-linear translation model #refr and a log-linear combination of", "cit": "[CLS] ibm models 1-5 #otherefr, recent improvements on the ibm models #refr, and the hmm algorithm described in #otherefr. [SEP]"}
{"pre": "[CLS] we use the moses toolkit #refr to train the weights for the feature weights for each model with the", "cit": "[CLS] future research should apply the work of blunsom et al #otherefr and #refr, who marginalize over derivations to find"}
{"pre": "[CLS] the second approach is to use mismatches in which the same lexical and syntactic features as the context #refr.", "cit": "[CLS] in many applications, researchers have shown that more data equals better performance #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the system used for wsd in the experiments was performed on the semeval 2007 task #refr. [SEP] [SEP] [SEP]", "cit": "[CLS] additionally, a domain adaptation technique of using domain predominant senses #refr is explored, but our primary goal is concerned"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a task of semantic relations between words and phrases and the", "cit": "[CLS] corpus-based word sense disambignation algorjthm~ such as #otherefr; #refr relied on supervised learning fzom annotated corpora. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? 2008. [SEP] ?", "cit": "[CLS] typed phrases 15 million triples #otherefr?)) 4freebase associates each entity with a set of types using the type property."}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] #refr exploited semantic features derived from the web to create a statistical model to detect unreliable parses produced by"}
{"pre": "[CLS] the model is trained using the averaged perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr.", "cit": "[CLS] ubl #refr is a semantic parser based on restricted higher-order unification with ccg #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal (wsj) corpus #refr. [SEP] trees and the penn", "cit": "[CLS] that is, most parsing algorithms assume that the test corpus was generated by the model, and then attempt o"}
{"pre": "[CLS] we use the stanford parser #refr to extract dependency relations for each sentence and parse trees from the sentence", "cit": "[CLS] negotiation annotation scheme #refr to identify utterances as either giving or receiving information. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within the source sentence into a target sentence using", "cit": "[CLS] on more general cross-language information transfer, #refr proposed and evaluated a method of propagating pos tagging, named mention, base"}
{"pre": "[CLS] the most popular approach to mt system combination is the one proposed by #refr, which uses a log-linear combination", "cit": "[CLS] much of this work has focused on up-weighting subsets of the training or language modeling data that are most"}
{"pre": "[CLS] we use the minimum error rate training (mert) #refr to tune the feature weights for the feature weights of", "cit": "[CLS] we use giza++ #otherefr, a suffix-array #refr, srilm #otherefr17 to obtain word alignments, translation models, language models, and the"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] #refr substantially reduce model size with a filtering method. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to word alignment is the one proposed by #refr. [SEP] model is to use a", "cit": "[CLS] initially, this was motivated by statistical machine translation #otherefr, but wordaligned texts have also been used to transfer linguistic"}
{"pre": "[CLS] the model is based on the simple and effective model of #refr. [SEP] score a given sentence pair that", "cit": "[CLS] for this task, normally supervised methods are used #refr, which require sufficient labeled training data. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the one of the most popular approach to use crowdsourcing, which has been to use a", "cit": "[CLS] part of speech taggers typically require input in the format of a single sentence per line (for example brill"}
{"pre": "[CLS] the first is the process of annotating the discourse structure in text with discourse structure in text with discourse", "cit": "[CLS] the task is of theoretical interest because disentanglement is a necessary pre-requisite to making sense of narrative texts, an"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a task of semantic relations between words and phrases and syntactic", "cit": "[CLS] it is one of the main reason for the performance drop of supervised srl systems in out-of-domain scenarios #otherefr"}
{"pre": "[CLS] the unsupervised method used by #refr is based on the statistics of the words in a dictionary of a", "cit": "[CLS] #refr information-theoretic similarity measure is commonly used in lexicon acquisition tasks and has demonstrated good performance in unsupervised wsd"}
{"pre": "[CLS] the second approach is to use a small number of manually annotated corpora such as the same data set", "cit": "[CLS] however, to compare with related work, we will also adopt boundary f-measure fb = 2rbpb/(rb + pb), where the"}
{"pre": "[CLS] the translation model is a log-linear model #refr. [SEP] model with the decoder that we use the decoder that", "cit": "[CLS] we plan to implement lexicalized reordering in future work; without this, the test system is 0.53 bleu #refr point"}
{"pre": "[CLS] the first is the case for the word alignment of word alignment of word alignment of word alignment of", "cit": "[CLS] 67-83], for which there is as yet no universally accepted nomenclature. multi-word #refr: when title indices and catalogs, subject"}
{"pre": "[CLS] the second approach is to use a very small number of manually annotated corpora such as the same data", "cit": "[CLS] first, we investigate the impact of using different flavours of covington?s algorithm #otherefr for nonprojective dependency parsing on the"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] licence details: http://creativecommons.org/licenses/by/4.0/ mooney, 1996; ge and #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained with a maximum entropy model #otherefr, which is trained on the penn treebank wall street", "cit": "[CLS] we use the general framework of hidden variable crfs #otherefr; #refr, where gradient-based optimization maximizes the likelihood of the"}
{"pre": "[CLS] the task of segmenting utterances into words, for example, johnson and colleagues have been shown to be useful in", "cit": "[CLS] in recent years, many supervised machine learning techniques for answer selection in open-domain question answering have been investigated in"}
{"pre": "[CLS] the semantic relation between nominals in the verb and the verb frame acquisition system has been used in the", "cit": "[CLS] recently, two events #otherefr; #refr targeted negation mostly on those subfields. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the moses toolkit #refr to train the weights for the feature weights for each model with the", "cit": "[CLS] lattices and forests can also be used in minimal error rate training and minimum bayes risk decoding phases #otherefr;"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] shared", "cit": "[CLS] this bionlp st 2009 formulation of the event extraction task was followed also in three 2011 main tasks: the"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr. [SEP]", "cit": "[CLS] most of the research in multimodal language processing over the past decade fits within two main trends that have"}
{"pre": "[CLS] the second approach is to use a word similarity measure and the similarity measure proposed by #refr. [SEP] score", "cit": "[CLS] furthermore, the simplified model has been shown to be equivalent to the models of #refr and thater et al"}
{"pre": "[CLS] the first is the same as #refr, which was used to create a dependency parsed with the berkeley parser", "cit": "[CLS] for accomplishing the conll 2005 shared task on semantic role labeling #otherefr, we capitalized on our experience on the"}
{"pre": "[CLS] the translation quality is evaluated by case-insensitive bleu-4 #refr. [SEP] quality measured by bleu score #refr with the bleu", "cit": "[CLS] carpuat and wu #otherefr integrated the translation predictions from a chinese wsd system #refr into a chinese- english word-based"}
{"pre": "[CLS] we use the averaged perceptron algorithm #refr to train the parameters of the feature weights for the log-linear models.", "cit": "[CLS] #refr use the phrasal cohesion of a dependency tree as a constraint on a beam search aligner. [SEP] [PAD]"}
{"pre": "[CLS] the system used in this paper is based on the c&c parser #otherefr and the stanford parser #refr. [SEP]", "cit": "[CLS] we rely on the stanford parser #refr, a treebank-trained statistical parser, for tokenization, part-of-speech tagging, and phrase-structure parsing. [SEP]"}
{"pre": "[CLS] the first is the automatic evaluation of the automatic metric of the automatic evaluation of the automatic evaluation of", "cit": "[CLS] another approach, presented by #refr, consists in generating coherent summaries that are shorter than a single sentence. [SEP] [PAD]"}
{"pre": "[CLS] the learning algorithm used in this paper is a variation of the winnow update rule based statistical parsing algorithm", "cit": "[CLS] unsupervised baseline: we are using a version of the unsupervised semantic role induction system of #refra) adapted to setup"}
{"pre": "[CLS] the most common approach to statistical machine translation is to use a log-linear translation system #otherefr; #refr. [SEP] model", "cit": "[CLS] for rule extraction we use moses #refr implementation of ghkm #otherefr, we apply the same rule extraction tool to"}
{"pre": "[CLS] the first is the case for the syntactic analysis of the english lexical substitution task of the penn treebank", "cit": "[CLS] 97- 108)); maximum:entropy model (see #refr). [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] this entailed two sets of experiments: in study #1, we tested whether single alternations of simple lateen em (as"}
{"pre": "[CLS] the generation of referring expressions is a subject of research in natural language generation #otherefr; #refr. [SEP] [SEP] is", "cit": "[CLS] slotfilling domains have been the subject of most of this research, with the exception of work on troubleshooting domains"}
{"pre": "[CLS] the first is the task of converting natural language processing #otherefr; #refr. [SEP] [SEP] to be able to help", "cit": "[CLS] while we are not the first to employ prosodic cues in a statistical parsing model, previous efforts #otherefr; #refr"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] the", "cit": "[CLS] the list of opinion words comes from the mpqa subjectivity lexicon #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for each sentence in the training corpus.", "cit": "[CLS] in this paper we consider the structured perceptron #refr ? with pos tagging as our practical application. [SEP] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the sentence planning problem that the user is relevant to a text", "cit": "[CLS] as a result of this mismatch, recent work motivated by spoken dialogue applications has started to use naturally-occurring speech"}
{"pre": "[CLS] the segmentation model is trained with the perceptron algorithm described by #refr. [SEP] score #refr with the best results", "cit": "[CLS] the evaluation on this task is motivated by the fact that #refr showed that good-quality morphological preprocessing can improve"}
{"pre": "[CLS] #refr use a probabilistic model to predict the probability of a parse tree in a sentence by a probability", "cit": "[CLS] #refr propose an account for constraints on topic selection based on probabilistic content models. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second dataset is the same as the use of the same as the same as the same as", "cit": "[CLS] we adopt a two-part evaluation setting used in previous semeval wsi and wsd tasks #otherefr; #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the semantic relations between nominals of a text and a text and", "cit": "[CLS] a typical example of the second category is the s-space package #refr, which defines a complete pipeline for building"}
{"pre": "[CLS] the first is the one of the most popular approach to use a word in a word in which", "cit": "[CLS] in order to detect automatically tile text genre we used discriminant analy.vis, a well-known classification technique of lnultivariate statistics"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? ? ? ?", "cit": "[CLS] quality assessment of a learned model output was explored by many previous works #otherefra; #refr, machine translation #otherefr. [SEP]"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to learn a translation models from a", "cit": "[CLS] arabic text is then segmented with amira #refr according to the atb scheme7. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of converting natural language processing #otherefr; #refr. [SEP] [SEP] to be a given a", "cit": "[CLS] although there are a handful of corpora that include punctuation errors in their annotation scheme, such as nucle #otherefr"}
{"pre": "[CLS] #refr use a similar approach to compute sentence importance. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] in this study, we focus on extractive summarization #refr, in particular, on sentence selection from a given set of"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] we used giza++ #refr to perform word alignment in both directions, and grow-diag-final-and #otherefr to generate symmetric word alignment."}
{"pre": "[CLS] the first is the case for the automatic acquisition of subcategorization frames from corpora #refr, which is the annotated", "cit": "[CLS] the accuracy of mwe acquisition systems can be further improved by combining our morphological and syntactic features with semantically"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] the exception is here the joint approach of marcu and wong #otherefr, but its refinement by #refr again relies"}
{"pre": "[CLS] the second approach is to use a small number of seed examples for example outputs and unsupervised learning of", "cit": "[CLS] the development data was 200 sentences of labeled biomedical oncology text (bio, the onco portion of the penn biomedical"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] sentence and thus a", "cit": "[CLS] dialogue acts are used to describe the function or role of an utterance in a discourse, and have been"}
{"pre": "[CLS] the first is the case for the generation of referring expressions that we will use the rst bank #refr", "cit": "[CLS] u t te rances we have already mentioned that speech repairs constitute a good benchmark for studying the generation"}
{"pre": "[CLS] #refr proposed a method for learning a word sense disambiguation which is a word sense disambiguation method to extract", "cit": "[CLS] unsupervised approaches proposed so far have involved a number of techniques, including language modeling #refr, graph-based ranking #otherefr. [SEP]"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] of a text with", "cit": "[CLS] in linguistic constructs uch as conjunctions, which impose constraints on the semantic orientation of their arguments #otherefr; #refr, the"}
{"pre": "[CLS] the first is the stanford parser #refr and then converted to dependency structures for english sentences. [SEP] [SEP] [SEP]", "cit": "[CLS] for more information, see #refr; agarwal et al., 2013a; agarwal et al., 2013b). [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the generation of referring expressions from the semantic parsing of the english grammar", "cit": "[CLS] they induce a probabilistic tree adjoining grammar from a training set algning frames and sentences using the grammar induction"}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders and predict the polarity of words in a sentence and", "cit": "[CLS] quantitatively, subjective sentences in the product reviews amount to 78% #refr, while subjective sentences in the movie review dataset"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the longrange reordering model is to use a word", "cit": "[CLS] our work is also closely related to other recent work on learning probabilistic models involving structural latent variables #otherefr;"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a task of identifying the degree of semantic relations between words", "cit": "[CLS] unlike other lft proposals #otherefr; #refr, transforming sentences into logic forms is a straightforward step, the quality of the"}
{"pre": "[CLS] the system is based on the moses toolkit #otherefr and the moses toolkit #refr. [SEP] test set for the", "cit": "[CLS] #refr compare single layer to multi layer perceptron #otherefr define the problem as a word sequence labeling task and"}
{"pre": "[CLS] the second approach is to use a small number of manually annotated corpora such as the same data set", "cit": "[CLS] we use the following baseline set of features from #refr: . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the moses decoder #refr. [SEP] score #refr with the decoder that the best derivation", "cit": "[CLS] different from the soft constraint modeling adopted in #otherefr; #refr, our approach encodes syntactic information in translation rules. [SEP]"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be np-hard #refr. [SEP] s", "cit": "[CLS] the last row of tabel 4 shows that if we remove ?null? in label annotation, the performance is degraded."}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for each sentence in the training corpus.", "cit": "[CLS] then we use the standard minimum error-rate training #refr to tune the feature weights to maximize the system.s bleu"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for dependency parsing #refr. [SEP] parser", "cit": "[CLS] semi-supervised dependency parsing has attracted a lot of attention recently #otherefr; #refr, but there has, to the best of"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the sentence in", "cit": "[CLS] branavan et al. #otherefr and #refr work in the domain of computer technical support instructions, mapping language to actions"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] recently, the gda corpus #otherefr and naist text corpus #refr were constructed in japanese, and these corpora have become"}
{"pre": "[CLS] the first is the case for the semantic role labeling of syntactic structure of the predicate-argument structure of the", "cit": "[CLS] besides hard algorithms there have also been studies to soft clustering (pereira, tishby, and #refr where the distribution of"}
{"pre": "[CLS] the system is based on the stanford parser #refr and the stanford parser #otherefr. [SEP] system #otherefr. [SEP] system", "cit": "[CLS] the resulting dependency bank was then merged with nombank #refr and propbank #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] this measure is considered appropriate for graded annotation tasks in general #refr, and has also been used for analysing"}
{"pre": "[CLS] the system uses the stanford ner tagger #refr to extract the stanford parser for the best parse the stanford", "cit": "[CLS] to do this, the feature definitions are used to extract the feature values of the links #refr. [SEP] [PAD]"}
{"pre": "[CLS] #refr proposed a method for extracting relations between pairs of sentences from documents by extracting entities from documents by", "cit": "[CLS] three main approaches for collecting paraphrases were proposed in the literature: manual collection, utilization of existing lexical resources, and"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for english and the english sentences. [SEP]", "cit": "[CLS] third, we hope that the improved parses of bitext will serve as higher quality training data for improving monolingual"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the sentence in", "cit": "[CLS] first, the approach demands a precise semantics for connectives, as in the work of #refr, grote et al #otherefr."}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? 2008. [SEP] ?", "cit": "[CLS] works that apply symmetric patterns in their model generally require expert knowledge in the form of a pre-compiled set"}
{"pre": "[CLS] the system uses a log-linear model trained on the wall street journal (wsj) corpus of #refr to maximize the", "cit": "[CLS] #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] sentence and thus a", "cit": "[CLS] research into frameworks for temporal expression extraction in the computational sciences, #otherefr, str?tgen and #refr, sun et al. #otherefr)"}
{"pre": "[CLS] the most popular approach to paraphrase generation has been the source sentences into either by using a monolingual target", "cit": "[CLS] #refr built finite state automata (fsa) from semantically equivalent translation sets based on syntactic alignment and used the fsas"}
{"pre": "[CLS] the model is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the em algorithm #refr with the", "cit": "[CLS] alternative approaches avoid such heuristics, instead learning structured alignment models directly from sentence aligned data (e.g., #refr). [SEP] [PAD]"}
{"pre": "[CLS] the first is the automatic evaluation of discourse structure of discourse structure of discourse structure of discourse relations #otherefr;", "cit": "[CLS] there is also a body of work in areas of creating semantic orientation #otherefr and their use in identifying"}
{"pre": "[CLS] in the literature, supervised learning methods have been applied to the task of learning a small number of unsupervised", "cit": "[CLS] system f1 (precision, recall) #refr, best single, no list 89.94 #otherefr, no list 90.26 (91.00, 89.53) crf baseline, no"}
{"pre": "[CLS] in this paper, we propose a method for learning a dependency parser based on the generative dependency parser of", "cit": "[CLS] this observation has led to a vast amount of research on unsupervised grammar induction #otherefr; #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] for predicting the grammatical features, we used the wapiti toolkit #refr.10 9english/german data released for the 2009 acl workshop"}
{"pre": "[CLS] the first is the polarity of a polarity of a polarity of a polarity of a polarity of a", "cit": "[CLS] sentiment analysis includes a variety of different problems, including: sentiment classification techniques to classify reviews as positive or negative,"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a text that is a", "cit": "[CLS] #refr use extracted knowledge to determine whether a parse of a sentence has a plausible semantic interpretation. [SEP] [PAD]"}
{"pre": "[CLS] in the context of unsupervised wsd has been explored for a number of nlp tasks, such as pos tagging", "cit": "[CLS] in nlp, label propagation has been used for word sense disambiguation #otherefr, sentiment analysis #refr, and relation extraction #otherefr."}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] #refr and suzuki et al. #otherefr use unsupervised wordclusters as features in a dependency parser to get lexical dependencies."}
{"pre": "[CLS] the first is the one of the most popular approach to use a word in which is to use", "cit": "[CLS] this problem is addressed by riloff and shepherd #otherefr, #refr and more recently by widdows and dorow #otherefr. [SEP]"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of meaning that are not", "cit": "[CLS] in recent coarse-grained evaluations, such systems have achieved accuracies of close to 90% #otherefr; #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to mt system combination is the one proposed by #refr, which uses a log-linear combination", "cit": "[CLS] similar observations have been made by previous researchers #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to identify the text in which a text in which a text spans that", "cit": "[CLS] #refr use top-level, composed distributed representations of sentences to guide generation in a machine translation setting. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] we use the averaged perceptron algorithm #refr to train the parameters of the averaged perceptron algorithm with the averaged", "cit": "[CLS] dataset and evaluation measures we evaluate our model on conll dependency treebanks for 14 different languages #otherefr; #refr, using"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a text with a text corpus to be", "cit": "[CLS] prior work on content or attribute selection has used a ?summarize and refine? approach #otherefr; #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? ? ? ?", "cit": "[CLS] thus, acquisition of such knowledge received considerable attention in the last decade #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to extract dependency relations for each sentence and parse trees from the sentence", "cit": "[CLS] linguistic indicators for aspectual classification are also used by #refr, who evaluates 14 indicators to test verbs for stativity"}
{"pre": "[CLS] the weights of the log-linear model are optimized with minimum error rate training (mert) #refr on the development set", "cit": "[CLS] system combination #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first group is a user to use dialogue acts to be able to help users to help users", "cit": "[CLS] there are many studies in respect of the dialogue strategy such as confirmation management using confidence measures of speech"}
{"pre": "[CLS] #refr use a bootstrapping method to detect the polarity of a document to determine the polarity of a given", "cit": "[CLS] previous weakly supervised methods often expand a seed set and identify opinion relation either by co-occurrence statistics #otherefr or"}
{"pre": "[CLS] the most common approach to parsing is to use a parse tree #otherefr; #refr. [SEP] grammars from a treebank", "cit": "[CLS] lastly roark obtained the results we quote here with selective use of left-corner transforms #otherefr; #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford corenlp suite to lemmatize and partof-speech tag each word #otherefr; #refr, and train a coreference", "cit": "[CLS] we employ two scoring programs, b3 #otherefr and ?3-ceaf #refr, to score the output of a coreference model. [SEP]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] we wish to apply this direct, bayesian approach to learn better translation rules for syntaxbased statistical mt #otherefr; #refr,"}
{"pre": "[CLS] the grammar we use is a standard grammar for the grammar development and test and test and test data", "cit": "[CLS] this word graph is sent to the integrated processing module which controls the three parsers (hpsg parser #refr, chunk"}
{"pre": "[CLS] the grammar we are extracted from the treebank using the stanford parser #refr. [SEP] parser #refr with the self-trained", "cit": "[CLS] an existing scfg parser #refr was then used, with a simple unknown word heuristic, to generate the viterbi n-best"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] when building probabilistic models for tag sequences, we often decompose the global probability of sequences using a directed graphical"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] a problem mentioned in #otherefr also used single machine and fewer bits to store the lm probability by using"}
{"pre": "[CLS] in the context of spoken dialogue systems #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] the tools of mate project#refr, which also directs multi-level annotation, can be a good candidate for our project. [SEP]"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] the", "cit": "[CLS] interestingly, the tees classifier has been modified to retrain itself on the grn data and to produce event annotations"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a text that are semantically", "cit": "[CLS] we used the spectral clustering #otherefr which is frequently used in verb clustering experiments #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the semantic parsing of the english language model of the penn treebank #otherefr;", "cit": "[CLS] but see, e.g., #refr, who use different models to predict each part of the triplet for spinal model pruning,"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? ? ? ?", "cit": "[CLS] to make such information available to the tree kernel functions, we rely on the shallow syntactic trees enriched with"}
{"pre": "[CLS] the task of identifying the event extraction of event descriptions has been well studied #otherefr; #refr. [SEP] event extraction", "cit": "[CLS] collective approaches utilize dependencies between different decisions and resolve all ambiguous mentions within the same context simultaneously #otherefr; #refr."}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to learn a translation hypotheses and a", "cit": "[CLS] chiang #otherefr; #refr), and also include systems employing contextfree models trained on parallel text without benefit of any prior"}
{"pre": "[CLS] the work of #refr is similar to ours in that the context of the discourse relation of discourse relations", "cit": "[CLS] a substantial body of work has been done on determining the affect #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best results for the best", "cit": "[CLS] nonparametric bayesian methods produce state-of-the-art performance on this task #otherefra; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to paraphrase generation is to paraphrase generation from multiple translation #otherefr; #refr. [SEP] input text", "cit": "[CLS] several approaches rely on bilingual parallel data #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking approaches #otherefr; #refr to learning for dependency", "cit": "[CLS] table 1 lists a typical set of feature templates chosen from the ones of #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] event and semantic orientation", "cit": "[CLS] like brody and elhadad #otherefr, we then construct a coordination graph that links adjectives modifying the same noun, but"}
{"pre": "[CLS] the second dataset is the same as the use of the same as the same as the same as", "cit": "[CLS] first, this methodology should be applied to additional wsi models, such as graphbased #otherefr; #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] however, the original sd design emphasizes word-tokens and configurational structures, and consequently, these schemes overlook properties and realization patterns"}
{"pre": "[CLS] the second approach is to use a maximum entropy model #otherefr; #refr. [SEP] model to predict the probability of", "cit": "[CLS] we used tagged sentences from the parse trees #refr and followed the standard approach of splitting the ptb, using"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] shared", "cit": "[CLS] despite this observation, many stateof-the-art supervised event extraction models still extract events and event arguments independently, ignoring their underlying"}
{"pre": "[CLS] the system used for the experiments in the senseval-3 english lexical sample task #refr provided the genia corpus #otherefr.", "cit": "[CLS] #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the work of #refr is similar to ours in that the generation of referring expressions is in that the", "cit": "[CLS] much work in the field of natural language processing concerns understanding and resolving these temporal expressions in text #otherefr;"}
{"pre": "[CLS] the first is the one of the most popular approach to use a word in which is to use", "cit": "[CLS] the immense volume of real-time, user-generated microblogs that flows through sites has been shown to have utility in applications"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] co-occurrence frequencies have long been used to resolve linguistic ambiguities #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be np-hard #refr. [SEP] s", "cit": "[CLS] several recent syntax-based models for machine translation #otherefr; #refr can be seen as instances of the general framework of"}
{"pre": "[CLS] the grammar is a synchronous grammar formalism which is a synchronous grammar (scfg) used to parse tree adjoining grammar", "cit": "[CLS] typical solutions involve constraints on the space of permutations, as in multi-document summarisation #otherefr; #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the process of annotating the correct grammatical relations between the words in a sentence and the", "cit": "[CLS] it is important to remember that the goal of this environment is not to replace existing software systems for"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] also the lexicon prior polarities have been proved very useful #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for each sentence in the sentence with", "cit": "[CLS] in recent years, conditional random fields #otherefr have shown success on a number of natural language processing (nlp) tasks,"}
{"pre": "[CLS] the most popular approach to translation models #otherefr; #refr rely on the source language to target language to target", "cit": "[CLS] we based the true-casing on #refr, who changed the case of the first word in each sentence, to the"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the sentences and the syntactic parse trees of the sentences and", "cit": "[CLS] mckp defines an automatic summarization problem as a maximum coverage problem with a knapsack constraint, which uses conceptual units"}
{"pre": "[CLS] the system used in this paper is based on the c&c parser #otherefr shared task #refr. [SEP] [SEP] [SEP]", "cit": "[CLS] whereas the training sizes of the data have not been that different as they were e.g. for the 2007"}
{"pre": "[CLS] the first is the task of identifying the semantic relations between nominals of noun phrases and the semantic relations", "cit": "[CLS] traditionally such semantic knowledge is handcrafted, though some software aids exist to enable greater productivity #otherefr; #refr. [SEP] [PAD]"}
{"pre": "[CLS] the first is the case for the automatic evaluation of the automatic evaluation of the automatic evaluation of the", "cit": "[CLS] the first results of the project have 1http://www.jibbigo.com 2http://emime.org been, for example, to bridge the gap between the asr"}
{"pre": "[CLS] the semantic annotation of text is to be the most frequent sense of the most frequent sense of the", "cit": "[CLS] framenet [#refr] and verbnet both contain the notion of verb groupings. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr proposed a method for identifying the polarity of a document by a document into a document to determine", "cit": "[CLS] we consider the top features of a distributional vector as a topic, and use recent measures for automatically measuring"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the longrange reordering problem as a sequence of words", "cit": "[CLS] although only a small number of previous studies in natural language processing have dealt with coordinations, this does not"}
{"pre": "[CLS] we use the averaged perceptron algorithm #refr to train the parameters of the model for the model parameters. [SEP]", "cit": "[CLS] several non-linear objective functions, such as f-score for text classification #otherefr, and bleu-score and some other evaluation measures for"}
{"pre": "[CLS] the grammar is a grammar that is a given a grammar that is a grammar that of the grammar", "cit": "[CLS] in this paper we present an algorithm for generating sentences using unification categorial grammars #otherefr but which extends to"}
{"pre": "[CLS] the first is the task of identifying the semantic textual similarity of a word is a word is a", "cit": "[CLS] to our knowledge, there is only one other method, recently reported, that disambiguates unrestricted words in texts #refr. [SEP]"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] table 1: the proportion of n-grams that overlap in a corpus of 137k sentence-aligned pairs from simple english wikipedia"}
{"pre": "[CLS] the task of identifying the semantic textual similarity of a document is a text #refr. [SEP] of a text", "cit": "[CLS] #refr adds more linguistic knowledge, such as syntactic features, to enrich term representation, which significantly improves the performance. [SEP]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for each sentence in the training corpus.", "cit": "[CLS] support for any style of synchronous context free grammar (scfg) including syntax augment machine translation (samt) grammars #refr ."}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the source sentence", "cit": "[CLS] drawing on previous successful attempts to relax syntactic constraints during grammar extraction in various ways #otherefr; #refr, we compare"}
{"pre": "[CLS] the grammar is a grammar that is a given a grammar that is a grammar that is a grammar", "cit": "[CLS] for agglutinative languages such as korean, finnish and turkish #refr, dimension (b) is very large, so creating an exhaustive"}
{"pre": "[CLS] the first is the process of annotating the discourse structure in text with discourse structure of a text with", "cit": "[CLS] most work on identification of discourse segments (or other forms of discourse structure in spoken interaction) depends on a"}
{"pre": "[CLS] the system uses a combination of natural language processing (nlp) systems to produce various types of syntactic parsing and", "cit": "[CLS] this approach is essentially the same as #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] the second is that it allows us to impose structural constraints in the solution of the parser, as described"}
{"pre": "[CLS] the task of event extraction has received a lot of attention in recent years #otherefr; #refr. [SEP] [SEP] [SEP]", "cit": "[CLS] meanwhile, relation extraction has also been studied extensively in recent years, ranging from supervised learning methods #otherefr to unsupervised"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] iii and marcu, 2004; toutanova et al, 2004; shen et al, 2003; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the tempeval community focused on the classification of the temporal relations between events and temporal relations and temporal relations", "cit": "[CLS] tempeval 2013 #refr allowed includes relations, but again only in particular constructions or when the relation seemed salient to"}
{"pre": "[CLS] the task of relation extraction has received much attention in recent years #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] semi-supervised relation extraction methods include knowitall #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford named entity recognizer #refr to extract named entities from the text and extract named entities", "cit": "[CLS] with the rise of social media, researchers have sought to induce models for predicting latent author attributes such as"}
{"pre": "[CLS] in the context of machine translation, itg has been explored for a variety of nlp tasks, including pos tagging", "cit": "[CLS] these are the result of unintentional errors, dialectal variation, conversational ellipsis, topic diversity, and creative use of language and"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] hierarchical phrase-based decoding #refr also allows for long range reordering without explicitly modeling syntax. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the translation model is a log-linear model #refr. [SEP] model weights are optimized to maximize bleu score #refr with", "cit": "[CLS] both best 3#refr studied up to 10000-best and show that the use of 1000-best candidates is sufficient for mbr"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] model with the average number of the average", "cit": "[CLS] #refr present a method for learning the parameters of a log-linear ccg parsing model from fully annotated normal?form parse"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be np-hard #refr. [SEP] s", "cit": "[CLS] in current work, rnn has only been verified to be useful on monolingual structure learning #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the task of relation extraction has received much attention in recent years #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] for the task, many machine learning methods have been proposed, including supervised methods #refr, semisupervised methods #otherefr. [SEP] [PAD]"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] as our task is not grounded in real-world relations but in rhetorical ones, constraints found in the context tend"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] while early works #otherefr exploited annotations obtained with automatic mt evaluation metrics like bleu #refr, the current trend is"}
{"pre": "[CLS] the second dataset is the same as the use of the same as the same as the same as", "cit": "[CLS] some approaches for clustering have made use of regular patterns of polysemy among words. #otherefr, and automatic attempts to"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] wiki: 25k articles of english wikipedia abstracts parsed by the #refr parser. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the overall sentence pairs from the same text with a text #refr.", "cit": "[CLS] multi-headedness and disconnected sub-graphs pose greater challenges to dependency parsing, although there has been research done on both #otherefr;"}
{"pre": "[CLS] the system is based on the inprotk toolkit for the best performing system described in #refr. [SEP] score for", "cit": "[CLS] it includes a generic top-level ontology whose purpose is to provide a basic structure of the world, i.e. abstract"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal (wsj) corpus #refr. [SEP] test set, using the", "cit": "[CLS] they have been applied successfully to a wide range of languages, among which french (candito and #refr, german #otherefr."}
{"pre": "[CLS] we use the berkeley parser #refr to obtain the syntactic parse trees for each sentence and the english and", "cit": "[CLS] because of this, much recent work in both word alignment and parsing has focused on changing aligners to make"}
{"pre": "[CLS] in this paper, we propose a method for learning a large scale monolingual parallel corpus of parallel corpus of", "cit": "[CLS] first, it would be useful to combine our method for filtering out non-parallel texts with methods for detecting omissions"}
{"pre": "[CLS] the first is the task of identifying the event extraction of a single document to be a single document", "cit": "[CLS] an ar model that carries out antecedent identification before anaphoricity determination, the decision whether a given np is anaphoric"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] sentence in a text", "cit": "[CLS] as for any generated text, a good summary also requires a text plan #refr #otherefr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the polarity of a text that is a text of a text", "cit": "[CLS] studying relationships between tagged named entities, #refr proposed unsupervised clustering methods that assign given (or semi-automatically extracted) sets of"}
{"pre": "[CLS] the system used in this paper is based on the c&c parser #otherefr and the stanford parser #refr. [SEP]", "cit": "[CLS] the goal of this paper is to address the above issues by using crowdsourcing, which has been proven effective"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to learn a translation probabilities from", "cit": "[CLS] as a result of this, different task on aligments in statistical machine translation have been proposed in the last"}
{"pre": "[CLS] the second approach is to use mismatches and is the syntactic information in the semantic information in the semantic", "cit": "[CLS] we should note that the finite-temperature decoding is quite different from annealing type algorithms or ?zero-temperature decoding?, which correspond"}
{"pre": "[CLS] the first is the case for the syntactic analysis of the english lexical substitution task of the penn treebank", "cit": "[CLS] it must include both primary data collection (as in #refr universal corpus) and analytical work elucidating the linguistic structures"}
{"pre": "[CLS] the first is the task of identifying the event descriptions of the bionlp?09 shared task on event extraction #refr.", "cit": "[CLS] the uncertainty-based approach has been applied to, for instance, named-entity recognition by shen et al. #otherefr who reports 67%"}
{"pre": "[CLS] we use the stanford ner tagger #refr to train the word alignment model with the ibm models of the", "cit": "[CLS] early efforts focus on building separate models #refr and adding features #otherefr to model domain information. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the task of identifying the semantic relations between nominals of noun phrases and their arguments are also key to", "cit": "[CLS] other implementations of the memory-based paradigm for nlp tasks include #refr, for pos tagging; cardie #otherefr, for word pronunciation."}
{"pre": "[CLS] #refr proposed a method for learning semantic inference based on a set of sentences paired with their frequency and", "cit": "[CLS] similar to this construction but used in the community-based question answering (cqa) scenario, #refr represented triples of question title,"}
{"pre": "[CLS] the grammar is a grammar that is a parsing strategy called synchronous grammar (stsg) #refr. [SEP] grammars are not", "cit": "[CLS] in the analysis of the australian free word-order language guugu yimidhirr, mark johnson uses a 'combine' predicate in a"}
{"pre": "[CLS] the first is the automatic evaluation of discourse coherence models in the natural language processing community #otherefr; #refr. [SEP]", "cit": "[CLS] our particular goal is to be able to use an article generator in conjunction with a symbolic generator for"}
{"pre": "[CLS] the task of identifying the semantic textual similarity of a target word is a target word is a given", "cit": "[CLS] in machine translation #refr and text summarization #otherefr, results are automatically evaluated based on sentence comparison. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] in the second international chinese word segmentation bakeoff #otherefr; #refr and the best results are achieved by the best", "cit": "[CLS] czech pos tags were obtained by the following two steps: first, we used ?feature-based tagger? included with the pdt"}
{"pre": "[CLS] #refr use a similar approach to identify the polarity of subjective words in their polarity in their polarity in", "cit": "[CLS] for this work, we use four sentiment scoring categories: the variance, average score, number of negative sentences, and controversiality"}
{"pre": "[CLS] in the literature, supervised learning methods have been applied to unsupervised learning of natural language processing #otherefr; #refr. [SEP]", "cit": "[CLS] unsupervised word segmentation #otherefr; #refr takes advantage of the huge amount of raw text to solve chinese word segmentation"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? ? ?", "cit": "[CLS] #refr present an approach for generating sentence level paraphrases, learning structurally similar patterns of expression from data and identifying"}
{"pre": "[CLS] #refr use a similar approach to identify the polarity of words in a text in a text in a", "cit": "[CLS] both works dispensed with the more complex features proposed by #refr which showed promising results. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the semantic relations between nominals of noun phrases and the semantic relations", "cit": "[CLS] in fact, a considerable number of works on semantic processing implicitly or explicitly presupposes the availability of a lexicon,"}
{"pre": "[CLS] the second approach is to use a maximum entropy model #otherefr; #refr. [SEP] model to predict the probability of", "cit": "[CLS] following #refr, we first calculate the maximum likelihood probabilities p? for unigrams, bigrams, and trigrams as in (1-3). [SEP]"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] the", "cit": "[CLS] as another example, the data sets of the epi task #refr also may be utilized for the newly added"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within the source sentence into a target sentence into", "cit": "[CLS] it also has wide ranging potential societal application as a reading aid for people with aphasis #refr, for low"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? 2008. [SEP] ?", "cit": "[CLS] recent developments aim to lift these limitations, either by reducing the amount of supervision #otherefr; #refr or by increasing"}
{"pre": "[CLS] in this paper, we propose a method for learning semantic parsing by using a large corpus of #refr. [SEP]", "cit": "[CLS] the progress in parsing technology are noteworthy, and in particular, various tatistical dependency models have been proposed#refr,, #otherefr. [SEP]"}
{"pre": "[CLS] the first is the case for the automatic acquisition of subcategorization frames from the web #otherefr; #refr. [SEP] grammars", "cit": "[CLS] we used a morpheme model and a dependency model identical to those proposed by uchimoto et al #otherefr; #refrb)."}
{"pre": "[CLS] the second dataset is the same as the use of the same as the same as the same as", "cit": "[CLS] #refr focus on the task of noun-phrase translation. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for each sentence with dependency trees for", "cit": "[CLS] table 1 shows the performance for the test data measured by case sensitive bleu #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the system is based on the inprotk toolkit for the best performing system described in #refr. [SEP] [SEP] [SEP]", "cit": "[CLS] german: german shallow processing is based on opennlp , stanford pos tagger and ne extractor #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be np-hard #refr. [SEP] s", "cit": "[CLS] since tags factor recursion from dependencies, discontinuities are captured straightforwardly without special devices (as opposed to #refr. [SEP] [PAD]"}
{"pre": "[CLS] the task of identifying the semantic textual similarity of a target word is a given a document #otherefr; #refr.", "cit": "[CLS] for evaluation, we compared our results to those of the three systems that participated in the task (melb: ye"}
{"pre": "[CLS] in the context of machine translation, itg has been explored for a variety of nlp tasks such as semantic", "cit": "[CLS] in particular, a joint model can better capture constraints on multiple variables and can yield higher quality results than"}
{"pre": "[CLS] the first is the process of annotating the sentence planning problem of a sentence planning problem #refr. [SEP] sentence", "cit": "[CLS] to our knowledge, the first to use stochastic techniques in an nlg realization module were #refra) and #otherefr). [SEP]"}
{"pre": "[CLS] the task of named entity recognition has been the focus of the focus of the focus of the focus", "cit": "[CLS] for fully supervised learning, which is the scenario we explore here, recent methods include: feature augmentation #otherefr; #refr, schema"}
{"pre": "[CLS] the most popular approach to mt system combination is the one proposed by #refr, which uses a log-linear combination", "cit": "[CLS] given a sentence-aligned bitext between the target language and a pivot language, we can align the text at the"}
{"pre": "[CLS] in the second category, the context of a word is used to improve the performance of a state-of-the-art statistical", "cit": "[CLS] figure 2: for the pcfg, when we initialize em with the supervised estimate ??gen, the likelihood increases but the"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] we define the mbr decision rule following #refr: e? = argmax e. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second approach is to use mismatches and is the same as #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] for involving semantics, #refr proposed a coreference resolution model with world knowledge; by using word associations, #otherefr showed its"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] a remedy is to aggressively limit the feature space, e.g. to syntactic labels or a small fraction of the"}
{"pre": "[CLS] the semantic annotation of words in the text is represented as a window of the basis for many nlp", "cit": "[CLS] for english, there is work on the automatic extraction of pvs from corpora #refr and the determination of compositionality"}
{"pre": "[CLS] the first is the case for the syntactic annotation of the penn treebank #otherefr; #refr, which we will use", "cit": "[CLS] treebanks have been enormously influential in the development of robust, state-of-the-art parsing technology: grammars #otherefr; #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] #refr proposed a method for learning a dependency tree in a probabilistic model that is a probabilistic model for", "cit": "[CLS] 7work such as smith and eisner #otherefr, #refr has been exploring more nonlocal features for dependency parsing. [SEP] [PAD]"}
{"pre": "[CLS] in the context of sentiment analysis and opinion mining has been used to determine if the word is to", "cit": "[CLS] often, methods for opinion, sentiment, and subjectivity analysis rely on lexicons of subjective #otherefr; #refr). [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] of a text with", "cit": "[CLS] entity disambiguation systems such as those of kulkarni et al #otherefr and #refr disambiguate references to people and organizations"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] algorithm with the average log likelihood algorithm #refr with", "cit": "[CLS] recently, specific probabilistic tree-based models have been proposed not only for machine translation #otherefr; #refr, but also for this"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a text that is a", "cit": "[CLS] #refr uses conjunction and appositive annotations in the vector representation. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the parsing of the english language model of the penn treebank #otherefr, which", "cit": "[CLS] morphotactics refers to the way morphemes combine together to form words #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] #refr use koehn and knight?s technique, include a larger list of possible modifier transformations and apply pos restrictions on"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] a large body of work has been dedicated to learning paraphrases and entailment rules, e.g. #otherefr; #refr, identifying appropriate"}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders and opinion holders in opinion holders in opinion holders and", "cit": "[CLS] recent studies have focused on exploring demographic language variations in personal email communication, blog posts, and public discussions #otherefr;"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking approaches #otherefr; #refr to learning semantic parsers", "cit": "[CLS] syntactic model we used two discriminative arcfactored models for labeled dependency parsing: a first-order model, and a second-order model"}
{"pre": "[CLS] the first is the automatic metric of discourse structure of discourse structure of discourse structure of text #refr. [SEP]", "cit": "[CLS] this distinction is key because, while predicting confidence alone is a stepping stone, self-awareness is the ability to additionally"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal (wsj) treebank #otherefr; #refr. [SEP] trees and the", "cit": "[CLS] while the restriction to projective analyses has a number of advantages, there is clear evidence that it cannot be"}
{"pre": "[CLS] the parser is trained on the penn treebank corpus and tested on the penn treebank corpus #refr. [SEP] test", "cit": "[CLS] interestingly, several studies suggested that the identification of propbank annotations would require linguistically-motivated features that can be obtained by"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] as future work, we would like to investigate possible semantic contributions of the se particle to the meaning of"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] #refr show how two different approaches to data-driven dependency pars- 6person would be another syntactically relevant information. [SEP] [PAD]"}
{"pre": "[CLS] we use the stanford corenlp suite to lemmatize and partof-speech tag each word #otherefr; #refr, and parse the input", "cit": "[CLS] wiki: 8k abstracts of english wikipedia articles parsed by the berkeley parser #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the penn treebank #otherefr. [SEP] score", "cit": "[CLS] however, the development of a richly annotated corpus such as an hpsg treebank is not an easy task, since"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of possible antecedents for the", "cit": "[CLS] the interest data was created by #refr by tagging all occurrences of interest in the acl/dci wall street journal"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] the emergence of phrase-based statistical machine translation (psmt) #refr has been one of the major developments in statistical approaches"}
{"pre": "[CLS] the model is trained using the averaged perceptron algorithm #refr. [SEP] score #refr with the best alignment algorithm #otherefr.", "cit": "[CLS] they have been shown to match the performance of state-of-the-art language models and to give estimates that follow appropriate"}
{"pre": "[CLS] the first is the automatic evaluation of discourse structure of discourse relations #refr. [SEP] quality in the fact that", "cit": "[CLS] they do not fall cleanly into mutually exclusive classes, but populate the continuum between the two extremes #refr. [SEP]"}
{"pre": "[CLS] the similarity between two sentences is computed using the wordnet #otherefr and the wordnet similarity measure described in #refr.", "cit": "[CLS] this fact was also realized by the organizers of the pilot semantic textual similarity task at semeval-2012 (see section"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for each sentence in the sentence with", "cit": "[CLS] our model leverages multiple automatically learned latent variable grammars, which differ only in the seed of the random number"}
{"pre": "[CLS] the first is the one of the most popular approaches to automatically identify the words in a text that", "cit": "[CLS] to achieve that, we utilized a lexical database called framenet #refr, which is built upon semantic frames of concepts"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] we add the standard lexical and derivation features from lopez #otherefrb) and #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford named entity recognizer #refr to extract named entities from the text and extract named entities", "cit": "[CLS] the experimental results demonstrate that our model is able to significantly outperform the state-ofthe-art coherence model by #refr, reducing"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] of a text with", "cit": "[CLS] typical target relations include ?reaction? and ?production? #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second approach is to use a small number of seed examples for training data #otherefr; #refr. [SEP] is", "cit": "[CLS] given this setting, the current state-of-art methods approach the problem by learning morph lexicons from both annotated and unannotated"}
{"pre": "[CLS] the semantic relation between nominals in the verb frame acquisition and the information extraction task is to be extracted", "cit": "[CLS] se?#refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr proposed a method for learning semantic parsers from a target language for a target language for a target", "cit": "[CLS] extrinsic evaluations have been performed e.g. for automatic set expansion #otherefr or phrase polarity identification (goyal and #refr. [SEP]"}
{"pre": "[CLS] we use the berkeley parser #refr to obtain the syntactic parse trees for each sentence and the english and", "cit": "[CLS] hierarchical phrase-based smt #refr uses a synchronous context free grammar (scfg), where the rules are of the form x"}
{"pre": "[CLS] the first is the task of identifying the semantic textual similarity of a text of a text of a", "cit": "[CLS] despite extensive research on semantic analysis and understanding of word and text #otherefr; #refr, little work studied the measurement"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the penn treebank #otherefr. [SEP] score", "cit": "[CLS] following best practices #refr, we removed sequences covered by edited nodes in the tree from the strings prior to"}
{"pre": "[CLS] #refr proposed a method for learning semantic inference based on a set of sentences paired with their underlying meaning", "cit": "[CLS] such models have been effective in discovering lexicons in many nlp tasks, e.g., named-entity recognition #refr, word-sense disambiguation #otherefr,"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] unlike in machine translation, the monotonicity constraint is enforced; i.e., we assume that x and y can be aligned"}
{"pre": "[CLS] the translation model is a log-linear model #refr. [SEP] model with the decoder that we use the decoder that", "cit": "[CLS] we use the minimum-error rate training procedure by #refr as implemented in the moses toolkit to set the weights"}
{"pre": "[CLS] #refr proposed a method for identifying the polarity of a word in a word in a target word in", "cit": "[CLS] in addition #refr investigate the use of amazon?s mechanical turk for annotating named entities in twitter, minkov et. al."}
{"pre": "[CLS] #refr use a similar approach to compute sentence importance. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] various methods for topic segmentation of textual data are described in the literature, e.g., #otherefr; #refr, most of them"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the input is to be", "cit": "[CLS] the paper is organized as follows: in section 2 we describe a lexiealized ependency formalism that is a simplified"}
{"pre": "[CLS] #refr proposed a method for learning a word sense disambiguation which is a word sense disambiguation method to extract", "cit": "[CLS] naturally, discriminative models such as #refr are also likely to improve by using the added data. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to translation lexicons from parallel corpora #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] #refr proposed a method to identify such non-transliteration pairs, and applied it successfully to noisy word pairs obtained from"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] shared", "cit": "[CLS] the native penn treebank output of bikel?s and mcclosky?s parser was converted to the stanford dependency (sd) collapsed dependency"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with the one of", "cit": "[CLS] parser uas las #otherefr ? 93.5 #refr ? 93.79 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the literature, slight variations of this problem are also referred to as unsupervised learning #otherefr, and semi-supervised learning", "cit": "[CLS] a common solution #refr is to begin by defining an alignment over the corresponding entities, predicates and their arguments"}
{"pre": "[CLS] in this paper, we propose a method for learning a semantic similarity measure that is proposed by #refr. [SEP]", "cit": "[CLS] in mt, a weak similarity function is compensated for by searching for a globally optimal alignment, using dynamic programming"}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders and extract opinion expressions in opinion expressions in opinion holders", "cit": "[CLS] in previous work, #refr proposed a method to identify the polarity of adjectives based on conjunctions linking them in"}
{"pre": "[CLS] the grammar is a grammar that is a grammar that is a given a grammar that is a grammar", "cit": "[CLS] improvements and extensions to this algorithm have been provided by #refr, karttunen #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to compute the similarity between two sentences in a sentence and their mrs. [SEP]", "cit": "[CLS] #refr use ica to improve sentiment polarity classification of dialogue acts in a corpus of multi-party meeting transcripts. [SEP]"}
{"pre": "[CLS] the second dataset is the use of a word sense disambiguation algorithm which is based on the most popular", "cit": "[CLS] the use of dictionary definitions as an optional seed for the unsupervised algorithm stems from a long history of"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the sentence in", "cit": "[CLS] several discoursebased summarization methods have been developed, such as #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the translation quality is evaluated by case-insensitive bleu-4 #refr. [SEP] quality measured by bleu score #refr with the bleu", "cit": "[CLS] these smt systems have also become popular in the transliteration field #refr, #otherefr use a machine translation system comprising"}
{"pre": "[CLS] the second approach is to use a word sense disambiguation algorithm to find the most probable parse of the", "cit": "[CLS] we used a maximummatching algorithm and a dictionary compiled from the ctb #otherefr and tag-based parser #refr on the"}
{"pre": "[CLS] the first is the task of identifying the polarity of a text that is a text of a text", "cit": "[CLS] while most of the noun compound research to date is not domain specific, #refr create and experiment with a"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be a difficult problem #otherefr;", "cit": "[CLS] in contrast, #refr have avoided the constraints of tree-based syntactic models and allow the relatively flat motion of empirically"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] to obtain better grammars, schluter and van genabith #otherefr slightly modified the original ftb pos tagset to optimize the"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] the same story has been told for machine translation #refr, inter alia), in which a target language sentence (the"}
{"pre": "[CLS] we use the averaged perceptron algorithm #refr to train the parameters of the averaged perceptron algorithm #otherefr. [SEP] [SEP]", "cit": "[CLS] meanwhile, some learning algorithms, like maximum likelihood for conditional log-linear models #otherefr; #refr, require summing over the scores of"}
{"pre": "[CLS] in this paper, we propose a method for automatically learning paraphrases from parallel corpora of comparable corpora of bilingual", "cit": "[CLS] information about the presence or absence of entailment between two sentences has been found to be beneficial for a"}
{"pre": "[CLS] #refr use a similar approach to identify the polarity of adjectives. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] more recent work has focused on identifying users? attitudes towards each other #refr, influential users and posts #otherefr. [SEP]"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be a difficult problem #otherefr;", "cit": "[CLS] for german, a lexicon of discourse connectives exists since the 1990s, namely dimlex for lexicon of discourse markers #refr."}
{"pre": "[CLS] the system is based on the inprotk project #refr. [SEP] score for the number of the number of the", "cit": "[CLS] note that the predicate language representation utilized by carmel-tools is in the style of davidsonian event based semantics #refr."}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of possible senses in the", "cit": "[CLS] one can view the possible interpretations of a potentially metonymic word (pmw) as corresponding to the word?s possible senses"}
{"pre": "[CLS] we use the stanford parser #refr to extract dependency relations for each sentence and parse trees from the sentence", "cit": "[CLS] these features are calculated by mining the parse trees, and then could be used for resolution by using manually"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best results for the best", "cit": "[CLS] taking the min- greedy algorithm #refr as a starting point, we improve it with several intuitive heuristics. [SEP] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] specifically, for en-ja translation we use the head finalization pre-ordering method of #refrb), and for ja-en translation, we use"}
{"pre": "[CLS] the first is the task of identifying the overall sentence pairs from the english text using the stanford parser", "cit": "[CLS] this means that to collect a corpus of 1,000 preposition errors, an annotator would have to check over 10,000"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] this 5all statistical significance tests in these experiments use the computationally-intensive randomisation test described in #refr, with p <"}
{"pre": "[CLS] the grammar is a synchronous grammar formalism which is a synchronous grammar (scfg) used to parse tree adjoining grammar", "cit": "[CLS] there are additional variants, such as the maximum jump d strategy (mjd), a polynomial-time strategy described by #refr, and"}
{"pre": "[CLS] the parser is trained on the penn treebank corpus and tested on the penn treebank wall street journal, parser", "cit": "[CLS] there was radiographic jj evidence nn of diminished jj or stabilized jj pleural jj effusion nn 2the lt ttt"}
{"pre": "[CLS] the second dataset is the same as the use of the same as the same as the same as", "cit": "[CLS] other distributional methods include the use of a word-category cooccurrence matrix, where categories are coarse senses obtained from an"}
{"pre": "[CLS] the most common approach to parsing is to use a log-linear parsing model and maximise the conditional random field", "cit": "[CLS] however, for non-projective parsing chu-liu-edmond?s algorithm has a complexity of o(n2) #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the penn treebank #otherefr. [SEP] score", "cit": "[CLS] the techniques of charniak #otherefr, #refr, and ratnaparkhi #otherefr achieved roughly comparable results using the same sets of training"}
{"pre": "[CLS] the feature weights were tuned using minimum error rate training (mert) #refr on the development set using minimum error", "cit": "[CLS] in contrast to this, our tuning framework provides a principled way of using the pareto optimal options using ensemble"}
{"pre": "[CLS] the translation quality is measured by bleu #refr. [SEP] score #refr on the test set. [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] usually, about 300 or more sentences are used to automatically rank mt systems #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the event descriptions of a text at the goal is to be", "cit": "[CLS] #refr and zhou et al #otherefr reported research on chinese coreference resolution. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] we produced alignments with the berkeley aligner #refrb) with standard settings and symmetrized via the grow-diag heuristic. [SEP] [PAD]"}
{"pre": "[CLS] the translation quality is measured by bleu #refr. [SEP] [SEP] = 4. [SEP] = 4. [SEP] = ? =", "cit": "[CLS] this alignment matrix contains scores for all word correspondences in the sentence pair and can be created using giza++"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] 6details of how these lists are constructed can be found in #refr. cle words (226), location names (1.8k), company"}
{"pre": "[CLS] the first is the task of identifying the sentence planning problem that is central to a text planning problem", "cit": "[CLS] the process of extension simply consists of deriving a more elaborate form with a richer meaning using the generator"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to learn a translation hypotheses and a", "cit": "[CLS] it is possible that the reliance on automatic metrics may have led smt researchers to pay insufficient attention to"}
{"pre": "[CLS] #refr and toutanova et al #otherefr proposed a method for inducing ccgs to use a probabilistic model to predict", "cit": "[CLS] resolving ambiguity through lexical associations #refr found lexical preferences to be the key to resolving attachment ambiguity. [SEP] [PAD]"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be np-hard #refr. [SEP] s", "cit": "[CLS] this suggests a new method for generalizing step 2 of the algorithm described in the previous section as follows"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] proposals include eisner?s #otherefr multitext grammar, and #refr tree-to-tree transducers. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in order to overcome this problem, we use a different approach to reduce the search space of linguistic knowledge", "cit": "[CLS] #refr proposed stm, a structural approach based on syntax to addresses the failure of lexical similarity based metrics in"}
{"pre": "[CLS] #refr use a similar approach to identify the topics of words in a sentence in a sentence and their", "cit": "[CLS] text summarization #refr, question answering #otherefr are some of the examples of such applications. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders and opinion holders in opinion holders and opinion holders in", "cit": "[CLS] the first is the movie review data of pang and lee #otherefr5; the remaining are four product review datasets"}
{"pre": "[CLS] #refr proposed a method for learning a bilingual lexicon from monolingual data. [SEP] pairs of sentences paired with their", "cit": "[CLS] in contrast, the c&c tagger, which is based on that of #refr, utilizes a wide range of features and"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? 2008.", "cit": "[CLS] in fact, #refr pointed out that implicit arguments can increase the coverage of argument structures in nombank by 71%."}
{"pre": "[CLS] topic models have been successfully applied to a wide range of nlp tasks including word sense disambiguation #otherefr, and", "cit": "[CLS] more recent work use techniques from graph theory #otherefr and machine learning #refr in order to find patterns in"}
{"pre": "[CLS] the first is the automatic evaluation of discourse structure of discourse relations #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] a sample of these includes methods to extract linguistic resources #otherefr; #refr, retrieve useful information in response to user"}
{"pre": "[CLS] topic models have been successfully applied to a variety of tasks including word sense disambiguation #otherefr, and sentence compression", "cit": "[CLS] another line of related work is discourse analysis in natural language processing: discourse segmentation #otherefr splits a document into"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] to get more general translation rules, we restructure our english training trees using expectationmaximization #otherefr, and to get more"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a similar to the similarity of two sentences that is based", "cit": "[CLS] for exam- 1here ?semantic relations? include both classical relations such as synonymy and meronymy, and non-classical relations as defined"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be a difficult problem #otherefr;", "cit": "[CLS] i{ecently, several groul)s have been exploring the possibility of aligning t)armlel syntacticalhj analyzed sentences fr<)m the source and target"}
{"pre": "[CLS] we use the stanford ner tagger #refr to train the classifier to extract named entities from the training data.", "cit": "[CLS] entity mention extraction model we re-cast the problem of entity mention extraction as a sequential token tagging task as"}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders and predict the polarity of words in a sentence and", "cit": "[CLS] table 1: an excerpt from a dvd review. instance, when performing single-aspect sentiment analysis, the most relevant aspect of"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the sentence in", "cit": "[CLS] recent attempts #otherefrb; #refr) are still considerably inferior when compared to human goldstandard discourse analysis. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] there are efficient training algorithms to find optimal weights relative to a labeled training data set once the feature"}
{"pre": "[CLS] the bionlp 2011 shared task #refr focuses on event extraction and event extraction and event extraction and event extraction", "cit": "[CLS] the local formulae are based on features employed in previous work #refr and are listed in table 1. [SEP]"}
{"pre": "[CLS] the most popular approach to translation lexicons from parallel corpora #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ?", "cit": "[CLS] researchers have used stemming #otherefr; #refr, or direct clustering #otherefr to identify such groups of words and use them"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] of a text with", "cit": "[CLS] temporal information extraction #otherefr; #refr is pivotal for many natural language processing (nlp) applications such as question answering, text"}
{"pre": "[CLS] the first is the case for the generation of referring expressions from the linguistic phenomena of referring expressions #refr.", "cit": "[CLS] in a previous study using the same video recordings but a different, simpler scheme #refr, facial displays could only"}
{"pre": "[CLS] we use the stanford corenlp suite to lemmatize and partof-speech tag each word #otherefr; #refr, and parse the input", "cit": "[CLS] human engagement in narrative is partially driven by reasoning about discourse relations between narrative events, and the expectations about"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] this algorithm is a natural synchronous generalization of the monolingual maximum constituents parse algorithm of #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the dependency parser is trained on the penn treebank #refr and the penn treebank #otherefr. [SEP] treebank #otherefr. [SEP]", "cit": "[CLS] broadly speaking, we can classify the methods to incorporate semantic information into parsers in two: systems using static lexical"}
{"pre": "[CLS] the task of identifying the event descriptions of event extraction and event descriptions has been addressed in recent years", "cit": "[CLS] several recent studies have stressed the benefits of using information redundancy on estimating the correctness of the ie output"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] #refr, popescu and etzioni #otherefr), determining their polarity (e.g. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford named entity recognizer #refr to extract named entities from the text and extract named entities", "cit": "[CLS] to classify posts? stance in dual-sided debates, previous approaches have used probabilistic #otherefr as well as machine learning techniques"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set.", "cit": "[CLS] the training process follows minimum error rate training (mert) described in #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the berkeley parser #refr to obtain the syntactic parse trees for each sentence and the english and", "cit": "[CLS] another method to address the re-ordering problem in translation is the hiero model proposed by #refr, in which a"}
{"pre": "[CLS] #refr use a similar approach to compute sentence importance. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] the first two are from written text domain and have been widely used in many previous studies for sentiment"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] license details: http://creativecommons.org/licenses/by/4.0/ and savoy #otherefr, and #refr all proposed competitive feature selection methods evaluated on the movie review"}
{"pre": "[CLS] the first is the case for the semantic textual similarity of the words in the context of the context", "cit": "[CLS] brent reports a method for automatically acquiring subcat frames but without frequency measurements (\\[#refr\\], \\[brent, 1991\\]). [SEP] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] in modern machine translation systems such as joshua #otherefr and cdec #refr, a translation model is represented as a"}
{"pre": "[CLS] the second approach is to use a discriminative model #otherefr; #refr. [SEP] model to predict the probability of the", "cit": "[CLS] system lp lr f #refr 84.8 81.9 83.3 #otherefr* 78.6 78.0 78.3 ours 84.57 83.68 84.13 ours (w/ parent"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a text with a word sense disambiguation task", "cit": "[CLS] this assumption is realistic: while truly parallel data (humanly created) might be in short supply or harder to acquire,"}
{"pre": "[CLS] we use the averaged perceptron algorithm #refr to train the weight for each system. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] others have treated transliteration as a phrase-based transduction #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr used a similar method to compute the similarity between two words in a sentence. of words in a", "cit": "[CLS] #refr propose a method to learn multilingual subjective language via cross-language projections. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in this paper, we propose a method for learning a bilingual lexicon induction from monolingual parallel corpus using monolingual", "cit": "[CLS] automatic ne transliteration is an important component in many cross-language applications, such as cross-lingual information retrieval #otherefr; #refr. [SEP]"}
{"pre": "[CLS] #refr proposed a method for learning semantic inference based on a set of sentences paired with their underlying meaning", "cit": "[CLS] however, #refr argues that vector space models, such as graph-learning, may fail to capture the richness of word meaning,"}
{"pre": "[CLS] the first is the same as #refr, which was used to create a word in the context of a", "cit": "[CLS] #refr?s common cover links model (ccl) does not need any prior tagging and is applied on word strings directly."}
{"pre": "[CLS] the most popular approach to paraphrase generation has been the extraction #otherefr; #refr. [SEP] input text is based on", "cit": "[CLS] techniques are usually developed for extracting paraphrase candidates from specific types of corpora, including monolingual parallel corpora #otherefr, bilingual"}
{"pre": "[CLS] the segmentation model is trained with the perceptron algorithm described by #refr. [SEP] model 2 is #refr. [SEP] model", "cit": "[CLS] first, we can let the number of nonterminals grow unboundedly, as in the infinite pcfg, where the nonterminals of"}
{"pre": "[CLS] the similarity between two sentences is computed using the wordnet #otherefr and the wordnet similarity measure proposed by #refr.", "cit": "[CLS] recently, there has been much interest in finding words which are distributionally similar e.g., #refr, lee #otherefr. [SEP] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] #refr relax syntactic phrase extraction constraints in their spmt model 2 to allow for phrases that do not match"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with a beam search", "cit": "[CLS] from a parsing perspective, the c&c parser #otherefr, penn treebank phrasestructure trees #refr, and unbounded dependencies #otherefr. [SEP] [PAD]"}
{"pre": "[CLS] the generation of referring expressions is a subject of research in natural language generation (nlg) in natural language generation", "cit": "[CLS] f~(~,s) (a structure ~ generate~ a string s for grammar g) iff 3 ~s ~ ~(~s,~) 2 most work"}
{"pre": "[CLS] the second approach is to use a discriminative model, which is based on the generative model of #refr. [SEP]", "cit": "[CLS] figure 1: a word dependency tree ferent tasks of natural language processing #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the one of the most popular approach to use a word in a word in which", "cit": "[CLS] porter?s stemmer is a well-known suffix-stripping algorithm #otherefr and pc-kimmo #refr only process inflectional morphology. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the system is based on the inprotk project at the other hand, #refr that we will not only on", "cit": "[CLS] the rags project initially set out to develop a reference architecture based on the three-stage pipeline suggested by #refr."}
{"pre": "[CLS] we use the berkeley parser #refr to obtain the syntactic parse trees for each sentence and the english and", "cit": "[CLS] binarizing the syntax trees for syntax-based machine translation is similar in spirit to generalizing parsing models via markovization #refr."}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] compared to previous methods that also decompose sentence pairs #otherefr; #refr, our method is more syntax-oriented. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the feature weights were tuned using minimum error rate training (mert) #refr on the development set using the development", "cit": "[CLS] all rules over scope 3 are pruned #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford chinese segmenter #refr to tokenize the english sentences. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] furthermore, the extraction of grammars for training is done in a leave-one-out fashion #otherefr and binarized for efficient querying"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for each sentence in the sentence with", "cit": "[CLS] similar constraints have been used in dependency parsing #refr, where the use of hard constraints on the distance between"}
{"pre": "[CLS] the most popular approach to word alignment is the one proposed by #refr. [SEP] model is to use a", "cit": "[CLS] a change in character sets compounds the problem: for instance, there are at least 32 english forms for the"}
{"pre": "[CLS] we use the moses toolkit #refr to train the weights for the feature weights for each model with the", "cit": "[CLS] while some authors try to integrate syntax into phrase-based decoding #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is not to be useful for the purpose", "cit": "[CLS] we had at our disposal the following: a rule-based japanese morphological processor (juman) from kyoto university. - a context-free"}
{"pre": "[CLS] the most common approach to parsing is to use a log-linear parsing model and maximise the conditional likelihood of", "cit": "[CLS] this may be done based on linear local context #refr, or jointly with parsing #otherefr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to word alignment is to use a similarity measure #refr. [SEP] score for each word", "cit": "[CLS] among this second group, several sources of cross-lingual knowledge have been used, such as dictionaries #otherefr; #refr, phrase and"}
{"pre": "[CLS] the translation quality is measured by bleu #refr. [SEP] score #refr with the bleu score of the translation quality.", "cit": "[CLS] the english and french language models (lms) are the same as for the last year?s french-english task #refr and"}
{"pre": "[CLS] the system used in this paper is based on the stanford parser #refr and the stanford parser #otherefr. [SEP]", "cit": "[CLS] the computational learning community is also witnessing a shift towards joint inference based evaluations, with the two previous conll"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] several groups have reported strong improvements over state-of-the-art baselines using feedforward neural network-based language models #refr, as well as"}
{"pre": "[CLS] the system is based on the stanford parser #refr and the stanford parser #otherefr. [SEP] system #otherefr. [SEP] system", "cit": "[CLS] furthermore, the use of linguistic information such as syntax or semantic knowledge has proved to be essential for high-precision"}
{"pre": "[CLS] the second approach is to use a maximum entropy model #otherefr, and conditional random fields #refr, and support vector", "cit": "[CLS] for english spelling correction #otherefr; #refr, most approaches make use of a lexicon which contains a list of well-spelled"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] the graphbased parser is similar to, except much faster, and performs slightly better than the mstparser #otherefr; #refr. [SEP]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the longrange reordering problem as a sequence of words", "cit": "[CLS] although wordbased lms #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the context of machine translation, itg has been explored for a variety of nlp tasks such as pos", "cit": "[CLS] we start by using web counts for two generation tasks for which the use of large data sets has"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the longrange reordering problem as a sequence of words", "cit": "[CLS] for instance, snover et al. #otherefr (and later on #refr) define a paradigm as ?a set of suffixes and"}
{"pre": "[CLS] the model is trained using the maximum entropy model #otherefr, which is based on the conditional probability of a", "cit": "[CLS] surface realisation decisions in a natural language generation #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in this paper, we propose a dynamic programming algorithm for dependency parsing #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] dependency parsing is useful for applications such as relation extraction #refr and machine translation #otherefr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was partially", "cit": "[CLS] part of speech (pos) tags, chunks, clauses, syntactic parse tree, and named entities, which is found crucial to the"}
{"pre": "[CLS] the evaluation metric is bleu #refr, which is based on the reference evaluation. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] most of the recent approaches employ various syntactic or tree edit models #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in recent years, there has been a growing interest in the task of generating referring expressions, reflected in the", "cit": "[CLS] researchers in discourse, such as grosz and sidner #otherefr, #refr, andernach #otherefr have suggested several features that might be"}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders and opinion holders in opinion holders in opinion holders in", "cit": "[CLS] lexicons the manually created lexicons we used include the nrc emotion lexicon #refr #otherefr (about 6,800 words). [SEP] [PAD]"}
{"pre": "[CLS] the task of identifying the semantic relations between nominals of noun phrases and their arguments are also key to", "cit": "[CLS] the task has assumed several guises, such as term extraction ? finding the concepts of the taxonomy #refr, term"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] some work has also attempted to automatically derive logical meaning representations directly from syntactic ccg parses #otherefr; #refr. [SEP]"}
{"pre": "[CLS] the first is the case for the automatic acquisition of subcategorization frames of subcategorization frames in the penn treebank", "cit": "[CLS] the news-sc corpus is part the corl)us of verb-final sentences used by #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford named entity recognizer #refr to extract named entities from the text and extract named entities", "cit": "[CLS] #refr automatically generate short children?s stories using patterns of event and entity co-occurrences. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second approach is to use a small number of manually annotated corpora such as the same data set", "cit": "[CLS] we are not aware of any machine-learning approach to arabic morphology, but find related issues treated in #refr, who"}
{"pre": "[CLS] the generation system is based on the inprotk project #refr. [SEP] score for the user is a given sentence", "cit": "[CLS] in text generation, researchers have been able to exploit automatic analysis of existing resources on such tasks as ordering"}
{"pre": "[CLS] in recent years, there has been a growing interest in the task of generating referring expressions, reflected in the", "cit": "[CLS] for example, many experimental setups #otherefr; #refr were developed based on a visual world for which the internal representation"}
{"pre": "[CLS] the first is the case for the word alignment of the ibm models #otherefr, and the hmm model 4", "cit": "[CLS] for example, work which failed to detect improvements in translation quality with the integration of word sense disambiguation #otherefr;"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] table 2: distribution of the sentences where the semantic role features give no/positive/negative impact to the sentence fluency in"}
{"pre": "[CLS] the parser is trained on the penn treebank corpus and tested on the penn treebank wall street journal, parser", "cit": "[CLS] in this separate-processing approach, reparanda are located through a variety of acoustic, lexical or string-based techniques, then excised before"}
{"pre": "[CLS] the feature weights were tuned using mert #refr on the development set. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] this restriction may be relaxed by adding constituent labels such as det+adj or np\\det to group neighboring constituents or"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a similar to the similarity of two sentences that is based", "cit": "[CLS] we hope to use these models in tasks such as diathesis alternation detection #refr and contrast with wordnet models"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the sentence in", "cit": "[CLS] see #refr for a way to extract the most relevant features from a model learned in the kernel space."}
{"pre": "[CLS] the translation quality is measured by bleu #refr. [SEP] score #refr on the test set. [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] to conduct evaluations of how helpful the translations retrieved by the various tm retrieval metrics would be for translating"}
{"pre": "[CLS] we use the averaged perceptron algorithm #refr to train the parameters of the averaged perceptron algorithm with the averaged", "cit": "[CLS] uses for k-best lists include minimum bayes risk decoding #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] we report mt performance in table 1 by case-insensitive bleu #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the grammar we use is a standard phrase-based statistical machine translation system for the synchronous context-free grammar #otherefr; #refr.", "cit": "[CLS] grammars with longer rules can represent a larger set of reorderings between languages #otherefr, but also require greater computational"}
{"pre": "[CLS] the system used for the experiments in this paper is based on the senseval-3 english all-words task of the", "cit": "[CLS] however, our system is the unsupervised learning with small pos-tagged corpus,and we do not restrict the word is sense"}
{"pre": "[CLS] the first is the stanford parser #refr and then applied to obtain dependency relations between named entities and semantic", "cit": "[CLS] we parsed the english and dutch corpora respectively with the stanford3 #refr and the alpino4 #otherefr parsers, and formalized"}
{"pre": "[CLS] the parser is trained using the c&c parser #refr. [SEP] parser #refr with the penn treebank #otherefr. [SEP] [SEP]", "cit": "[CLS] this parsing approach is very similar to the one used successfully by #refr, but we use a maximum entropy"}
{"pre": "[CLS] the first is the task of identifying the overall sentence pairs from the input text with a text that", "cit": "[CLS] as a result, many timex interpretation systems are a mixture of both rule-based and machine learning approaches #refr. [SEP]"}
{"pre": "[CLS] the first is the case for the semantic role labeling task of the predicate-argument structure of the work on", "cit": "[CLS] i,exri'er has been developped at der-ei)f #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to paraphrase generation is to use a lexicon to paraphrase database into a target language", "cit": "[CLS] automatic paraphrasing has been recognized as an important component for nlp systems, and many methods have been proposed to"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] #refr improved the parsing accuracy of sentences in which coordinated noun phrases are known to exist. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the second approach is to use a word segmentation and pos tagger to tag dictionary to tag each word", "cit": "[CLS] microblog texts frequently contain informal words that are spelled in a non-standard manner, e.g., ?oredi (already)?, ?b4 (before)?, and"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the sentence in", "cit": "[CLS] lexical heads have been calculated using the projection rules of #refr, and annotated between brackets. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] shared", "cit": "[CLS] based on this model, our crf tagger is trained with a stochastic gradient descent-based method described in #refr, which"}
{"pre": "[CLS] the first is the case for syntactic parsing of the english lexical substitution task #refr. [SEP] and then the", "cit": "[CLS] crucially, any comprehensive model of human language understanding needs to combine an incremental parser with an incremental semantic processor"}
{"pre": "[CLS] in the second international chinese word segmentation bakeoff #otherefr; #refr and the same task as the best results are", "cit": "[CLS] another strain of research has sought to exploit resources and tools in some languages #otherefr; das and #refr or"}
{"pre": "[CLS] the data set contains 14,619 items and is available from the same data collection used in the conll shared", "cit": "[CLS] only rarely has there been work on detecting errors in more morphologically-complex languages #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford ner tagger #refr to train the training data with the training data and test data", "cit": "[CLS] in addition, miller et al #otherefr and #refr employ distributional and hierarchical clustering methods to improve the performance of"}
{"pre": "[CLS] the chinese word segmentation method is used to extract the word segmentation and pos tags for arabic #otherefr; #refr.", "cit": "[CLS] for example, #refr combines dictionary-based and crf-based approaches for chinese word segmentation in order to avoid outof-vocabulary (oov) words."}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a task of identifying the degree of semantic relations between words", "cit": "[CLS] among the methods that have been proposed for specifying lexical relationships are natural language description and rules (gi#refr, distributional"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a task of semantic relations between words and phrases and logical", "cit": "[CLS] in contrast, classifiers for semantic tagging #otherefr; huang and #refr) label word instances and focus on the local context"}
{"pre": "[CLS] the first is the task of identifying the polarity of a text with a text that is a text", "cit": "[CLS] this task has been tackled in #refr for modern hebrew, and in #otherefr for finnish. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a text that is a", "cit": "[CLS] following on from previous studies in named entity we chose a set of linguistically motivated wordlevel features that include"}
{"pre": "[CLS] the most common approach to machine translation is to use a log-linear translation system to generate the synchronous context-free", "cit": "[CLS] these sentences are annotated with a subset of the auxiliary information used to annotate the 1#refr also reports that"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] model with respect to the entire candidate set.", "cit": "[CLS] recently, there has been much progress in capturing compositionality in vector spaces, e.g., #otherefr; #refr; zanzotto et al 2010;"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a text that are semantically", "cit": "[CLS] the most relevant work to this paper are #refr, toral and munoz #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the system used in the conll shared task #refr was the 2007 shared task on event extraction and the", "cit": "[CLS] pos tags using the brill tagger #refr; . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the one of the most popular approach to use a word in which is to use", "cit": "[CLS] collocation extraction methods have been used not only for english, but for many other languages: french #refr, german #otherefr,"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] sentence pair of a", "cit": "[CLS] #refr show that parse features can further increase performance, and we use the dependency parse features based on their"}
{"pre": "[CLS] the first is the process of annotating the word in a sentence planning problem of a sentence structure in", "cit": "[CLS] the topic of bi-directional phoneme-to-grapheme conversion is becoming important for application to unknown words and new word acquisition in"}
{"pre": "[CLS] the grammar we are extracted from the parallel corpus using the berkeley parser #refr and the berkeley parser #otherefr.", "cit": "[CLS] several researchers in the field of lexicalized grammar parsing have recently started to seek a common representation of parsing"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] given a derivation d, most existing phrasebased models approximate the derivation probability through a linear interpolation of a finite"}
{"pre": "[CLS] the task of identifying the semantic relations between nominals of noun phrases and their arguments are also key to", "cit": "[CLS] #refr has primarily approached personal name disambiguation using document context profiles or vectors, which recognize and distinguish identical name"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] in the case of bleu, we verified the significance of the improvements by conducting paired bootstrap resampling #refr on"}
{"pre": "[CLS] #refr use a similar approach to compute the similarity between two words in a sentence. [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] we evaluate the sequential output with precision, recall and f-score as defined in #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the penn treebank #otherefr. [SEP] score", "cit": "[CLS] #refr evaluated a ccg parser on a small corpus of object extraction cases. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking approaches #otherefr; #refr to learning for dependency", "cit": "[CLS] #refr use web counts to filter out certain ?semantically bad? parses from extraction candidate sets but are not concerned"}
{"pre": "[CLS] the translation quality is measured by bleu #refr. [SEP] [SEP] = 4. [SEP] = 4. [SEP] = ? i", "cit": "[CLS] after a brief period following the introduction of generally accepted and widely used metrics, bleu #refr and nist #otherefr,"}
{"pre": "[CLS] in the last decade, there has been a lot of work on learning parsing with models #otherefr; #refr. [SEP]", "cit": "[CLS] the -tbl system described in #refr attempts to cut down on training time with a more e cient prolog"}
{"pre": "[CLS] the semantic parser used in this paper is the penn treebank #otherefr, which is based on the c&c parser", "cit": "[CLS] these tasks may additionally drive interest in theoretical frameworks for probabilistically mapping between graphs and strings #refr. [SEP] [PAD]"}
{"pre": "[CLS] in the literature, supervised approaches have been applied to the task of learning a small number of seed examples", "cit": "[CLS] we used orthographic features described in (collier, nobata, & #refr and modified some as for medication information such as"}
{"pre": "[CLS] the second approach is to use a word segmentation and pos tagger to tag dictionary to tag each word", "cit": "[CLS] segmentation performance has been improved significantly, from the earliest maximal match #otherefr approaches and recent state-of-the-art machine learning approaches"}
{"pre": "[CLS] the most common approach to paraphrase generation is to use a lexicon to paraphrase generation #otherefr; #refr. [SEP] words", "cit": "[CLS] a thesaurus constructed from corpus statistics #refr is utilized for the content similarity. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the longrange reordering model is to use of the", "cit": "[CLS] more recent methods have yielded better performance than em (see #refr for an overview). [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford ner tagger #refr to identify named entities and named entities in the text and extract", "cit": "[CLS] subclauses are labeled as ?sbar? in the parser tree generated by a commonly used nlp tool, stanford core nlp"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] the", "cit": "[CLS] the results of these tasks were promising, suggesting that the single ptm type could be extracted at over 80%"}
{"pre": "[CLS] the first is the case for the generation of the generation of referring expressions from the linguistic phenomena of", "cit": "[CLS] the work in this paper is similar to research in statistical surface realisation #otherefr; #refr; filippova and strube #otherefr)."}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for each sentence in the sentence with", "cit": "[CLS] using tree- level inference, we obtained an f1 score of 91.6, suggesting that weighting is not so important in"}
{"pre": "[CLS] in the second international chinese word segmentation bakeoff #otherefr; #refr and the best results are achieved by using the", "cit": "[CLS] there are two major models #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] starting from being a document level classification task #otherefr; #refr, it has been handled at the sentence level #otherefr."}
{"pre": "[CLS] the translation quality is evaluated by case-insensitive bleu-4 #refr. [SEP] quality measured by bleu score #refr with the bleu", "cit": "[CLS] a common approach to obtain such material is to look for it on the web.9 the use of already"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for learning and beam-search decoding #otherefr;", "cit": "[CLS] table 8: average accuracies over conll evaluation sets #otherefra, s) from resource-rich languages with translations #refr, mph). [SEP] [PAD]"}
{"pre": "[CLS] the first is the task of learning a semantic parser that is a complete parse of a semantic role", "cit": "[CLS] for the test data, we report on results using the gold-standard treebank data, and in addition we also report"}
{"pre": "[CLS] #refr proposed a method for learning a word sense disambiguation which uses a small set of seed words to", "cit": "[CLS] in our parsing experiments, we use the berkeley parser #otherefr; #refr, a split-merge latent variable parser, for our monolingual"}
{"pre": "[CLS] #refr use a similar approach to identify the polarity of words in a sentence. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] ing suicide notes #otherefr; #refr.1 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second approach is to use a small number of seed examples for example outputs #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] the argumentadjunct distinction is difficult even for humans, and is further complicated by the fact that both arguments and"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking approaches #otherefr; #refr to learning for the", "cit": "[CLS] we use all 23 train/test splits from the 2006/7 conll shared tasks #refr,6 which cover 19 different languages.7 we"}
{"pre": "[CLS] the system used in this paper is based on the c&c parser #otherefr shared task #refr. [SEP] [SEP] [SEP]", "cit": "[CLS] combining the output from several different systems has been shown to be beneficial #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] all previous work in semantic parsing has exclusively focused on labeling semantic roles, rather than analyzing the full structure"}
{"pre": "[CLS] in the context of sentiment analysis #refr used a word based word based on word based on word sense", "cit": "[CLS] #refr organized a monolingual english lexical substitution task in semeval- 2007, i.e finding english substitutions for an english target"}
{"pre": "[CLS] #refr proposed a method for learning a probabilistic model that learns latent dirichlet allocation (lda) as a latent variable", "cit": "[CLS] dp+lp) on the nw corpus. also faces many obstacles common to decoding in machine translation, and a number of"}
{"pre": "[CLS] the most common approach to machine translation is to use a log-linear combination of statistical machine translation #otherefr; #refr.", "cit": "[CLS] this similarity score is computed as a max over a number of component scoring functions, some based on external"}
{"pre": "[CLS] in the second international chinese word segmentation bakeoff #otherefr; #refr and the best results are achieved by the best", "cit": "[CLS] in particular, distributed representations of words have been used for many nlp problems, which represent a word by information"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] reordering model can be either used for source pre-ordering (tromble and eisner, ), integrated into decoding via translation rules"}
{"pre": "[CLS] we use the minimum error rate training (mert) #refr to tune the feature weights for the feature weights of", "cit": "[CLS] #refr represent the search space as a recursive transition network and the language model as a weighted finite state"}
{"pre": "[CLS] the system is based on the inprotk project at the university of pennsylvania and the xerox implementation of the", "cit": "[CLS] the development of this formalism was strongly influenced by the ideas found in the (lispbased) formalism of the tg/2"}
{"pre": "[CLS] the first is the task of identifying the polarity of a text that is a text of a text", "cit": "[CLS] for both corpora, we first selected only the english documents using langid.py, an off-theshelf language identification tool #refr. [SEP]"}
{"pre": "[CLS] the work of #refr is similar to ours in that it is possible to use a more complex model", "cit": "[CLS] #refr also introduced a graph-based method for part-of-speech tagging in which edge weights are based on feature vectors similarity,"}
{"pre": "[CLS] the task of identifying the semantic textual similarity of a text is a text of a text #refr. [SEP]", "cit": "[CLS] to estimate the correctness of each substitution, we calculate a plausibility score using a modified version of the lexical"}
{"pre": "[CLS] the first is the automatic evaluation of the automatic metric of the automatic evaluation of the automatic evaluation of", "cit": "[CLS] as a response to these problems and following the work in #refr, we propose a method to construct japanese"}
{"pre": "[CLS] we use the stanford ner tagger #refr to train the classifier to extract named entities from the training data.", "cit": "[CLS] besides the mention-pair model, two other commonly used models are the entity-mention model #refr and ranking models #otherefr. [SEP]"}
{"pre": "[CLS] the first is the process of annotating the sentence pairs in a sentence pair into a text with a", "cit": "[CLS] in addition, those approaches that do handle hierarchical segmentation do not address automatic unification methods #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] we are especially interested in this scheme as it is designed to capture semantically contentful relations (de marneffe and"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? 2008. [SEP]", "cit": "[CLS] this choice is justified by previous studies #refrb) showing that the accuracy of classification is higher for lower nodes;"}
{"pre": "[CLS] the system is based on the inprotk project #refr. [SEP] score for the user is not a given sentence,", "cit": "[CLS] the give software infrastructure #refra) consists of three different modules: the client, which is the program which the user"}
{"pre": "[CLS] the most common approach to parsing is to use a word lattice parsing model that of a dependency parser", "cit": "[CLS] as suggested by #refr, the use of word clusters may also reduce the need for annotated data. [SEP] [PAD]"}
{"pre": "[CLS] the first is the case for the syntactic parsing of the english lexical substitution task #refr. [SEP] grammars in", "cit": "[CLS] dridan #otherefr tests two supertaggers, one induced using the tnt tagger #refr and another using the c&c supertagger #otherefr,"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] tuning of models used minimum error rate training #refr, repeated 3 times and averaged #otherefr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] syntactic structure-based reordering has been shown to be significantly helpful for handling word order issues in phrase-based machine translation"}
{"pre": "[CLS] the most common approach to this problem is to use a word representation of a word representation of a", "cit": "[CLS] while automated resolution of entity coreference has been an actively researched area #refr, there has been relatively little work"}
{"pre": "[CLS] we use the averaged perceptron algorithm #refr to train the parameters of the averaged perceptron algorithm with the averaged", "cit": "[CLS] our approach to dependency parsing is based on the linear model of mcdonald et al#refrb). [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of meaning that are not", "cit": "[CLS] #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to compute similarities between words in a sentence. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] the work of #refr does not have this problem, and appears to perform well on small data sets, but"}
{"pre": "[CLS] in the second international chinese word segmentation bakeoff #otherefr; #refr and the best results are achieved by the best", "cit": "[CLS] graphbased models parameterize the parsing problem by the structure of the dependency graph and normally use dynamic programming for"}
{"pre": "[CLS] the second approach is to use a small number of seed examples for example outputs #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] the use of nlp techniques for document classification has not produced significant improvements in performance within the standard term"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a similar to the one of the target word in that", "cit": "[CLS] interestingly, recent approaches to the semantic composition of adjectives with nouns such as baroni and zamparelli #otherefr and #refr"}
{"pre": "[CLS] the task of event extraction has received a lot of attention in recent years #otherefr; #refr. [SEP] [SEP] [SEP]", "cit": "[CLS] the search process assumes that if a fluent?s entity and slot value co-occur in a sentence,5 that sentence is"}
{"pre": "[CLS] in recent years, there has been a growing interest in the task of paraphrase generation #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] previous work has therefore suggested to learn a reward function from human data as in the paradise framework #refr."}
{"pre": "[CLS] the system is based on the inprotk toolkit for the best performing system described in #refr. [SEP] system, which", "cit": "[CLS] this also confirms some of the findings of #refr, who found features similar to 12significance of improvement in f-measure"}
{"pre": "[CLS] the system used in the shared task #refr is the follow-up event to extract the best system for coreference", "cit": "[CLS] linkpeople is inspired by the stanford deterministic coreference resolution system #refr, using a multi-pass architecture which applies a battery"}
{"pre": "[CLS] #refr proposed a method for learning semantic inference based on a set of sentences paired with their meaning representation", "cit": "[CLS] a good trade-off between fully supervised and fully unsupervised approaches is distant supervision, a semi-supervised procedure consisting of finding"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? ? ?", "cit": "[CLS] as part of the statement map project we proposed the development of a system to support information credibility analysis"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] there has been considerable work in the areas of subjectivity classification #otherefr and the related sentiment relevance (scheible and"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] simard and plamondon #otherefr (itself based on cognate matching) and the second pass uses ibm model-1, following #refr. [SEP]"}
{"pre": "[CLS] #refr proposed a method for learning a word alignment. [SEP] model that learns bilingual corpora. [SEP] pairs from a", "cit": "[CLS] this measure has been shown to correlate well with human judgements #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the sentence planning problem that is central to a text planning problem", "cit": "[CLS] it will respond by identifying the likely source of error, if possible (e.g., a word it does not understand)"}
{"pre": "[CLS] #refr proposed a method for learning semantic inference based on a set of sentences paired with their meaning representation", "cit": "[CLS] vector space models form the basis of modern information retrieval #otherefr, inter alia), or that contextualize the meaning of"}
{"pre": "[CLS] in the second international chinese word segmentation bakeoff #otherefr; #refr and the best results are achieved by the best", "cit": "[CLS] a similar line of research investigated the use of integer linear programming (ilp) based parsing #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to word alignment is to use a log-linear combination of models #otherefr; #refr. [SEP] model", "cit": "[CLS] we use minimum error rate training (mert) #refr to tune the decoder. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford ner tagger #refr to train the training data with the feature templates and extract named", "cit": "[CLS] in this paper, we focus on an inference technique called amortized inference #refr, where previous solutions to inference problems"}
{"pre": "[CLS] the task of coreference resolution is a well studied problem #otherefr; ng and #refr. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] entity coreference resolution is a well studied problem with many successful techniques for identifying mention clusters #otherefr; #refr; haghighi"}
{"pre": "[CLS] the second approach is to use a small number of manually annotated corpora such as the same data set", "cit": "[CLS] we first pick up off-the-shelf technology, in our case the rule-based brill tagger #otherefr and the statistically-based tnt tagger"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] thus, the fn srl results are translatable fairly directly into formal representations which can be used for reasoning, question"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? 2008. [SEP]", "cit": "[CLS] the state-of-the-art in semantic role labelling has now advanced so much that a number of studies have shown that"}
{"pre": "[CLS] we use the stanford ner tagger #refr to train the training data with the training data and test data", "cit": "[CLS] we split the treebank into training (sections 0-18), development (sections 19- 21) and test (sections 22-24) as in #refr."}
{"pre": "[CLS] the second approach is to use a word lattice as a sequence of a sequence of word segmentation and", "cit": "[CLS] while there are fully statistical surface realizers #refr, they operate in a phrase-based fashion on word forms with no"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the penn treebank #otherefr. [SEP] s", "cit": "[CLS] 4the repetition error of ?it? is interpreted as a topicalization. gions and potentially capturing meaningful text is a shallow"}
{"pre": "[CLS] the task of semantic textual similarity (sts) task #refr is a similar to the similarity of similarity of the", "cit": "[CLS] moving to sentences increases the complexity exponentially and as a result has led to measurements of similarity at various"}
{"pre": "[CLS] in the last decade, there has been a lot of work on relation extraction #otherefr; #refr. [SEP] [SEP] [SEP]", "cit": "[CLS] we give usp the required stanford dependency format as input (de marneffe and #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to paraphrase generation has been the source sentences into either by using a monolingual parallel", "cit": "[CLS] other methods obtain paraphrases from raw monolingual text by relying on distributional similarity #otherefr; #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the idea of using cross-lingual information retrieval #otherefr and machine translation #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] we use mgiza++ #refr as the implementation of the ibm models. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to learn a translation hypotheses and a", "cit": "[CLS] some of them developed syntax-based models on complete syntactic trees with treebank annotations #otherefr, and others used source-language syntax"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set.", "cit": "[CLS] #refr present a discriminative translation system. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to paraphrase generation has been the source sentences into either by using a source or", "cit": "[CLS] paradigm extends the ideas in parametric from lexical and phrasal paraphrasing techniques to paraphrasing techniques that also generate syntactic"}
{"pre": "[CLS] in the context of machine translation, itg has been explored for a variety of nlp tasks, including sentiment analysis", "cit": "[CLS] we use berkeley pcfg parser #refr to parse sentences. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a graph to identify the polarity of words in a sentence. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] we evaluate our methods using the benchmark test collection from the acl semeval-2007 web person search task (weps hereafter)"}
{"pre": "[CLS] we use the stanford ner tagger #refr to train the training data with the feature templates and the training", "cit": "[CLS] integer linear programming #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] we focus on passive, coordination, and relative clause constructions, as analysed in the parc #otherefr, gr #refr, and stanford"}
{"pre": "[CLS] the features used in our system are based on the stanford parser #refr and the stanford parser #otherefr. [SEP]", "cit": "[CLS] #refr, by building a graph representation and applying graph clustering techniques #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of sentence compression which has been addressed by many researchers #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] similarly, the naturalowl system #refr has been proposed to generate fluent descriptions of museum exhibits from an owl ontology."}
{"pre": "[CLS] the evaluation metric is the bleu score #refr on the test set and the test set for the test", "cit": "[CLS] the dataset was created following the crowdsourcing methodology proposed in #refr, which consists of the following steps: [SEP] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the polarity of a text that is a text that a text", "cit": "[CLS] similarly as the work of zhu et al#otherefr, #refr). [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of identifying the semantic relations between nominals of noun phrases and their arguments are also key to", "cit": "[CLS] while several studies propose relationship identification methods using distributional analysis of feature vectors #otherefr manually designed lexico-syntactic patterns for"}
{"pre": "[CLS] the system is based on the inprotk toolkit for the best system described in #refr. [SEP] evaluation of the", "cit": "[CLS] our firs/prototype for corect will be based on the tool for authoring knowledge bases which was developed as part"}
{"pre": "[CLS] the most common approach to paraphrase generation is to use a lexicon to paraphrase generation #otherefr; #refr. [SEP] words", "cit": "[CLS] #refr presents an algorithm to align phonetic sequences by computing the similarities of these words. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the automatic evaluation of the automatic metric of subjectivity analysis methods that have been proposed #otherefr;", "cit": "[CLS] we use the decision tree classifier from the natural language toolkit for python #refr and train and test it"}
{"pre": "[CLS] the second approach is to use a word sense disambiguation algorithm to find the most probable word sense disambiguation", "cit": "[CLS] for that problem, some statistical methods have been applied and succeeded#otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the word alignment of the output of a phrase structure grammar based statistical", "cit": "[CLS] we performed several experiments, in which the amount of' training data, the algorithm (brill is original formulation and 'lazy'"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a text with a text that are semantically", "cit": "[CLS] several properties of hebrew mwes are described by al-haj #otherefr; al-haj and #refr use them in order to construct"}
{"pre": "[CLS] the second approach is to use a small number of seed examples for example outputs #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] #refr shallow, gappy language model allows arbitrary token groupings within a sentence, whereas our model imposes projectivity and nesting"}
{"pre": "[CLS] the model is trained using the moses decoder #refr. [SEP] score #refr with the decoder that the best alignment", "cit": "[CLS] we use the sri language modeling toolkit to train a 5-gram language model on the xinhua portion of the"}
{"pre": "[CLS] the first is the automatic evaluation of discourse structure of discourse structure of discourse structure of discourse structure of", "cit": "[CLS] we used the stanford parser #refr to obtain the nps: there are 4361 nps, where (by the gold standard"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to translate the phrases as the", "cit": "[CLS] in a similar manner to mbr decoding over multiple k-best lists in de #refr, the path posterior probability of"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] another approach is to incorporate features, based on the target syntax, during modeling and decoding, and this is shown"}
{"pre": "[CLS] the learning algorithm used in this paper is a variation of the winnow update rule based statistical parsing algorithm", "cit": "[CLS] on one end of the scale is unsupervised srl, such as #refr, which requires some expert knowledge, but no"}
{"pre": "[CLS] in the context of machine translation, itg has been explored for a variety of nlp tasks, including pos tagging", "cit": "[CLS] in this section we review a number of proposals regarding concrete methods of constructing tensors for relational words in"}
{"pre": "[CLS] the task of identifying the overall event descriptions of the bionlp?09 shared task on event extraction #refr, the best", "cit": "[CLS] to get semantic roles, ccg srl tool #refr was used for english, and ancora for spanish. . [SEP] [PAD]"}
{"pre": "[CLS] the learning algorithm used is a variation of the winnow update rule incorporated in snow #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] in response to this challenge, many researchers have investigated semi-supervised learning algorithms that learn from a #otherefr; #refr). [SEP]"}
{"pre": "[CLS] in the context of machine translation, the machine learning approach has been applied to a number of tasks in", "cit": "[CLS] another problem that should be addressed is the scope of the downward entailment, generalizing work being done in detecting"}
{"pre": "[CLS] #refr use a similar approach to identify the topics of discourse relations. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] citation context has been explored in several studies #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the wsd system is based on the standard approach described in #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] 7http://code.google.com/apis/language/ evaluation metric the best precision and recall metric was introduced by #refr in the framework of the semeval-2007"}
{"pre": "[CLS] the generation of referring expressions is a subject of research in natural language generation (nlg) in natural language generation", "cit": "[CLS] to overcome its limitation to one-place predicates, #refr introduced a constraint-based procedure that could generate referring expressions involving relations;"}
{"pre": "[CLS] the first is the polarity of a polarity of a polarity of a polarity of a polarity of a", "cit": "[CLS] to compile the lexicon, we began with a list of subjectivity clues from #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the berkeley parser #refr to obtain the syntactic parse trees for parse trees for english and the", "cit": "[CLS] in our experiments, we build a competitive baseline #refr incorporating a 5-gram lm trained on a large part of"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] the other approach is to estimate a single score or likelihood of a translation with rich features, for example,"}
{"pre": "[CLS] #refr use a bootstrapping method to identify opinion holders and extract opinion expressions from documents in a document pairs", "cit": "[CLS] most existing work in opinion summarization focus on predicting sentiment orientation on an entity #refr #otherefr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the averaged perceptron algorithm #refr. [SEP] score #refr with the best score calculation of", "cit": "[CLS] turboparser), zhang and mcdonald #otherefr, #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford chinese word segmenter #refr to tokenize the english side of the parallel corpus #otherefr. [SEP]", "cit": "[CLS] 2006; shen et al, 2008) that take a string as input and tree-based systems #refr that take a tree"}
{"pre": "[CLS] the translation quality is measured by bleu #refr. [SEP] score #refr with the bleu score of the translation quality.", "cit": "[CLS] one approach that can, in principle, better exploit both alignments from bitexts and make use of non-parallel corpora is"}
{"pre": "[CLS] #refr used a supervised sentiment classification approach to classify reviews as well as sentiment classification and classify reviews as", "cit": "[CLS] #refr show that bag-of-words beat other feature types (based on token bigrams, parts-of-speech information and word position in the"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] researchers in the literature used to approach this task for various languages such as: english #otherefr, japanese #refr, arabic"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to learn a translation probabilities from", "cit": "[CLS] similar aims are pursued by #otherefr; #refr but differently approached. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford named entity recognizer #refr to extract named entities from the stanford named entities from the", "cit": "[CLS] similarly to #refr, we have addressed the task as a document clustering problem. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second dataset is the use of a word sense disambiguation algorithm which is based on the most similar", "cit": "[CLS] most researchers working on word sense disambiguation (wsd) use manually sense tagged data such as sem- cor #refr to"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] our work is closely related to recent studies on detecting subgroups from online discussions #otherefr; #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] in the context of machine translation, itg has been explored for a variety of nlp tasks such as pos", "cit": "[CLS] however, structured features have only been applied to a handful of nlp tasks such as semantic role labeling #refr,"}
{"pre": "[CLS] id participant cmu carnegie mellon university #otherefr cu-bojar charles university - bojar #refr cu-depfix charles university - depfix #otherefr", "cit": "[CLS] id participant cmu carnegie mellon university #otherefr ug university of toronto #refr uk charles university - zeman #otherefr commercial-[1,2]"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] 12python code for these algorithms appears in #refr and the accompanying software release. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the word alignment of word alignment of word alignment using giza++ #refr, which", "cit": "[CLS] among the possible applications, souza et al#otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence and the english and", "cit": "[CLS] all features weights were optimized using lattice-based minimum error rate training #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the case of dependency parsing, previous work has shown that it is possible to improve the performance of", "cit": "[CLS] even if the project was discontinued, it is still heavily used as part of the venerable conll 2006 and"}
{"pre": "[CLS] we use the averaged perceptron algorithm #refr to train the parameters of the feature weights for the log-linear models.", "cit": "[CLS] they showed that jointly learning these related tasks lead to overall improvements. #refr applies similar methods for machine transliteration."}
{"pre": "[CLS] the most common approach to paraphrase generation is to use a lexicon to paraphrase database into a target language", "cit": "[CLS] previous works usually take a generative approach, #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] we adopted the case-insensitive bleu- 4 #refr as the evaluation metric, which uses the shortest reference sentence length for"}
{"pre": "[CLS] the wsd system is based on the standard approach of #refr and the best performing wsd system for wsd", "cit": "[CLS] the isi rewrite decoder #refr, which implements an efficient greedy decoding algorithm, is used to translate the chinese sentences,"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] algorithm with the best derivation of the best derivation", "cit": "[CLS] although speakers may retrace much of the reparandum for clarity or other reasons, ideally the reparandum contains nothing but"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] indeed, successful semantic parsers often resemble mt systems in several important respects, including the use of word alignment models"}
{"pre": "[CLS] the first is the case for the automatic evaluation of the automatic evaluation of the automatic evaluation of the", "cit": "[CLS] among those who have studied the problem of tense and/or aspect, many have taken the work of \\[reichenbach, 1947\\]"}
{"pre": "[CLS] the model is trained using the averaged perceptron algorithm #refr. [SEP] score #refr with the best derivation forest, [SEP]", "cit": "[CLS] this algorithm is really an extension of viterbi to the case when scores factor over dynamic substrings of the"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to translate the phrases as the", "cit": "[CLS] hiero search refinements #refr offer several refinements to cube pruning to improve translation speed. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the sentence planning problem that the user is a text from a", "cit": "[CLS] the transition from the former to the later can be regarded as a scaling process, since virtually every processing"}
{"pre": "[CLS] the task of event extraction has received a lot of attention in recent years #otherefr; #refr. [SEP] [SEP] [SEP]", "cit": "[CLS] there are also some approaches regarding entity linking as a ranking task #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in this paper, we propose a method for learning the context of transliteration pairs from the web #otherefr; #refr.", "cit": "[CLS] the results were evaluated using the character/pinyin-based 4-gram bleu score #refr, word error rate (wer), position independent word error"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with a probability distribution", "cit": "[CLS] #refr demonstrated that linguistically informed splitting of non-terminal symbols in treebank-derived grammars can result in accurate grammars. [SEP] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders and opinion holders in opinion holders and opinion holders in", "cit": "[CLS] however, our approach can be readily used in tandem with a system that extracts opinion expressions (e.g., #refr, breck"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] for example, in #refr #otherefr, the naive bayes algorithm is used to separate positive reviews from negative ones. [SEP]"}
{"pre": "[CLS] the second dataset is the same as the use of the same as the same as the same as", "cit": "[CLS] from analysis of manually sense tagged corpora, #refr has demonstrated that distributions of the senses of words are often"}
{"pre": "[CLS] we use the moses toolkit #refr to train the weights for the feature weights for the log-linear model with", "cit": "[CLS] left-to-right (lr) decoding #refrb) is a promising decoding algorithm for hierarchical phrase-based translation (hiero). [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal, parser #refr and the penn treebank #otherefr. [SEP]", "cit": "[CLS] the process generates many nonsensical hypotheses unless we restrict the search space either by heuristies-based cost functions \\[#refr\\], or"}
{"pre": "[CLS] the second approach is to use a small number of seed examples for example outputs #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] the perception has been that this cannot be done reliably #otherefr; #refr (sec. 5). [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the best results for the best", "cit": "[CLS] charniak and johnson #otherefr and #refr use sub-tree features, all of which we plan to try in future work."}
{"pre": "[CLS] the first is the annotation tool which was the annotation tool #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] a propbank has been built on top of the penn treebank, and has been used to train ?semantic taggers?,"}
{"pre": "[CLS] in this paper, we propose a method for learning a word alignment of word alignments from a small set", "cit": "[CLS] #refr pointed out that, with the presence of a dictionary, even an incomplete one, a modest pos tagger can"}
{"pre": "[CLS] the first is the task of identifying the event extraction of a single document to be a single document", "cit": "[CLS] applying dirt on a ts dependency representations like the tree skeleton have been explored by many researchers, e.g. #refr"}
{"pre": "[CLS] the first is the task of identifying the polarity of a sentence, a sentence, a document into a document", "cit": "[CLS] #refr presented an approach to extract relations from the web using minimal supervision. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr proposed a method for learning a word clustering based on a probabilistic model that learns vector space representations", "cit": "[CLS] sentence compression methods have been devised using manually crafted rules #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford named entity recognizer #refr to extract named entities from the set of named entities from", "cit": "[CLS] sorting by computing the paretofrontier has been applied to training machine translation systems #refr to combine the translation quality"}
{"pre": "[CLS] the grammar is a grammar that is a grammar that is a parsing model that is a word lattice", "cit": "[CLS] the lingo grammar matrix customization system #refr is a web-based tool that creates starter grammars based on users? input"}
{"pre": "[CLS] the approach of #refr is based on the assumption that the assumption of the assumption that the assumption of", "cit": "[CLS] for the language model, we use an n-grammodel, which is remarkably useful in ranking candidate generated sentences #otherefr; #refr."}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] recent semeval tasks #otherefr; #refr pursue this more open-ended strategy. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second dataset is the use of a word sense disambiguation algorithm which is based on the most similar", "cit": "[CLS] we use the implementation in #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of identifying positive and negative sentiments from a text #otherefr; #refr. [SEP] of a text that is", "cit": "[CLS] several studies utilize some preprocessing, including parsing #otherefr and usage of syntactic #refr and morphological #otherefr information in patterns."}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] sentence pair of words", "cit": "[CLS] two limitations of this method are: (1) use of only a biological corpus, so 1http://www-tsujii.is.s.u-tokyo.ac.jp/genia/ that the general domain-independent"}
{"pre": "[CLS] the model is trained using the averaged perceptron algorithm #refr. [SEP] score #refr with the best alignment model with", "cit": "[CLS] some authors have attempted to automatically detect cognate words #otherefr; #refr, but these methods typically work on language pairs"}
{"pre": "[CLS] the most popular approach to translation models #otherefr; #refr rely on large amounts of training data from the training", "cit": "[CLS] recent models use linear or svm regression and train them against human judgments to automatic learn feature weights, and"}
{"pre": "[CLS] the first is the automatic metric of text with human annotation of text segments as shown in #refr. [SEP]", "cit": "[CLS] with its set of linguistic filters, our frequency-based summarizer can exploit linguistic dimensions beyond single word analysis; this is"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] #refr reported how serious this problem can be when he coupled a tagger with a subsequent parser, and noted"}
{"pre": "[CLS] the system is based on the inprotk toolkit for incremental dialogue systems developed by #refr. [SEP] grammars #otherefr used", "cit": "[CLS] both systems? output for the 9 scenarios was recorded with a screenrecorder, resulting in 18 videos that were played"}
{"pre": "[CLS] we use the stanford ner tagger #refr to train the classifier to generate the feature templates and the parameters", "cit": "[CLS] for example, both haghighi and klein #otherefr and #refr have demonstrated results better than 66.1% on the apartments task"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] in our setup, we apply maltparser #otherefr and optimize feature models and learning parameters using maltoptimizer #refr. [SEP] [PAD]"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] we use the features of #refr, except that all lexical identities are dropped from the templates during training and"}
{"pre": "[CLS] #refr proposed a joint model for unsupervised learning of pos tagging and pos tagging and pos tagging and pos", "cit": "[CLS] #refr, zhang et al. #otherefra) experiment with co-training for semi-supervised chinese word segmentation. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to identify the polarity of words in a sentence. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] lexpagerank #refr is an approach for computing sentence importance based on the concept of eigenvector centrality. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to learn a translation probabilities from", "cit": "[CLS] jump models #otherefr; #refr predict the direction and length of a jump to perform after a given input word2."}
{"pre": "[CLS] the first is the case for the automatic evaluation of the automatic evaluation of the automatic evaluation of the", "cit": "[CLS] with the availability of large corpora and memory devices, there is once again growing interest in extracting n-grams with"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for each sentence in the training corpus.", "cit": "[CLS] iii and marcu, 2004), named entity recognition #otherefr; #refr and relation extraction #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the training data is to", "cit": "[CLS] nonetheless, the problem of data sparsity remainsit is difficult even for latent variable parsers to learn accurate patterns based"}
{"pre": "[CLS] in the context of machine translation, itg has been explored for applications in machine translation #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] recently, several works have leveraged the web for improved spelling correction #otherefr, #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a supervised approach to identify the polarity of words in a sentence. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] a rationale #refr is the span of text a user highlights in support of his/her annotation. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the system used in the conll shared task #refr is based on coreference and the genia corpus #otherefr. [SEP]", "cit": "[CLS] in fact, modern systems try to account for complex coreference links by incorporating lexicographic and world knowledge, for example,"}
{"pre": "[CLS] the second dataset is the same as the use of the same as the same as the one used", "cit": "[CLS] the subtleties of sense distinctions captured by wordnet#otherefr are helpful for language learners #refr and in machine translation of"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be np-hard #refr. [SEP] s", "cit": "[CLS] phrase-tables are filtered using entropy-based pruning #refr as implemented in moses. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to parsing is to use a word lattice parsing model that of a dependency parser", "cit": "[CLS] to facilitate comparisons with previous work #otherefr; #refr, which represents a balance between strong performance and fast training times."}
{"pre": "[CLS] the first is the case for the semantic role labeling task of the predicate-argument structure of the process of", "cit": "[CLS] lhip (left-corner head-driven island parser) #refr is a system which performs robust analysis of its input, using a grammar"}
{"pre": "[CLS] we use the stanford named entity recognizer #refr to extract named entities from the text and extract named entities", "cit": "[CLS] also we plan to compare svm against methods such as crf and maximum entropy which have proved successful in"}
{"pre": "[CLS] in the context of machine learning techniques have been applied to many natural language processing tasks, including machine translation", "cit": "[CLS] for further perspective on these results, note chodorow et al #otherefr achieved 69% with 7m training examples, while #refr"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] in #refr, shallow syntactic analysis such as pos tagging and morphological analysis were incorporated in a phrasal decoder. [SEP]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] 2 in this paper, we used the berkeley parser #refr for learning these structures. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] in order to avoid spurious ambiguities, we restrict our derivations to be normal-form #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained with the perceptron algorithm of #refr. [SEP] score entire sequence models and the entire sequence", "cit": "[CLS] aone and bennett #otherefr; ng and #refr). [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the longrange reordering problem as a sequence of words", "cit": "[CLS] finally, the work that is the closest to ours in spirit is the idea of joint estimation #refr. [SEP]"}
{"pre": "[CLS] we use the stanford parser #refr to extract dependency relations for each sentence and parse trees from the sentence", "cit": "[CLS] the convenience and efficiency of reading transcripts #otherefr; #refr: trading off the expected salience of excerpts with their recognition-error"}
{"pre": "[CLS] the features used in the experiments are the coreference resolution system of ng and #refr. [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] regarding the evaluation, the scorer uses the modification of #refr, unprecedented so far, and the corpora was published very"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] hassan et al#otherefr), whereas source-language labels provide suitable context for reordering (see #refr). [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the system is based on the stanford parser #refr and the stanford parser #otherefr. [SEP] system #otherefr. [SEP] system", "cit": "[CLS] puls?s english pattern base has over 150 patterns for the epidemics domain, and over 300 patterns for security.9 these"}
{"pre": "[CLS] in this paper, we propose a method for estimating the translation model from the bilingual lexicon induced from monolingual", "cit": "[CLS] #refr test the ?one sense per discourse? hypothesis #otherefr for mt and find that enforcing it as a constraint"}
{"pre": "[CLS] the first is the one of the most popular approaches to automatically identify the text that use of the", "cit": "[CLS] verb type seed sets using the lcs database #otherefr verbs, e.g. alter, knock, resign), or mixed (215 verbs, e.g."}
{"pre": "[CLS] #refr and toutanova et al #otherefr proposed a method for learning a dependency parser based on a probabilistic model", "cit": "[CLS] since sentence structure is underdetermined by raw text, there have been efforts to simplify the task, via (i) pooling"}
{"pre": "[CLS] the most popular approach to translation lexicons from parallel corpora #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ?", "cit": "[CLS] glmd extends the glm framework by parallel perceptron training #refr and dynamic learning with adaptive weight updates in the"}
{"pre": "[CLS] the first is the annotation tool which was the annotation tool for the annotation tool #refr. [SEP] project at", "cit": "[CLS] recently created corpora include ontonotes #otherefr, and the manually annotated sub-corpus (masc)1 #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal (wsj) treebank #otherefr; #refr. [SEP] trees and the", "cit": "[CLS] we use relative conditional probability thresholds (1% and 5%) to filter the selection of semantic forms (table 4). (o?dono#refr"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a text that are semantically", "cit": "[CLS] in order to address this, #refr define the notion of weak net and conjecture that it covers all semantically"}
{"pre": "[CLS] the most popular approach to translation models #otherefr; #refr rely on parallel data #otherefr. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] for example, de #refr combined systems trained with different tokenizations. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the generation of referring expressions is a subject of research in natural language generation (nlg) in natural language generation", "cit": "[CLS] once the user is words have been interpreted, a layer of production rules constructs obligations for response #refr; then,"}
{"pre": "[CLS] #refr use a similar approach to identify the polarity of words in a sentence and their polarity in a", "cit": "[CLS] a variety of approaches exist for determining the salient sentences in the text: statistical techniques based oll word distribution"}
{"pre": "[CLS] the feature weights were tuned using mert #refr on the development set. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] other relevant work in nlp includes #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the overall sentence pairs from the same text with a text #refr.", "cit": "[CLS] studying the errors in learner language is undertaken in the form 1http://www.cambridge.org/gb/elt of error annotation like in the projects"}
{"pre": "[CLS] the first is the stanford ner tagger #refr and the stanford parser #otherefr. [SEP] shared task on coreference in", "cit": "[CLS] the publicly available systems we used are: berkeley #otherefr, uiuc #refr, and cherrypicker #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] one could remove potentially unfavorable sentence pairs when training a statistical mt system, to avoid incorrect word alignments #refr,"}
{"pre": "[CLS] we use the stanford corenlp suite to lemmatize and partof-speech tag each word #otherefr; #refr, and parse the input", "cit": "[CLS] chen et al #otherefr participated with their srl system semafor #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the feature weights were tuned using mert #refr on the development set. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] we build two translation systems: one using tree-based models without additional linguistic annotation, which are known as hierarchical phrasebased"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] meanwhile, work on semantic parsing has focused on producing semantic parsers for answering simple natural language questions #otherefr; #refr."}
{"pre": "[CLS] the system uses a combination of natural language processing (nlp) systems to produce various types of syntactic parsing and", "cit": "[CLS] #refr propose an alternative evaluation scheme which, along with other properties, overcomes this by operating in terms of tokens"}
{"pre": "[CLS] the second dataset is the use of a word sense disambiguation algorithm which is based on the most widely", "cit": "[CLS] the semeval-2007 task 7 dataset for coarsegrained english all-words wsd #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within the source sentence into a target sentence into", "cit": "[CLS] on the other hand, graf can be used as a pivot format between other formats #refr, e.g. there is"}
{"pre": "[CLS] the model is trained using the averaged perceptron algorithm #refr. [SEP] score #refr with the best alignment model with", "cit": "[CLS] clustering words into classes has been used to overcome data sparseness in word-based language models #otherefr; #refr. [SEP] [PAD]"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? the best performing", "cit": "[CLS] as is customary in unsupervised parsing work (e.g. #refr), we bounded sentence length by 10 (excluding punctuation). [SEP] [PAD]"}
{"pre": "[CLS] the second approach is to use a small number of seed examples for example outputs #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] the nlp community has recently devoted much attention to developing accurate and robust methods for performing role-semantic analysis automatically,"}
{"pre": "[CLS] the most popular approach to word alignment is the hmm alignment model #refr. [SEP] model is based on the", "cit": "[CLS] the existing nlp task for inuktitut is that of word alignment #refr, where inuktitut tokens align to entire english"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr.", "cit": "[CLS] dependency representations for srl #refr were made popular by conll-2008 and conll-2009 shared tasks #otherefr, but for english were"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a text that are semantically", "cit": "[CLS] for lexical freenet, #refr adds over 350 000 collocation pairs (trigger pairs) extracted from a 160 million word corpus"}
{"pre": "[CLS] the first is the automatic evaluation of discourse structure of discourse structure of discourse structure of discourse structure of", "cit": "[CLS] different machine learning algorithms have been used, including memory-based learners #refr, support vector machines #otherefr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] our system also compares favourably with the system of carreras et al #otherefr that relies on a more complex"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] using such techniques #otherefr; candito and #refr on the french treebank #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the taxonomy of identifying the polarity of a text that is a", "cit": "[CLS] the first-sense heuristic can be thought of as striving for maximal specificity at the risk of precluding some admissible"}
{"pre": "[CLS] the translation quality is evaluated by case-insensitive bleu-4 #refr. [SEP] quality measured by bleu score #refr with the bleu", "cit": "[CLS] this preprocessing step can be accomplished by applying the giza++ toolkit #refr that provides viterbi alignments based on ibm"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] three participating teams use the mate parser #otherefr in their systems: the basqueteam #refr, igm:alpage #otherefr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to translate the words and the", "cit": "[CLS] the feature weights of the translation system are tuned with the standard minimum-error-ratetraining #refr to maximize the systems bleu"}
{"pre": "[CLS] the second dataset is the use of a word sense disambiguation task as a sense disambiguation task #refr, which", "cit": "[CLS] however, human annotators often disagree about which sense is present #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] id participant cmu carnegie mellon university #otherefr cu-bojar charles university - bojar #refr cu-depfix charles university - depfix #otherefr", "cit": "[CLS] metric ids participant amber national research council canada #otherefr simbleu university of sheffield #refr spede stanford university #otherefr [SEP]"}
{"pre": "[CLS] #refr use a similar approach to compute similarities between words in a sentence. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] it was parsed with the stanford parser #refr and includes a total of 215,154 unique phrases from those parse"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for each sentence with dependency trees for", "cit": "[CLS] in recent years, syntax-based smt has made promising progress by employing either dependency parsing #otherefr; #refr on the source"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for each sentence in the training corpus.", "cit": "[CLS] while em has worked quite well for a few tasks, notably machine translations #otherefr, named-entity recognition #refr and context-free-grammar"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a text that are semantically", "cit": "[CLS] #refr described a method for automatically clustering words according to their distribution in particular syntactic ontext, for example verbs"}
{"pre": "[CLS] the parser is trained on the penn treebank corpus and tested on the penn treebank wall street journal, parser", "cit": "[CLS] in addition, many more sophisticated parsing models are elaborations of such pcfg models, so understanding the properties of pcfgs"}
{"pre": "[CLS] the system is based on the stanford ner tagger #refr and the stanford parser #otherefr. [SEP] system #otherefr. [SEP]", "cit": "[CLS] semantic knowledge like word associations was involved by #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford ner tagger #refr to train the training data with the training data and test data", "cit": "[CLS] examples include question answering #otherefr; #refr, dialog systems #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] xle selects the most probable analysis from the potentially large candidate set by means of a stochastic disambiguation component"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] used in text structuring research #refr, texts in 6insertion is only one type of recorded update, others include deletions"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] in the recent work of #refr, the values of these parameters were tuned to be #otherefr arabic-english evaluation datasets."}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] for instance, #refr detected verb-particle constructions in raw texts with the help of information based on pos-tagging and chunking,"}
{"pre": "[CLS] the first is the automatic evaluation of the automatic metric of the automatic evaluation of the automatic evaluation of", "cit": "[CLS] a further example of corpus-based identification of light verb constructions in english is described in #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] #refr proposed a method for extracting parallel sentences from parallel corpora. [SEP] words from the same sentence pairs of", "cit": "[CLS] #refr report low overlapping between vocabulary of natural english documents and the vocabulary of documents translated to english from"}
{"pre": "[CLS] we use the stanford parser #refr to extract dependency relations for each word and parse trees from the sentence", "cit": "[CLS] meanwhile, it is common for np chunking tasks to represent a chunk (e.g., np) with two labels, the begin"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a text with a text corpus consisting of", "cit": "[CLS] although many proposals have been made #refr, there is no real agreement in the nlp community about the evaluation"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a text that is a", "cit": "[CLS] recently, there has been interest in corpus-based methods to identify alternations #otherefr; #refr, and associated verb classifications #otherefr. [SEP]"}
{"pre": "[CLS] the dependency parser is trained on the penn treebank #refr and the penn treebank #otherefr. [SEP] model #otherefr with", "cit": "[CLS] in addition, before the conll-2008 shared task there have been a few attempts to jointly learn syntactic and semantic"}
{"pre": "[CLS] the task of identifying the event descriptions of event extraction and event descriptions has been well studied in the", "cit": "[CLS] #refr deal with learning contextual patterns for extracting family relationships from wikipedia. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to parsing is to use a linear model and maximise the structure to parse tree", "cit": "[CLS] we used yam- cha #refr as a text chunker, which is based on support vector machine (svm). [SEP] [PAD]"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] in the cross-lingual study of #refr, where delexicalized parsing models from a number of source languages were evaluated on"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with the one of", "cit": "[CLS] previous research on english shows that structured perceptron #refr is one of the strongest machine learning algorithms for parse"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] #refr, in contrast, took the syntactic relationship between co-occurring words into account: the distributional features of a word are"}
{"pre": "[CLS] the most popular approach to translation lexicons from parallel corpora #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] we are inspired by previous work in unsupervised learning for natural language, e.g. #refr; wfsa - a wfst -"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] 1recently, a similar task has been addressed by the affective text task at semeval-1 where even shorter units ?"}
{"pre": "[CLS] #refr use a similar approach to compute the polarity of the polarity of words in a sentence. [SEP] [SEP]", "cit": "[CLS] for english we seed the bootstrapping process with the strongly subjective terms from the mpqa lexicon3 #refr. [SEP] [PAD]"}
{"pre": "[CLS] the most common approach to this problem is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] convolution kernels have been used before in sentiment analysis: wiegand and klakow #otherefr for opinion expression detection and #refr"}
{"pre": "[CLS] the task of identifying the semantic textual similarity of a document is a document level #refr. [SEP] of a", "cit": "[CLS] we note that a similar lda model construction was employed also in (se?#refr, for estimating predicate-argument likelihood. [SEP] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the semantic textual similarity of a text of a text of a", "cit": "[CLS] macro-averaging in repa thus assumes that senses are equally distributed, which is an oversimplification, as word senses are known"}
{"pre": "[CLS] the first is the case for the case of the grammar formalism used by #refr, which is the most", "cit": "[CLS] the computational model is based on #refr dependency parsing algorithm. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to smt is the one proposed by #refr, who use a large set of features", "cit": "[CLS] this observation has been used in the development of automatic evaluation metrics #refr, and is something we hope to"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for parsing #refr. [SEP] grammars with", "cit": "[CLS] the time column show how many seconds per sentence each parser takes.7 approach uas las time zhang and clark"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be np-hard #refr. [SEP] s", "cit": "[CLS] we propose a new rule extraction algorithm based on ghkm #refr that directly induces a synchronous tag from an"}
{"pre": "[CLS] we use the stanford corenlp suite to lemmatize and partof-speech tag each word #otherefr; #refr, and train a word", "cit": "[CLS] given a suitable pairing between words and semantic representations of them, this method can produce structured sentential representations with"}
{"pre": "[CLS] #refr use a similar approach to compute the polarity of subjective nouns in the polarity of subjective nouns in", "cit": "[CLS] the advent of online social networks has produced a crescent interest on the task of sentiment analysis for short"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] recent advances #refr; talbot and osborne, 2007b) involve the development of bloom filter based models, which allow a considerable"}
{"pre": "[CLS] the second approach is to use a maximum entropy model #otherefr; #refr. [SEP] model for the probability distribution over", "cit": "[CLS] the parsers are: the berkeley parser with gold pos tags as input (berk-g), the berkeley product parser with two"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the training data is to", "cit": "[CLS] the shared tasks of multi-lingual dependency parsing took place at conll-2006 #refr and conll-2007 #otherefr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of possible antecedents for the", "cit": "[CLS] however, supervised approaches require huge amounts of annotated data #refr, an effort which cannot easily be repeated for new"}
{"pre": "[CLS] #refr proposed a method for learning a word clustering based on a probabilistic model that learns vector space representations", "cit": "[CLS] for comparison, our best model, the pl- mrf, achieved a 96.8% in-domain accuracy on sections 22-24 of the penn"}
{"pre": "[CLS] the entity mention extraction #otherefr; #refr) is an important step in which the most popular in the most popular", "cit": "[CLS] pleonastic its have been detected using heuristics #otherefr) and learning-based techniques such as rule learning (e.g., mu?#refr), kernels #otherefr)."}
{"pre": "[CLS] the task of event extraction was first introduced by #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] for this reason, kernels have been recently used to develop innovative approaches to relation extraction based on syntactic information,"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] both techniques implement variations on the approaches of #otherefr and #refr for the purpose of differentiating between complement and"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with a probability distribution", "cit": "[CLS] a note on related work studies on parsing mh to date concentrate mostly on spelling out the integration of"}
{"pre": "[CLS] the most common approach to word alignment is to use a word alignment model that is to model that", "cit": "[CLS] the baselines 1 and 2 use the dependency parsing method proposed by uchimoto et al. #otherefr and the dependency"}
{"pre": "[CLS] the system is based on the inprotk project #refr. [SEP] score for the number of the number of the", "cit": "[CLS] also, relatedly, #refr use a statistical language model to train svm classifiers to classify text for grade levels 2-5."}
{"pre": "[CLS] in the context of machine learning techniques have been applied to many natural language processing tasks, including machine translation", "cit": "[CLS] we finally look at the annotated metaphor corpus by #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to parsing is to use a linear model trained on a maximum spanning tree (mst)", "cit": "[CLS] not limited to parsing, supertags can be used for np chunking #otherefr and machine translation #refr to explore rich"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a text with a text corpus of english", "cit": "[CLS] for example, #refr used a thesaurus to generate 1042 statistical models of the most general categories. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the system is based on the moses toolkit #otherefr and the moses toolkit #refr. [SEP] decoder with the moses", "cit": "[CLS] for the system combination task, we first use the minimum bayes-risk (mbr) #refr decoder to select the best hypothesis"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be a given by using", "cit": "[CLS] for instance, we may find metrics based on full constituent parsing #refr, and on dependency parsing #otherefr. [SEP] [PAD]"}
{"pre": "[CLS] we use the moses toolkit #refr to train the weights for the feature weights for the log-linear model with", "cit": "[CLS] shallow-n grammars #otherefr were introduced to reduce over-generation in the hiero translation model #refr resulting in much faster decoding"}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders and predict the polarity of words in a sentence and", "cit": "[CLS] in evaluations of summarization algorithms, it is common practice to derive the gold standard content importance scores from human"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] for this reason, to our knowledge, all discriminative models proposed to date either side-step the problem by choosing simple"}
{"pre": "[CLS] the first is the case for the automatic evaluation of the automatic evaluation of the automatic evaluation of the", "cit": "[CLS] the speed and cost benefits for annotation are certainly impressive #otherefr; #refr but we hope to show that some"}
{"pre": "[CLS] the first is the automatic evaluation of discourse structure of discourse structure of discourse structure of discourse structure of", "cit": "[CLS] in general this problem can be solved by using tree-banks for mwe extraction, #otherefr; #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking approaches #otherefr; #refr to learning for dependency", "cit": "[CLS] there has been a growing interest in dependency parsing in recent years. #refr found that the dependency structures of"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] section 4 reports the results of the experiments conducted on the penn treebank #refr as well as on the"}
{"pre": "[CLS] the model is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the best results for the best", "cit": "[CLS] the first feature ?0(q(r), s) is calculated with a pcfg-based generative parsing model #refr, as defined in (4) below,"}
{"pre": "[CLS] the first is the task of converting natural language processing #otherefr; #refr. [SEP] [SEP] [SEP] to be a number", "cit": "[CLS] earlier research on inside-outside estimation of pcfg models has reported some positive results as well #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the system uses a combination of the maximum entropy model of #refr to generate a set of features from", "cit": "[CLS] automatic acquisition of scfs has therfore been an active research area since the mid-90s #otherefr; #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] we use the averaged perceptron algorithm #refr to train the parameters of the averaged perceptron algorithm with the averaged", "cit": "[CLS] uses for k-best lists include minimum bayes risk decoding #otherefr; #refr, discriminative reranking #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the topic of topic models has been studied in the literature #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] other examples include nlp research on 10all 50 can be found in the supplemental materials at http://umiacs.umd.edu/?resnik/papers/ emnlp2013-supplemental/. autistic"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a task of identifying the degree of semantic relations between words", "cit": "[CLS] the resulting english corpus contained 10,000 sentences which were syntactically parsed #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second approach is to use a small number of seed examples for example outputs and automatic word sense", "cit": "[CLS] we can see that the recall has increased significantly, and is now closer to the mfs baseline, which is"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? 2008. [SEP] ?", "cit": "[CLS] these include syntactic, semantic and mixed syntacticsemantic classifications #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the automatic evaluation of discourse coherence models in discourse structure of the discourse structure of discourse", "cit": "[CLS] #refr discuss the relevance of syntactic structure to the perception of sentiment. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a bootstrapping method to identify the polarity of words in a text in a text to determine", "cit": "[CLS] this task has been approached by investigating methodologies for identifying meaningful topics through semantic coherence #otherefr; #refr and for"}
{"pre": "[CLS] the model is trained using the averaged perceptron algorithm #refr. [SEP] score #refr with the best alignment algorithm #otherefr.", "cit": "[CLS] ilp has been used extensively for applications ranging from text-to-text generation #otherefr; #refr to dependency parsing #otherefr. [SEP] [PAD]"}
{"pre": "[CLS] the translation system is evaluated using bleu #refr, which is based on the quality of the translation model scores", "cit": "[CLS] many strategies have been proposed to integrate morphology information in smt, including factored translation models #otherefr, and using porter"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] every sentence was part-of-speech tagged using a maximum entropy tagger #refr and parsed using a state-of-the-art wide coverage phrase"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a text that is a", "cit": "[CLS] to date research in computational linguistics using wikipedia includes: automatic derivation of taxonomy information #otherefr and relation extraction #refr."}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for parsing #otherefr; #refr for parsing", "cit": "[CLS] an important part of this research effort are the conll 2006 and 2007 shared tasks #otherefrb; #refr), which combine"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal, parser #refr and tested on the penn treebank", "cit": "[CLS] several features of the tool have been introduced to suite the requirements imposed by the architecture of the annotation"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the sentence and", "cit": "[CLS] their work used ngrams, mpqa opinion words #refr, liwc #otherefr, and a different dataset (q/r instead of p1,p2 datasets),"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with the one of", "cit": "[CLS] al- type system lp lr f pos our systems pipeline 80.0 80.3 80.1 94.0 jointparsing 82.4 83.0 82.7 95.1"}
{"pre": "[CLS] the bionlp shared task (bionlp-st, hereafter) series has been instrumental in the bionlp 2009 shared task data for a", "cit": "[CLS] for some genres of text #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] our approach improves upon #refrb) state-of-theart decoder, creating a 28.5% relative improvement in transliteration accuracy on a japanese katakana-to-english"}
{"pre": "[CLS] #refr use a similar approach to compute the semantic similarity between words by a sentence. [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] because of a dearth of resources for this fine-grained task, we also develop new crowdsourcing techniques for labeling word-level,"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] sentence pair of words", "cit": "[CLS] our previous studies used machine learning and nlp techniques to automatically identify the presence of such useful features in"}
{"pre": "[CLS] the second approach is to use a small number of approximate inference rules #otherefr; #refr. [SEP] is to avoid", "cit": "[CLS] it is a exible method which is easily extended to various tasks and domains, and it has been applied"}
{"pre": "[CLS] the most popular approach to translation models #otherefr; #refr rely on parallel data #otherefr. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] this is motivated by studies that showed that the coverage of translation rules is critical to smt #refr. [SEP]"}
{"pre": "[CLS] the model is trained using the giza++ implementation of ibm model 4 alignments #refr. [SEP] model 4 alignments were", "cit": "[CLS] examples include the maximum entropy model of #refr or the conditional random field jointly normalized over the entire sequence"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to learn a translation models from a", "cit": "[CLS] word class-based reordering was a part of och?s alignment template system #refr; the main criticism of this approach is"}
{"pre": "[CLS] the first is the case for the automatic evaluation of the automatic evaluation of discourse structure of discourse structure", "cit": "[CLS] we used florian and ngai?s fast tbl system (fntbl) #refr to train rules using disfluency annotated conversational speech data."}
{"pre": "[CLS] the work of #refr is similar to ours in that it is possible to use a more complex model", "cit": "[CLS] humor recognition includes double entendre identification in the form of that?s what she said jokes #otherefr, and one-liner joke"}
{"pre": "[CLS] the task of identifying the semantic textual similarity of a text is a document #otherefr; #refr. [SEP] of a", "cit": "[CLS] leveraging structured, relational data should allow systems to achieve strong accuracy, as with domain-specific or database-specific ned techniques like"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? ? ? ?", "cit": "[CLS] supervised semantic parsers #otherefr; ge and #refr rely on manual annotation of logical forms, which is expensive. [SEP] [PAD]"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model #refr to learn a translation probabilities from", "cit": "[CLS] our reordering system is inspired by the direction taken in #refr, but differs in defining the space of permutations,"}
{"pre": "[CLS] the system is based on the inprotk toolkit for the paradise system described in #refr. [SEP] system, which has", "cit": "[CLS] a similar assumption is made by #refr, who view pointing gesturs as mainly concerned with the ?where? of an"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal (wsj) corpus #refr. [SEP] trees and the penn", "cit": "[CLS] nivre?s arc-eager dependency parser #otherefr is one of the most widely known and used transition-based parsers (see for example"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] recent work in machine translation has evolved from the traditional word #otherefr and phrase based #refra) models to include"}
{"pre": "[CLS] the most popular approach to translation models #otherefr; #refr rely on parallel data #otherefr. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] table 2: german-english monolingual corpora statistics: the number of tokens is given in millions [m], ppl is the perplexity"}
{"pre": "[CLS] the work of #refr is similar to ours in that it is possible to use a more complex model", "cit": "[CLS] finally, it would be interesting to explore which of these dimensions of state actually matter most for dialog success"}
{"pre": "[CLS] the second approach is to use a small number of approximate inference rules, and unsupervised methods #otherefr; #refr. [SEP]", "cit": "[CLS] for the atb, we use the same split as #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the automatic evaluation of the automatic metric of the automatic evaluation of the automatic evaluation of", "cit": "[CLS] also, a score distribution heavily skewed towards zero makes meta-analyses of evaluation stability difficult to perform #refr. [SEP] [PAD]"}
{"pre": "[CLS] the first is the nlg community #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] , which is the", "cit": "[CLS] inequalities are more 6note, by contrast, that vague properties tend to be realized first #otherefr, #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] it has been remarked that hmm trigram taggers using single multivalued context features can perform better and run faster"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? 2008. [SEP] ?", "cit": "[CLS] information about verb subcategorization is useful for tasks like information extraction #otherefr and parsing #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] the others are always statistically significant at p?0.005, calculated with approximate randomization #refr. advantage of test data with a"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best results for the best", "cit": "[CLS] the approach proposed by #refr is also related as they propose a map adaptation via gaussian priors of a"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] simultaneously, mounting efforts have been directed towards smt models employing linguistic syntax on the source side #refr, target side"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] in #otherefr; #refr, people presented models that use lexical features from the phrases to predict their orientations. [SEP] [PAD]"}
{"pre": "[CLS] the first is the annotation tool which was the annotation tool for the annotation tool #refr. [SEP] project in", "cit": "[CLS] there is a long-standing interest in temporal reasoning within the biomedical community #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the berkeley parser #refr to obtain the syntactic parse trees for each sentence and the sentence and", "cit": "[CLS] #refr took the source constituency tree into account and added soft constraints to the hierarchical phrasebased model #otherefr. [SEP]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] the vectors for nonterminals are computed via a new type of recursive neural network which is conditioned on syntactic"}
{"pre": "[CLS] the system is based on the inprotk toolkit for the best performing system described in #refr. [SEP] system, which", "cit": "[CLS] choi #otherefr; #refr). [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the syntactic analysis of the english lexical substitution task of the penn treebank", "cit": "[CLS] our work differs from the muc-related ne tagging task and its possible extension to name indexing of web pages"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to learn a translation models from a", "cit": "[CLS] the goal of integrating syntactic information into the translation model has prompted many researchers to pursue tree-based transfer models"}
{"pre": "[CLS] in this paper, we present a bayesian approach to pos tagging in which pos tags and use a maximum", "cit": "[CLS] for work on l-pcfgs estimated with em, see #refr, matsuzaki et al #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to learn a translation models from a", "cit": "[CLS] a set of language-dependent heuristics was applied in an attempt to restore the opening/closing quotation marks (i.e. \"quoted\" ?"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to learn a translation models from a", "cit": "[CLS] results are reported using caseinsensitive bleu with a single reference and no punctuation #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] id participant cmu carnegie mellon university #otherefr cu-bojar charles university - bojar #refr cu-depfix charles university - depfix #otherefr", "cit": "[CLS] this has the added advantage of eliminating the criticism made of wmt evaluations that systems sometimes gain advantage from"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a natural language generation of an nlg system", "cit": "[CLS] preliminary machine learning experiments involving emotion annotation and automatic feature extraction from our corpus suggest that itspoke can indeed"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of words in the same", "cit": "[CLS] categories roughly correspond to coarse senses of a word #refr, and the two terms will be used interchangeably. [SEP]"}
{"pre": "[CLS] the second approach is to use a small number of seed examples for example outputs #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] indeed, it is a field where more is almost always better, as indicated by the traditional use of named"}
{"pre": "[CLS] we use the moses toolkit #refr to train the weights for the weights for the log-linear model with minimum", "cit": "[CLS] several approaches have been proposed to address these issues: from filtering the extracted synchronous grammar #otherefr; #refr. [SEP] [PAD]"}
{"pre": "[CLS] the grammar we use is a standard grammar for the grammar development and test and test data from the", "cit": "[CLS] one approach to grammar-learning is data- oriented parsing (dop), whose strategy is to simply take all subtrees in the"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with a probability distribution", "cit": "[CLS] when looking at the related task of finding the most likely assignment in large graphical models #otherefr; #refr. [SEP]"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] the opennlp and the stanford parser #refr are used to extract linguistic features for the baselines and our method."}
{"pre": "[CLS] in the future, we plan to investigate the use of a more sophisticated approach to solve the problem of", "cit": "[CLS] zelenko et al #otherefr and #refr proposed kernels for dependency trees (dts) inspired by string kernels. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the predominant sense of word senses in a word senses in a", "cit": "[CLS] unlike the #refr dominance system, this approach can be applied to much smaller target texts (a few hundred sentences)"}
{"pre": "[CLS] the first is the case for the syntactic parsing of the english lexical substitution task #refr. [SEP] grammars in", "cit": "[CLS] the queries in this corpus are more complex than those in the atis database-query corpus used in the speech"}
{"pre": "[CLS] the second approach is to use a semantic similarity measure and is based on the similarity of the similarity", "cit": "[CLS] in order to make different natural language processing tasks be able to help each other, jointly modeling methods become"}
{"pre": "[CLS] the first is the automatic evaluation of the automatic metric of the automatic evaluation of the automatic evaluation of", "cit": "[CLS] we extend our previous work #otherefr by experimenting with combination features, as well as features derived from the google"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] for example, in the context of syntactic disambiguation, #refr and magerman #otherefr proposed statistical parsing models based-on decision-tree l"}
{"pre": "[CLS] in the literature, supervised learning methods have been applied to unsupervised learning of natural language processing #otherefr; #refr. [SEP]", "cit": "[CLS] twitter contains a vast amount of information, including first stories and breaking news #otherefr and recommendations of relevance to"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] most start-of-the-art natural language parsers #otherefr; #refr use lexicalised features for parse ranking. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] #refr was designed for jointly learning to predict syntactic and semantic dependencies. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs in the sentence planning problem of identifying the sentence", "cit": "[CLS] unlike the current mainstream in automatic linguistic knowledge acquisition, which can be characterized as quantitative, surface-oriented bulk processing of"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for learning and beam-search decoding #otherefr;", "cit": "[CLS] averaged structured perceptron #refr is used for parameter estimation. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the annotation tool which was the annotation tool #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] #refr looked beyond the core temporal expressions and into prepositional phrases that contained temporal relations, i.e. before, during, etc"}
{"pre": "[CLS] the task of identifying the semantic relations between nominals of noun phrases and their arguments are also key to", "cit": "[CLS] furthermore, contradiction in itself is important knowledge for recognizing textual entailment (rte) #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of segmenting utterances into words, for example, johnson and colleagues have been shown to be useful in", "cit": "[CLS] to create a gold standard dataset of entities in context labeled with fine-grained classes, we first randomly select 20"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] corpus-based methods have usually been applied to obtain domain-specific polarity lexicons: they have been created by either starting from"}
{"pre": "[CLS] the most common approach to machine translation is to use a log-linear translation system to produce a log-linear translation", "cit": "[CLS] the lexicon-based metrics, such as bleu #refr, ter #otherefr, are good at capturing the lexicon or phrase level information,"}
{"pre": "[CLS] the parser is trained on the penn treebank corpus and tested on the penn treebank wall street journal, parser", "cit": "[CLS] i implemented the algorithms for packing and unification in lilfes #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the system used for the experiments in this paper is based on the senseval-3 english lexical sample task #refr.", "cit": "[CLS] recent estimations of the inter-annotator agreement when using the wordnet inventory report figures of 72.5% agreement in the preparation"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with a beam search", "cit": "[CLS] the early dependency model of #refr, in which model features were defined over only dependency structures, was partly motivated"}
{"pre": "[CLS] the grammar is a synchronous grammar formalism which is a synchronous grammar (scfg) used in grammar (scfg) which is", "cit": "[CLS] the difference is statistically significant at p < 0.05 based on 1000 iterations of paired bootstrap resampling #refr. [SEP]"}
{"pre": "[CLS] the translation model is a log-linear model #refr. [SEP] model weights are optimized to maximize bleu score #refr with", "cit": "[CLS] different techniques to widen the search space have been described #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders and predict the polarity of words in a sentence and", "cit": "[CLS] for the multi-aspect summarization task, we perform chi-square analysis on the rouge scores as well as on precision and"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the sentence in", "cit": "[CLS] we evaluate performance in two domains: the tempeval-3 corpus of newswire text #otherefr and the wikiwars corpus of wikipedia"}
{"pre": "[CLS] #refr proposed a joint model for unsupervised word segmentation and pos tagging and pos tagging and pos tagging using", "cit": "[CLS] recent work encodes similar sparsity information with non-parametric priors, relying on bayesian inference to achieve strong results without any"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of possible antecedents for the", "cit": "[CLS] for example, in #refr, pos top 80% noun 5.14 verb 8.75 adj 5.87 adv 4.22 top 90% 4.48 6.89"}
{"pre": "[CLS] the second approach is to use a word lattice as a sequence of a sequence of a sequence of", "cit": "[CLS] secondly, n-gram models including mm have indeed been shown to be successful for various slavic languages, e.g., czech #refr"}
{"pre": "[CLS] in the second international chinese word segmentation bakeoff #otherefr; #refr and the state-of-the-art accuracies of the state-of-the-art accuracies of", "cit": "[CLS] in the case of oracle training, we expected em to walk away from supervised solutions #refr; meri- 5 10"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the longrange reordering problem as a sequence of words", "cit": "[CLS] traditionally, entirely different approaches have been used for lexical #otherefr; #refr and syntactic simplification #otherefr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to parsing is to use a log-linear parsing model and maximise the conditional likelihood of", "cit": "[CLS] there were some trials to build korean constituent parsers, but due to the lack of appropriate corpus those trials"}
{"pre": "[CLS] we use the stanford corenlp suite to lemmatize and partof-speech tag each word #otherefr; #refr, and parse the input", "cit": "[CLS] there has been a lot of recent work on anaphoricity determination #otherefr, #refr, ng #otherefr). [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of semantic relation extraction has received a lot of attention in recent years #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] many of them focus on using tree kernels to learn parse tree structure related features #otherefr; #refr. [SEP] [PAD]"}
{"pre": "[CLS] in the second international chinese word segmentation bakeoff #otherefr; #refr and the best results are achieved by the best", "cit": "[CLS] transition-based dependency parsing was pioneered by yamada and matsumoto #otherefr and #refr, who used classifiers trained to predict individual"}
{"pre": "[CLS] the generation system is based on the inprotk project #refr. [SEP] system, which is a set of possible antecedents", "cit": "[CLS] text planning: an expression in the isi sentence planning language (spl) \\[#refr\\] is formed from the information specified in"}
{"pre": "[CLS] in the context of machine translation, itg has been explored for statistical word sense disambiguation #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] most of the applications of comparable corpora focus on discovering translation equivalents to support machine translation, such as bilingual"}
{"pre": "[CLS] the first is the automatic evaluation of discourse structure of discourse structure of discourse structure of discourse structure of", "cit": "[CLS] we re-examine the literature on the topic in detail, and propose a revised algorithm, taking advantage of the improved"}
{"pre": "[CLS] #refr use a similar approach to identify the text in which the relations between entities in a text and", "cit": "[CLS] one previously explored approach, e.g., (ono, #refr #otherefr, was to extract discourse structure elements and then generate the summary"}
{"pre": "[CLS] the second approach is to use a word segmentation and pos tagger to tag dictionary to tag each word", "cit": "[CLS] final word segmentation is carried out by a crf-based word segmenter #otherefr (peng and #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second approach is to use a small number of manually annotated corpora such as the same data set", "cit": "[CLS] we investigated four individual approaches for the syntax-features ? a regular-expression-based quasi-parser, a system based on dekang lin?s mini-"}
{"pre": "[CLS] the second approach is to use a word lattice as a sequence of a sequence of a sequence of", "cit": "[CLS] #otherefr present a hybrid method between word-based and segmentbased pos tagging for arabic and report good results. #refr perform"}
{"pre": "[CLS] the system used in this paper is based on the stanford parser #refr and the stanford parser #otherefr. [SEP]", "cit": "[CLS] parse trees predicted parse trees were produced using the charniak parser #refr.12 some additional tag types used in the"}
{"pre": "[CLS] the most common approach to statistical machine translation is to use a log-linear translation system to produce a translation", "cit": "[CLS] other variations of ebmt systems are hybrids that integrate an ebmt component as one of multiple sources of transfer"}
{"pre": "[CLS] the system is based on the inprotk toolkit for the best performing system described in #refr, which uses a", "cit": "[CLS] it is worth adding that, although the present paper focuses on entailments strictly based on syntax, the proposed methodology"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] algorithm with the best derivation of the best derivation", "cit": "[CLS] model parameters that maximize the loglikelihood of the training data are computed using a numerical optimization method #refr. [SEP]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the training data is to", "cit": "[CLS] knight and chander #otherefr use decision trees to choose between the and a/an, ignoring nps with no determiner, and"}
{"pre": "[CLS] the system uses a maximum entropy classifier trained on the wall street journal (wsj) corpus of the penn treebank", "cit": "[CLS] unlike previous works #otherefr; #refr that train a model upon each error type, we use one single model for"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a text that are semantically", "cit": "[CLS] our approach to stc uses a thesaurus based on corpus statistics #refr for real-valued similarity calculation. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the event descriptions of the bionlp?09 shared task on event extraction #refr.", "cit": "[CLS] #refr use wordnet, looking for a synonymy or hypernymy relation (additionally, for coordinate sisters in word- net). [SEP] [PAD]"}
{"pre": "[CLS] we use the averaged perceptron algorithm #refr to train the parameters of the averaged perceptron algorithm with the averaged", "cit": "[CLS] #refr presented an improved pipeline model in which upstream subtask outputs are regarded as hidden variables, together with their"}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders and predict the polarity of words in a sentence and", "cit": "[CLS] this distinction is similar to modality #refra). [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] for this reason there is currently a great deal of interest in methods which incorporate syntactic information within statistical"}
{"pre": "[CLS] we use the stanford named entity recognizer #refr to extract named entities from the text and extract named entities", "cit": "[CLS] reviews from this website have been previously used in other sentiment analysis tasks #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the grammar is a grammar that is a given a grammar that is a grammar that of the grammar", "cit": "[CLS] we suggest, as do #refr, that integrating these constraints simultaneously is more efficient then pipelining them. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the automatic evaluation of the automatic evaluation of the automatic evaluation of the", "cit": "[CLS] decision trees #refr have been usefully applied to word-sense ambiguities, and hmm part-of-speech taggers #otherefr have addressed the syntactic"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] syntactic information is incorporated in tree-based approaches in smt, allowing one to provide a more detailed definition of translation"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with a probability distribution", "cit": "[CLS] this is a true pipeline approach, as was done in other successful parsers, e.g. #refr, in that the classifiers"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be np-hard #refr. [SEP] s", "cit": "[CLS] figure 2: an example of non-binarizable rule from the hand-aligned chinese-english data in #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the context of spoken dialogue systems #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ?", "cit": "[CLS] furthermore, there are likely to be a large number of domain models/knowledge bases that could have motivated the production"}
{"pre": "[CLS] the translation quality is measured by bleu #refr. [SEP] score #refr on the test set. [SEP] [SEP] [SEP] =", "cit": "[CLS] #refr proposed another way to use paraphrases in smt. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the translation quality is measured by bleu #refr. [SEP] score #refr on the test set. [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] this is achieved by optimizing the regression learning model?s output to correlate against a set of training examples, where"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? ? ? ?", "cit": "[CLS] previous works #otherefr; #refr usually leveraged mappings between nl phrases and logical predicates as lexical triggers to perform transformation"}
{"pre": "[CLS] the most popular approach to word alignment is the hmm alignment model #refr. [SEP] model is based on the", "cit": "[CLS] competitive linking was described by #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] the scores are reported in case-sensitive bleu #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] recently there have been some improvements to the charniak parser, use n-best re-ranking as reported in #otherefr and selftraining"}
{"pre": "[CLS] the grammar we are extracted from the treebank using the stanford parser #refr. [SEP] parser #refr with the parser", "cit": "[CLS] we describe some of the strategies and notational devices that enable the basic english grammar developed for the pargram"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a text that is a", "cit": "[CLS] at the senseval competitions #otherefr, the choice of a sense inventory also frequently presented problems, spurring the efforts to"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] similar to speech recognition?s recognizer output voting error reduction #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the work presented in this paper connects and extends several areas of research: grounded semantics #otherefr, which aims to", "cit": "[CLS] this type of reference is different from the type that has been studied traditionally by researchers who have usually"}
{"pre": "[CLS] #refr use a bootstrapping method to identify opinion holders and extract opinion holders and extract opinion expressions from documents", "cit": "[CLS] fine-grained methods were also introduced to extract opinion holder, opinion expression, opinion target, and other opinion elements #refr. [SEP]"}
{"pre": "[CLS] the grammar is a grammar that is a grammar that is a parsing system for the grammar that is", "cit": "[CLS] the following order-preserving constraint, which follows more primitive directionality features #refr, limits the directions of the slashes in gtrcs."}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to learn a translation probabilities from", "cit": "[CLS] to this end we applied the fill-up method #refr in which outof-domain phrase tables are merged with the indomain"}
{"pre": "[CLS] we use the stanford named entity recognizer #refr to extract named entities from the named entities from the stanford", "cit": "[CLS] last, we note that transitive relations have been explored in adjacent fields such as temporal information extraction #otherefr, and"}
{"pre": "[CLS] #refr use a similar approach to compute the polarity of subjective words in the polarity of words in a", "cit": "[CLS] following #refr, the fh and fc thresholds were set to 1000 words per million (upper bound for fc) and"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best results for the best", "cit": "[CLS] most work in the area of unknown words and tagging deals with predicting part-of-speech information based on word endings"}
{"pre": "[CLS] #refr use a similar approach to identify the polarity of words in a sentence. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] the first work that had a significant performance improvement over the previous research was by #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to machine translation is the most widely used in statistical machine translation system combination approach", "cit": "[CLS] posbleu the standard bleu score #refr calculated on pos tags instead of words; . [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] recently, several studies #otherefr; #refr proposed modeling targetside morphology in a phrase-based factored models framework #otherefr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to translation lexicons from parallel corpora #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] cognates among european languages have been shown effective in word alignments #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the grammar we are extracted from the treebank using the berkeley parser #refr and the charniak parser #otherefr. [SEP]", "cit": "[CLS] more radically, it has been suggested that the power of lcfrs should be limited to well-nested structures, which gives"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] we used 14 datasets with non-projective dependencies from the conll-2006 and conll-2008 shared tasks #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best derivation of the best", "cit": "[CLS] #refr and `22 regularization #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the averaged perceptron algorithm #refr to train the parameters of the feature weights. [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] it is also interesting to note that the best results in zhang et al #otherefr with the self-trained reranking"}
{"pre": "[CLS] we use the stanford ner tagger #refr to identify the text in the text and extract the text and", "cit": "[CLS] named entity recognition is a well-studied problem, especially on newswire and other longdocument genres #otherefr; #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the automatic acquisition of subcategorization frames from corpora #refr, which is the web", "cit": "[CLS] since we are only interested in acquiring new lexical entries for mwes which are not covered by the grammar,"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a task of identifying the degree of semantic relations between words", "cit": "[CLS] #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] previous work on german #refr pursued a similar strategy and showed promising results after considerable effort transforming the output"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within the source sentence into a target sentence using", "cit": "[CLS] truecasing can be viewed in a lexical ambiguity resolution framework #refr as discriminating among several versions of a word,"}
{"pre": "[CLS] the task of identifying the event descriptions of event extraction and event extraction of event descriptions has been well", "cit": "[CLS] #refr learned event templates (or frames), where events that are related to one another and their semantic roles are"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] besides semantic parsing for querying databases #otherefr, playing computer games #refr, following navigational instructions #otherefr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr. [SEP]", "cit": "[CLS] such pos tagging work has been plentiful and includes efforts to induce pos tags without labels #otherefr, and creating"}
{"pre": "[CLS] we use the averaged perceptron algorithm #refr to train the weight for each system. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] the ibm and hmm word alignment models #otherefr; #refr have underpinned the majority of statistical machine translation systems for"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best results for the best", "cit": "[CLS] as demonstrated by #otherefr; #refr, the quality and consistency of segmentation has important downstream impacts on system performance in"}
{"pre": "[CLS] the first is the automatic metric of text with human annotation of text #refr. [SEP] quality is the automatic", "cit": "[CLS] in #refr it is observed that higher linguistic levels in measures increases the correlation with human judgements at the"}
{"pre": "[CLS] the system uses a combination of natural language processing (nlp) systems to produce various types of syntactic parsing and", "cit": "[CLS] he has achieved state-of-the art results by applying m.e. to parsing #refra), part-of-speech tagging #otherefr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second dataset is the use of a word sense disambiguation algorithm which is based on the most frequent", "cit": "[CLS] an important research direction identifies distributionally similar neighbors in raw text as cues for determining the predominant sense of"}
{"pre": "[CLS] the feature weights were tuned using minimum error rate training (mert) #refr on the development set using minimum error", "cit": "[CLS] we constructed a 5-gram language model using srilm #otherefr from the provided english monolingual training data and parallel data"}
{"pre": "[CLS] the parser is trained on the penn treebank corpus and tested on the penn treebank corpus #refr. [SEP] test", "cit": "[CLS] 1998), word sense disambiguation #refr and recently in parsing #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to paraphrase generation is to use a lexicon to paraphrase database into a target language", "cit": "[CLS] moreover, #otherefr extracts an arbitrary length of word correspondences and #refr identifies collocations through word-level sorting. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to this problem is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] the same approach is used by #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] model with the average number of the average", "cit": "[CLS] for this we use an approach similar to pereira and schabes? grammar induction from partially bracketed text #refr. [SEP]"}
{"pre": "[CLS] the task of semantic relation extraction has received a lot of attention in recent years #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] there is a large body of research in the supervised tradition that does not use explicit pattern representations ?"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] sentence pair of a", "cit": "[CLS] finally, we evaluate dsp on a common application of selectional preferences: choosing the correct antecedent for pronouns in text"}
{"pre": "[CLS] the first is the annotation tool which was the annotation tool for the annotation tool #refr. [SEP] project in", "cit": "[CLS] increasing the quantity of such annotations requires exhaustive inter-annotator agreement studies (which has been rare in framenet corpora generation)"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] 1the apparent difficulty of adapting constituency models to non-configurational languages has been one motivation for dependency representations #otherefr; #refr."}
{"pre": "[CLS] the task of relation extraction has received much attention in recent years #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] previous work on english ore has evolved from shallowsyntactic #otherefr and semantic #refr systems. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the word sense disambiguation of word sense disambiguation of word sense disambiguation", "cit": "[CLS] the best example of such an approach is #refr, who proposes a method that automatically identifies collocations that are"}
{"pre": "[CLS] the second approach is to use a semantic parser to generate the parse tree and the sentence in a", "cit": "[CLS] more recently, we applied shallow syntactic structures and discourse parsing with slightly better results #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to word alignment is the hmm model #otherefr; #refr. [SEP] model is based on the", "cit": "[CLS] a similar dual decomposition algorithm to ours was proposed by #refr for biomedical event detection. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] syntactic structure-based reordering has been shown to be significantly helpful for handling word order issues in phrase-based machine translation"}
{"pre": "[CLS] in this paper, we propose a discriminative model for training using a discriminative word alignment model based on the", "cit": "[CLS] we will show in the next section that our augmentedloss method is general and can be applied to any"}
{"pre": "[CLS] the grammar is a grammar that is a grammar that is a grammar that is a grammar that is", "cit": "[CLS] the underlying formalism for the transfer is \"synchronous tree adjoining grammars\" #refr\\]) 1. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford ner tagger #refr to train the word segmenter #otherefr to obtain the word segmenter for", "cit": "[CLS] #refr used latent semantic analysis to find affixes. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a bootstrapping method to identify the polarity of words in a text in a text to determine", "cit": "[CLS] output: gold standard topic label for each of the lda topics for tw. 1: for each topic i ?"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for learning and beam-search decoding #otherefr;", "cit": "[CLS] #refr used a re-ranking scheme to provide global features while we simply augment the features of an existing parser."}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal (wsj) corpus #refr. [SEP] trees and the penn", "cit": "[CLS] the use of chart pruning techniques, typically some form of beam search, is essential for practical parsing using penn"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best results for the best", "cit": "[CLS] instead of gold parts-of-speech, we plugged in 200 context-sensitive unsupervised tags, from #refrc),3 for the word categories. [SEP] [PAD]"}
{"pre": "[CLS] the first is the case for the generation of the generation of referring expressions from the linguistic phenomena of", "cit": "[CLS] the grammar ules are based on constraint dependency grammar #refr, and are essentially constraints between modifications. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to paraphrase generation is to use a lexicon to paraphrase database into a target language", "cit": "[CLS] another approach (del?ger and #refr matches similar paragraphs in comparable texts, creating smaller comparable documents for paraphrase extraction. [SEP]"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from the sentence pairs in the input text with", "cit": "[CLS] a corpus description and motivation to build such corpus can be found in #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr proposed a method for learning a word sense disambiguation which uses a small set of seed words from", "cit": "[CLS] machine learning-based approaches are taken in #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] other related approaches include #refr, where a cross-lingual transfer of relations is performed #otherefr, where semantic structure matching is"}
{"pre": "[CLS] the most popular approach to paraphrase generation has been the source sentences into either by using a monolingual parallel", "cit": "[CLS] past work #otherefr; #refr has examined the use of monolingual parallel corpora for paraphrase extraction. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] e.g., #refr evaluate on unlabeled dependencies, tsarfaty #otherefr evaluate on constituents. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the semantic role labeling task of the predicate-argument structure of the work on", "cit": "[CLS] the data was tagged using tnt #refr, using a model trained on the wall street journal. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to tag the one of the source language side of the target language to", "cit": "[CLS] the right-corner transform used in this paper is simply the left-right dual of a leftcorner transform #refra). [SEP] [PAD]"}
{"pre": "[CLS] in the second international chinese word segmentation bakeoff #otherefr; #refr and the segmentation bakeoff #otherefr, the segmentation bakeoff 2005", "cit": "[CLS] we follow a long tradition of older #otherefr and newer #refr work on creating distributional features for pos tagging"}
{"pre": "[CLS] the task of coreference resolution is a related task of identifying the task of identifying the extensively studied problem", "cit": "[CLS] we first briefly describe the supervised system (described in more detail in #refr) to which we will compare the"}
{"pre": "[CLS] the task of segmenting utterances into words, for example, johnson and colleagues have been shown to be useful in", "cit": "[CLS] recently, #refr proposed an approach that uses co-occurrence patterns to find entity type candidates, and then learns their applicability"}
{"pre": "[CLS] in the second international chinese word segmentation bakeoff #otherefr; #refr and the best results are reported in table 1.", "cit": "[CLS] in the well-known chinese word segmentation bakeoff tasks, for example, different segmentation standards have been used by different data"}
{"pre": "[CLS] the first is the case for the semantic textual similarity of the words in the context of a text", "cit": "[CLS] as such, it emphasizes robustness at web scale, without taking advantage of existing specification languages for representing events and"}
{"pre": "[CLS] the first is the polarity of a polarity of a polarity of a polarity of a polarity of a", "cit": "[CLS] yu and #refr, kim and hovy #otherefr4 all begin by first creating prior-polarity lexicons. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to smt is the one proposed by #refr, who use a large set of features", "cit": "[CLS] #refr puts the cost of professionally translated english at about $0.30 perword for translation fromtamil. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] we report case-insensitive scores for version 0.6 of meteor #refr with all modules enabled, version 1.04 of ibm-style bleu"}
{"pre": "[CLS] the idea of using semantic representations for syntactic structures has been explored by #refr and semantic representations for semantic", "cit": "[CLS] this provides a very weak form of semantic equivalence valid only in that world/context #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the future, we plan to investigate the use of a more sophisticated method to incorporate incomplete constituents, and", "cit": "[CLS] #refr were the first to beat a simple parsing heuristic, the right-branching baseline; today?s state-of-the-art systems #otherefra) are rooted"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a text with a text that are semantically", "cit": "[CLS] the perceptual information in such models is generally mined directly from images #otherefr or from data collected in psychological"}
{"pre": "[CLS] the first is the process of annotating the word segmentation and then the phrase structure parses of a sentence", "cit": "[CLS] the basic framework for the current study develops around two such complex networks namely, the phoneme-language network or planet"}
{"pre": "[CLS] the second approach is to use a small number of seed examples for example outputs #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] the baseline 1-gram and the background memm capitalizer were trained on various amounts of wsj #refr data from 1987"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the training data is to", "cit": "[CLS] several recent projects #otherefr; #refr use nl instructions to guide reinforcement learning from independent exploration with delayed rewards. [SEP]"}
{"pre": "[CLS] we use the stanford word segmenter #refr to extract the word pairs from the target language model for each", "cit": "[CLS] in #refr, domain-specific language models are obtained by including only the sentences that are similar to the ones in"}
{"pre": "[CLS] the second dataset is the same as the use of the same as the same as in #refr, which", "cit": "[CLS] kohomban and lee #otherefr, and #refr). [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a bootstrapping method to extract opinion polarity lexicons from the polarity of words from a large corpus", "cit": "[CLS] the underlying motivation is analogous to the novel idea ?mine the easy, classify the hard? #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] in the context of machine translation, itg has been explored for a variety of nlp tasks such as semantic", "cit": "[CLS] the methods which intuitively seem most plausible are based on information content, e.g.#refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the berkeley parser #refr to obtain the syntactic parse trees for each sentence and the english and", "cit": "[CLS] to circumvent these computational limitations, various pruning techniques are usually needed, e.g., #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second approach is to use mismatches in which the context of the source side to be subjective/objective or", "cit": "[CLS] distributional similarity if the head words of the two conjuncts have high distributional similarity #refr, this is taken to"}
{"pre": "[CLS] in this paper, we propose a new approach to use syntactic parsing to learn a parse tree and parse", "cit": "[CLS] this observation has led to a vast amount of research on unsupervised grammar induction #otherefr; #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to identify the topics of words. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] a number of token-based approaches have been proposed: supervised #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] automatic semantic role labelers have been developed by training classifiers on hand annotated data #otherefr; #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to phrase-based translation models #otherefr; #refr rely on the source side and the target side", "cit": "[CLS] recently, discriminative methods for alignment have rivaled the quality of ibm model 4 alignments #otherefr; #refr; taskar et al,"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] the", "cit": "[CLS] mcccj the bllip parser #refr, also variously known as the charniak parser, the charniak-johnson parser, or the brown reranking"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the sentence in", "cit": "[CLS] however, this has been achieved by either using a subset of relations that can be found in discourse theories"}
{"pre": "[CLS] #refr use a probabilistic model to model the probability of a word in a word in a word in", "cit": "[CLS] np2 vp5) as well as their probabilities; this approach can therefore be viewed as a bayesian version of the"}
{"pre": "[CLS] the system uses a combination of natural language processing (nlp) systems to produce various types and syntactic parsing #refr.", "cit": "[CLS] finally, we use type-changing rules, which is standard practice in ccg parsing #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to smt is the one proposed by #refr, who use a large set of linguistic", "cit": "[CLS] further approaches to domain adaptation for smt include adaptation using in-domain language models #otherefr, meta-parameter tuning on in-domain development"}
{"pre": "[CLS] the first is the automatic evaluation of the automatic metric of the automatic evaluation of the automatic evaluation of", "cit": "[CLS] #refr discusses an implemented program that automatically classifies verbs into two groups, stative vs. non-stative, on the basis of"}
{"pre": "[CLS] the task of identifying the event descriptions of event extraction and event descriptions has been well studied in the", "cit": "[CLS] known entities are recognized and mapped to the kb using a recent tool for named entity disambiguation #refr. [SEP]"}
{"pre": "[CLS] the most popular approach to translation lexicons from parallel corpora #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] arabic and urdu are segmented using mada #refr and uws #otherefr, and the no-reordering-overpunctuation heuristic. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second approach is to use a word segmentation and pos tagger to tag dictionary to tag each word", "cit": "[CLS] there has been a surge of interest in lexical normalization with the advent of microblogs #otherefr; #refr. [SEP] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] as a benchmark vpc extraction system, we use the charniak parser #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to extractive summarization is to use a lexicon to model that is to model the", "cit": "[CLS] then, the dependency parser generates the following parse tree: novel it is a 2008 young adult collins nsubj cop"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] the propbank project is to add a semantic layer on the penn treebank #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the automatic evaluation of discourse coherence models in the natural language processing (nlp) tasks such as", "cit": "[CLS] our generation implementation is based on simplenlg #refr which is a surface realizer api that allows us to create"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] the", "cit": "[CLS] following #refr, we examined what percentage of disagreement is due to negligence on behalf of one or the other"}
{"pre": "[CLS] the translation quality is measured by bleu #refr. [SEP] [SEP] = 4. [SEP] = 4. [SEP] = ? i", "cit": "[CLS] there have been some recent studies which have used the online micro-market, amazons mechanical turk, to collect human annotations"}
{"pre": "[CLS] the first is the task of identifying the event descriptions of the bionlp?09 shared task on event extraction #refr.", "cit": "[CLS] for our experiments, we used the first 125 articles of the coreferentially annotated tu?ba-d/z corpus of written newspaper text"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] the study is conducted on both a simple air travel information system (atis) corpus #refr and the more complex"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for parsing #refr. [SEP] grammars with", "cit": "[CLS] recently, some pruning techniques have been proposed to improve the efficiency of third-order models #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of identifying the semantic textual similarity of a text is a document #otherefr; #refr. [SEP] of a", "cit": "[CLS] other works #otherefr; #refr have applied heuristic formulas to estimate the similarity between text fragments based on a similarity"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be a difficult problem #otherefr;", "cit": "[CLS] firstly conceived to optimally encode multiple transcription hypothesis produced by a speech recognizer, word lattices have later been used"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] in addition, we employed existing out-ofdomain sentiment lexicons: #otherefr, and (2) three manually created sentiment lexicons, nrc emotion lexicon"}
{"pre": "[CLS] topic models have been successfully applied to a wide range of nlp tasks including word sense disambiguation #otherefr, and", "cit": "[CLS] in previous work #refr, we described a method of finding topic boundaries using an optimisation algorithm based on word"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking approaches #otherefr; #refr to learning for dependency", "cit": "[CLS] dependency relations have also been found to be useful in information extraction #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] following previous work #refr, we minimize spam posts by removing tweets that contain urls, and tweets from users that"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] firstly, we run giza++ #otherefr on the training corpus in both directions and then apply the ogrow-diag-finalp refinement rule"}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders and predict the polarity of opinion expressions in opinion expressions", "cit": "[CLS] most existing works on sentiment summarization focus on predicting the overall rating on an entity #otherefr; #refr or estimating"}
{"pre": "[CLS] the first is the task of identifying the mentions of a text with a semantic role labeling task as", "cit": "[CLS] numerous studies are concerned with feature extraction, typically trying to enrich the classifier with more linguistic knowledge and/or more"}
{"pre": "[CLS] the second dataset is the use of a word sense disambiguation algorithm which is based on the most similar", "cit": "[CLS] we looked at three different lemmatizers: the lemmatizing backend of the xtag project #otherefr, and the lemmatizing component of"}
{"pre": "[CLS] in the context of machine translation, information extraction is used to extract relevant information from text from text corpora", "cit": "[CLS] we evaluate our system by comparing real novels to artificially produced surrogates, a procedure previously used to evaluate models"}
{"pre": "[CLS] the most common approach to translation is to use a log-linear translation model #refr and a log-linear translation model", "cit": "[CLS] our results will be extended to slight extensions of 2- scfgs, incl. the extension of itgs proposed by #refr"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the sentences and the syntactic parse trees of the sentences in", "cit": "[CLS] one of the tasks in #refr concerned predicting gender in scientific writing, but they use a corpus of only"}
{"pre": "[CLS] the first group (asr features) includes a number of distinct dialogue systems that are able to handle dialogue acts", "cit": "[CLS] in terms of surface realisation from graphical models #otherefr and dethlefs and #refrb), who use hmms, dethlefs and cuaya?huitl"}
{"pre": "[CLS] the second approach to word segmentation is the one used in our experiments of the following two of #refr,", "cit": "[CLS] we use a statistical word model to assign a probat>ility to each subs|ring #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the automatic evaluation of the automatic evaluation of the automatic evaluation of the", "cit": "[CLS] 4also independently elicited from either the human informant or compiled from any on-line resources for the language in question."}
{"pre": "[CLS] the work of #refr is similar to ours in that it is possible to use a more complex linguistic", "cit": "[CLS] cda-94-01024 at harvard university and by at&t bell laboratories. that discourse structural information can be inferred from orthographic cues"}
{"pre": "[CLS] we use the stanford chinese segmenter #refr to tokenize the english sentences. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] the mt literature does cover some prior work on extracting or augmenting partial lexicons using non-parallel corpora #refr. [SEP]"}
{"pre": "[CLS] the first is the task of identifying the event descriptions of the bionlp?09 shared task on event extraction #refr.", "cit": "[CLS] since muc-6 #refr, named entities have been proper names falling into three major classes: persons, locations and organizations. [SEP]"}
{"pre": "[CLS] the first is the one of the most popular approach to use crowdsourcing, which has been to use a", "cit": "[CLS] in the classic natural language generation (nlg) architecture #refr, sentence boundary decisions are made during the sentence planning stage"}
{"pre": "[CLS] the system is based on the moses toolkit #otherefr and the moses toolkit #refr. [SEP] test set for the", "cit": "[CLS] these fourteen scores are weighted and linearly combined #refr; their respective weights are learned on development data so as"}
{"pre": "[CLS] the model is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the decoder with the decoder with", "cit": "[CLS] proposed decipherment solutions for letter substitution ciphers include techniques that use expectation maximization #otherefr, and bayesian learning with dirichlet"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with the one of", "cit": "[CLS] parser action #otherefr; #refr [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? 2008. [SEP] ?", "cit": "[CLS] recently, #refr proposed a global graph optimization procedure that uses integer linear programming (ilp) to find the best set"}
{"pre": "[CLS] we use the averaged perceptron algorithm #refr to train the parameters of the averaged perceptron algorithm #otherefr. [SEP] [SEP]", "cit": "[CLS] subsequent work #otherefr; #refr has expanded on this in various ways, with accuracy between 86% and 96%. [SEP] [PAD]"}
{"pre": "[CLS] the system is based on the inprotk toolkit for the best performing system described in #refr. [SEP] [SEP] [SEP]", "cit": "[CLS] there has also been significant work on #otherefr; #refr, such that even 3more details about the chunk parser can"}
{"pre": "[CLS] the first is the annotation tool which was the annotation tool #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] moreover, for bulgarian a more general integrated system was developed, called lingua #refr, which ? apart from modules for"}
{"pre": "[CLS] the most common approach to discourse coherence modeling is to use a sequence of discourse structure #otherefr; #refr. [SEP]", "cit": "[CLS] other work has shown that co-occurrence of words #otherefr; #refr and discourse relations #otherefr also predict coherence. [SEP] [PAD]"}
{"pre": "[CLS] in the literature, supervised learning methods have been applied to part-of-speech tagging and unsupervised #otherefr and unsupervised part-of-speech tagging", "cit": "[CLS] recent work on contrastive estimation #refr has shown that maximum-entropybased latent variable models can yield more accurate clusterings for"}
{"pre": "[CLS] the model is trained using the averaged perceptron algorithm #refr. [SEP] score #refr with the best derivation forest, [SEP]", "cit": "[CLS] alternative approaches avoid such heuristics, instead learning structured alignment models directly from sentence aligned data #otherefr; #refr). [SEP] [PAD]"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] this technique has since been used for other languages and tasks, e.g. morphological analysis #otherefr, and tagging and inducing"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the words is to the", "cit": "[CLS] lvcs are usually distinguished from productive or literal verb + noun constructions on the one hand and idiomatic verb"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for each sentence with dependency trees for", "cit": "[CLS] statistical translation methods can be divided into word-based #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of identifying the semantic textual similarity of a text is a document #otherefr; #refr. [SEP] of a", "cit": "[CLS] however, for specific domains like weather forecasts, medical reports or student reports, more varied domain entities form slots in"}
{"pre": "[CLS] the system is based on the inprotk toolkit for the best performing system described in #refr. [SEP] [SEP] [SEP]", "cit": "[CLS] this type of reasoning has been identified as a core semantic inference task by the generic textual entailment framework"}
{"pre": "[CLS] the system is based on the stanford parser #refr and the stanford parser #otherefr. [SEP] system #otherefr. [SEP] system", "cit": "[CLS] all the data used throughout the paper come from the conll-2012 shared task #refr, which included the 1.6m english"}
{"pre": "[CLS] the feature weights were tuned using minimum error rate training (mert) #refr on the development set using the development", "cit": "[CLS] in this work, we describe results from the open-source syntax augmented machine translation (samt) toolkit #refr applied to the"}
{"pre": "[CLS] the first is the task of identifying the event descriptions of the bionlp?09 shared task on event extraction #refr.", "cit": "[CLS] for example, one can ignore infrequent collocations entirely (e.g., ng & lee), consider only the single best property (e.g.,"}
{"pre": "[CLS] the first is the automatic evaluation of the automatic metric of the automatic evaluation of the automatic evaluation of", "cit": "[CLS] thus, this problem sounds like an excellent application for frame semantics! we present initial results in #refr, and summarize"}
{"pre": "[CLS] the second approach is to use a small number of seed examples for example outputs #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] seneff #otherefr and #refr used stochastic grammars for csl. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the automatic evaluation of the automatic evaluation of the automatic evaluation of the", "cit": "[CLS] the stanford tools perform part of speech tagging #refr, constituent and dependency parsing #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to identify the polarity of words in a sentence. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] in recent years, there has been an ever increasing interest #otherefr; #refr in automating this task, with many works"}
{"pre": "[CLS] the bionlp shared task (bionlp-st, hereafter) series has been instrumental in the bionlp 2009 shared task data for a", "cit": "[CLS] event extraction is understood here as an activity, that broadly follows the bionlp 2009 shared task view #refr, in"}
{"pre": "[CLS] the system is based on the moses toolkit #otherefr and the moses toolkit #refr. [SEP] test set for the", "cit": "[CLS] provided speed-related aspects can be dealt with, this makes the method directly applicable to language models, or distortion tables"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a text with a text corpus of sentences", "cit": "[CLS] recently, research into nlg systems that generate text from georeferenced data has begun to emerge #otherefr; #refr. [SEP] [PAD]"}
{"pre": "[CLS] the system is based on the inprotk project #refr. [SEP] score for the user is not a given sentence,", "cit": "[CLS] dialogue systems are built for many different purposes, including information gathering #otherefr), collaborative interaction (e.g., #refr), tutoring #otherefr). [SEP]"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with the one of", "cit": "[CLS] we provide (declarative specifications of) novel, polynomial-time algorithms for two widelyused transition-based parsing models: arc-standard #refr and arc-eager #otherefr."}
{"pre": "[CLS] we use the averaged perceptron algorithm #refr to train the weights for the weights for each training and the", "cit": "[CLS] initialization of a in step 1 can be arbitrary, but for faster convergence special initializations have been used, e.g.,"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] first, there is a free/open-source implementation facilitating experimental replication and comparison.1 more importantly, until recently it was the only"}
{"pre": "[CLS] the most popular approach to word alignment is the hmm alignment model #refr. [SEP] model is based on the", "cit": "[CLS] much of the literature has explored one task ? selecting sentences to translate and add to the training corpus"}
{"pre": "[CLS] the second approach is to use a word lattice as a source language to be a target word to", "cit": "[CLS] knowledge based approaches to wsd such as lesk?s algorithm #otherefr and random walk algorithm #refr are fundamentally overlap based"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] after splitting snippets into sentences, we applied named entity recognizer (ner) #refr to recognize entities in sentences. [SEP] [PAD]"}
{"pre": "[CLS] the model is trained using the averaged perceptron algorithm #refr. [SEP] score #refr with the best results for the", "cit": "[CLS] our research is also in line with recent research on improving the speed of nlp systems with little or"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] sentence pair of a", "cit": "[CLS] among a variety of surface cues, lexical cohesion#otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to identify the polarity of words in a sentence. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] langid systems appear to perform fairly well on twitter #refr, but twitter-specific systems seem to perform better #otherefr. [SEP]"}
{"pre": "[CLS] the second approach is to use a small number of seed examples for example outputs #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] such textual units are typically sentences, sub-sentences #refr, or excerpts #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to this problem is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] typically, dependency parsing evaluations ignore punctuation token attachment #refr, and we believe it is a waste of annotator (and"}
{"pre": "[CLS] we use the moses toolkit #refr to train the weights for the feature weights for the log-linear model with", "cit": "[CLS] several approaches have been proposed to address these issues: from filtering the extracted synchronous grammar #refr to alternative bayesian"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? 2008. [SEP]", "cit": "[CLS] in current knowledge harvesting algorithms, seeds are chosen either at random #otherefr; #refr, or by asking experts #otherefr. [SEP]"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a series of text that is a text in the same", "cit": "[CLS] this is useful from a practical functional perspective because it limits both the conceptual nd linguistic diversity which needs"}
{"pre": "[CLS] we use the stanford parser #refr to extract dependency relations from the dependency trees for each sentence with dependency", "cit": "[CLS] sentence compression has been widely studied in language processing. #otherefr; #refr applied the noisy-channel framework to predict the possibilities"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] most previous work on following human instructions are based on supervised learning #otherefr or reinforcement learning #refr. [SEP] [PAD]"}
{"pre": "[CLS] the second approach is to use a word lattice as a sequence of a sequence of a sequence of", "cit": "[CLS] aiming to improve both tasks, work by #refr and sun et al #otherefr conduct segmentation and detection sequentially, but"}
{"pre": "[CLS] the generation of referring expressions is a subject of research in natural language generation (nlg) in natural language generation", "cit": "[CLS] although studies of large speech corpora have found that approximately 10% of spontaneous tterances contain disfluencies involving self-correction, or"}
{"pre": "[CLS] we use the stanford corenlp suite to lemmatize and partof-speech tag each word #otherefr; #refr, and extract the stanford", "cit": "[CLS] other state-of-the-art systems, such as #refr and berkeley?s coreference resolution system #otherefr, also treat coreference as a task on"}
{"pre": "[CLS] the system is based on the inprotk toolkit for the paradise system described in #refr. [SEP] system, which is", "cit": "[CLS] an obvious candidate is the incremental approach #refr which allows the system to process partial user inputs, back-channels, predict"}
{"pre": "[CLS] the second dataset is the use of a word sense disambiguation task as a sense disambiguation task #refr, which", "cit": "[CLS] we contrast the performance of first sense heuristics i) from semcor #refr and ii) derived automatically from the bnc"}
{"pre": "[CLS] in the context of machine translation, itg has been explored for mt #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] we employ 8 different mt metrics for identifying paraphrases across two different datasetsthe well-known microsoft research paraphrase corpus #otherefr"}
{"pre": "[CLS] in the context of machine translation, itg has been explored for word sense disambiguation #otherefr; #refr. [SEP] [SEP] [SEP]", "cit": "[CLS] in order to enable multilinguality, the vectors of comparable documents written in different languages are concatenated, making up the"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] mainstream ways of coping with these problems and increasing alignment quality include considering sentence length #otherefr and using bilingual"}
{"pre": "[CLS] id participant cmu carnegie mellon university, usa #otherefr systran systran, france uedin-birch university #refr pct a commercial mt provider", "cit": "[CLS] like invwer, other common surface-form oriented metrics like bleu #otherefr; #refr, cder #otherefr do not correctly reflect the meaning"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a text that are semantically similar to the", "cit": "[CLS] other proposals to move beyond word repetition in topical segmentation include the use of bigram overlap in #refr, information"}
{"pre": "[CLS] the idea of using distributional semantics to measure the similarity between words in a target language is known as", "cit": "[CLS] in particular, given two independent vectors v1 and v2, the semantically compositional result v3 is modelled by: ? vector"}
{"pre": "[CLS] in the context of machine translation, the spoken dialogue systems has been used to determine if the current sentence", "cit": "[CLS] detection has received much attention (e.g., #refr), but less work has been done on adaptation, due to the difficulty"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal (wsj) section of the penn treebank #otherefr; #refr.", "cit": "[CLS] earlier studies by dubey and keller #otherefr using the negra treebank #refr reports that lexicalization of pcfgs decrease the"}
{"pre": "[CLS] in the second international chinese word segmentation bakeoff #otherefr; #refr and the state-of-the-art accuracies of the state-of-the-art accuracies of", "cit": "[CLS] recent research on dependency parsing usually overlooks this issue by simply adopting gold pos tags for chinese data #otherefr;"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] but we recently showed that using phrases during search gives better coverage of translation, better future cost estimation and"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] #refr present a two-step process to recognize contextual polarity that employs machine learning and a variety of features. [SEP]"}
{"pre": "[CLS] we use the minimum error rate training #refr to tune the feature weights for the feature weights of the", "cit": "[CLS] recent work have shown that smt benefits a lot from exploiting large amount of features #otherefr; #refr. [SEP] [PAD]"}
{"pre": "[CLS] in the context of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] #otherefr", "cit": "[CLS] bootstrapping has been widely adopted in nlp applications such as word sense disambiguation #otherefr, named entity recognition #refr and"}
{"pre": "[CLS] the first is the annotation tool which was the annotation tool for the annotation tool #refr. [SEP] project in", "cit": "[CLS] another possibility to explore is the to integrate muli annotation with, e.g., the salsa corpus #refr, which provides more"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] table 1: performance of bestand worst-performing parsing models (uas diff. vs. baseline with ? : ? p < .05,"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] adding syntactic labels to synchronous context-free translation rules can improve performance, but labeling with phrase structure constituents, as in"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] we reimplemented the feature templates from charniak and johnson #otherefr and #refr excluding the features based on external corpora"}
{"pre": "[CLS] the task of identifying the event descriptions of event extraction and event extraction of event descriptions has been well", "cit": "[CLS] building on timeml #otherefr; #refr identify temporal relationships in free text, but don?t focus on fact extraction. [SEP] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to compute similarities between words in a sentence. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] a common weakly-supervised approach starts with a small set of sentiment knowledge (seed words associated with a given sentiment)"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within the source sentence into a target sentence into", "cit": "[CLS] many transliteration systems #refr use word origin information. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] #refr report on ?frustrating? results on the conll 2007 semi-supervised adaptation task for dependency parsing, i.e. ?no team was"}
{"pre": "[CLS] #refr use a similar approach to identify the topics of words in a sentence. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] other works have correlated vsms built with text or images with brain activation data #otherefrb; #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders and opinion holders and opinion holders in opinion holders in", "cit": "[CLS] conversely, research efforts that focus on constructing a general purpose polarity lexicon (e.g., #refr, andreevskaia and bergler #otherefr) generally"}
{"pre": "[CLS] #refr proposed a method for extracting relations from wikipedia articles from corpora and the web by using wikipedia articles", "cit": "[CLS] pereira #otherefr and #refr use syntactic features in the vector definition. #otherefr improves on the latter by clustering by"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr.", "cit": "[CLS] the corpus has been part of speech tagged and lemmatized with stanford part-of-speech tagger #refr, and parsed with maltparser"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] score #refr with the entire sequence of the", "cit": "[CLS] other global optimization methods include submodularity #refr and graph-based approaches #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the system is based on the inprotk toolkit for the best performing system described in #refr. [SEP] score for", "cit": "[CLS] testing parameters include it-filter, which switches on and off the module for the detection of nonreferential it described in"}
{"pre": "[CLS] the first is the annotation tool which was the annotation tool for the annotation tool #refr. [SEP] project in", "cit": "[CLS] for example, the conll shared task has focused on semantic role labeling four times, twice as a separate task"}
{"pre": "[CLS] the most common approach to this task is to use a lexicon to identify the polarity classification task #refr.", "cit": "[CLS] truth-finders based on textual entailment such as truthteller #refr determine if a sentence states something or not. [SEP] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] supertagging has since been effectively applied to other formalisms, such as hpsg #otherefr; #refr, and as an information source"}
{"pre": "[CLS] #refr proposed a method for learning a word clustering based on a probabilistic model that uses a hidden markov", "cit": "[CLS] cluster-f1 is an item-based evaluation metric which we propose drawing inspiration from the supervised metric presented in #refr. [SEP]"}
{"pre": "[CLS] the first is the one of the most popular approaches to automatically identify the words in a text that", "cit": "[CLS] giesbrecht #otherefr e.g. find that for noun-noun compounds the product of context vectors (corresponding to the intersection of contexts)"}
{"pre": "[CLS] the first is the annotation tool which was the annotation tool for the annotation tool #refr. [SEP] project at", "cit": "[CLS] the statistically trained fergus #refr contrasts with our rule-based approach. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] algorithm with the best derivation of the average over", "cit": "[CLS] discriminative log-linear models are now becoming a de facto standard for probabilistic disambiguation models for deep parsing #otherefr; #refr."}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to translate the phrases as the", "cit": "[CLS] we used a multi-threaded version of the giza++ tool #refr.1 this speeds up the process and corrects an error"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for english and the english sentences. [SEP]", "cit": "[CLS] to date, qgs have been successfully applied to various tasks, such as word alignment #otherefr, question answering #refr, and"}
{"pre": "[CLS] the most common approach to word sense disambiguation is to use a word order context #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] identifying translation correspondences in this type of corpora permits to construct bilingual lexicons for low-resourced languages, and to complement"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] the experiments made use of morgan pecelli is noun/verb group annotations and some of david day is programs. et"}
{"pre": "[CLS] the feature weights were tuned using minimum error rate training (mert) #refr on the development set using minimum error", "cit": "[CLS] with some exceptions #refr, most still rely on tuning a handful of common dense features, along with at most"}
{"pre": "[CLS] the second approach is to use a small number of manually annotated corpora such as the same data set", "cit": "[CLS] many different learning approaches have been used, including neural networks #otherefr; #refr, decision lists #otherefr, etc. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] #refr proposed a method for automatically learning entailment rules from the web by using a bilingual corpora. [SEP] [SEP]", "cit": "[CLS] for natural language engineers, the problem bears on information management systems like abstractive summarizers that must measure semantic overlap"}
{"pre": "[CLS] the system is based on the inprotk toolkit for dialogue systems developed by #refr. [SEP] system, which uses a", "cit": "[CLS] in the remainder of this paper, we discuss the par- adise framework (paradigm for dialogue system evaluation) #refr, and"}
{"pre": "[CLS] the second approach is to use a maximum entropy model #otherefr; #refr. [SEP] model for the conditional random field", "cit": "[CLS] although the context word distribution is similar to word embeddings, we believe they complement each other, as reported by"}
{"pre": "[CLS] the task of identifying the overall sentence pairs in a text is a text #refr. [SEP] of the same", "cit": "[CLS] terns, as introduced at #refr, are added into our final pattern sets, such as ??c?, ?f??. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the feature weights were tuned using minimum error rate training (mert) #refr on the development set using minimum error", "cit": "[CLS] syntactic parse trees of the whole training corpus and the word alignment between source and target language are used"}
{"pre": "[CLS] the most common approach to semantic role labeling is to use syntactic information #otherefr; #refr. [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] we also experiment with publicly released word embeddings #refr, which were trained using both local and global context. [SEP]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] these issues are particularly problematic in phrase-based smt #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the entire sequence of the entire", "cit": "[CLS] in the spirit of recent work in randomized algorithms for large-scale hlt #otherefr, #refr,van durme and lall #otherefr as"}
{"pre": "[CLS] in the literature, supervised approaches have been applied to part-of-speech tagging and unsupervised #otherefr and unsupervised #refr and semi-supervised", "cit": "[CLS] a different approach in evaluating nonparametric bayesian models for nlp is statesplitting #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the semantic relations between nominals of noun phrases and the semantic relations", "cit": "[CLS] this idea has already been explored before by various researchers from different methodological angles including distribution-based statistical approaches #otherefr,"}
{"pre": "[CLS] the system used for the experiments in this paper is based on the c&c parser #otherefr and the c&c", "cit": "[CLS] for this area, there is some connection to the work of goldberg and elhadad #otherefr and #refr, which are"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within the source sentence into a target sentence into", "cit": "[CLS] the evaluation was run with respect to precision, recall, f -measure, and alignment error rate (aer) considering sure and"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] of a text with", "cit": "[CLS] recently, sequential learning methods, such as hidden markov models (hmms) and conditional random fields (crfs), have been used successfully"}
{"pre": "[CLS] #refr proposed a method for identifying word pairs of sentences in a target word in a sentence by clustering", "cit": "[CLS] therefore, many approaches have concentrated on different ways of estimating lexical coherence of text segments, such as semantic similarity"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to learn a translation models from a", "cit": "[CLS] during the last four years, various implementations and extentions to phrase-based statistical models #otherefr; #refr have led to significant"}
{"pre": "[CLS] the most common approach to word alignment is to use a log-linear translation model #refr. [SEP] model to combine", "cit": "[CLS] even so, various models have been proposed to explore coherence for document summarization and generation #otherefr; #refr. [SEP] [PAD]"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking approaches #otherefr; #refr to learning for dependency", "cit": "[CLS] in recent years, several dependency parsing algorithms #otherefr; #refr have been proposed and achieved high parsing accuracies on several"}
{"pre": "[CLS] the grammar is a grammar that is a grammar that is a parsing model that is a dependency grammar", "cit": "[CLS] see #otherefr and #refr for a thorough discussion. #otherefr suggests that the way to overcome this problem is to"}
{"pre": "[CLS] the work of #refr is similar to ours in that it is possible to use a more complex linguistic", "cit": "[CLS] a preliminary report on this project was presented at the 2004 workshop on frontiers in corpus annotation #refra), where"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] most work has focused on automatically learning reordering rules #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the sentence planning problem that the sentence planning problem of identifying the", "cit": "[CLS] we used bleu (4-grams) #refr to measure the similarities between the feedback summaries generated by the three systems. [SEP]"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to learn a translation probabilities from", "cit": "[CLS] we apply a leave-one-out heuristic #refr to make better use of the training data. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] we chose to use the top 3 runs (rather than the best run) of each system to account for"}
{"pre": "[CLS] the first is the automatic metric of text with human annotation of text segments as shown in #refr. [SEP]", "cit": "[CLS] in the statistical keyphrase extraction, many variations for term frequency counts have been proposed in the literature including relative"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] moses baseline: we trained a moses system #otherefr with the following settings: maximum sentence length 80, grow-diag-finaland symmetrization of"}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with a probability distribution", "cit": "[CLS] in coreference, we have to select high-scoring clusters of mentions from an exponential number of candidate clusters, such that"}
{"pre": "[CLS] the first is the polarity of a word is usually analyzed by a lexicon with a word in a", "cit": "[CLS] as a sanity check, we measured the accuracy of our sentiment analyzer on film reviews by training and evaluating"}
{"pre": "[CLS] the first is the automatic classification of discourse structure of discourse relations #refr. [SEP] is the discourse structure of", "cit": "[CLS] as noted previously #otherefr; #refr, it is often difficult to outperform a unigram/bigram model in the task of disagreement"}
{"pre": "[CLS] in the second international chinese word segmentation bakeoff #otherefr; #refr and the same task as the best results are", "cit": "[CLS] this idea was explored for joint inference by #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] sentence is a text", "cit": "[CLS] #refr also used a lexical coherence metric, but applied a graphical model where segmentations are graph cuts chosen to"}
{"pre": "[CLS] we use the minimum error rate training #refr to tune the feature weights for the feature weights of the", "cit": "[CLS] to parse quickly, we only visit the tight consistent #refr bi-spans with the help of word alignment a. [SEP]"}
{"pre": "[CLS] the second approach is to use a small number of seed sets of words to automatically extracted from corpora", "cit": "[CLS] for further experiments we used the stanford constituent parser #refr to get only the words that syntactically depend on"}
{"pre": "[CLS] the dependency parser is trained on the penn treebank #refr and the penn treebank #otherefr. [SEP] treebank #otherefr. [SEP]", "cit": "[CLS] the amount of information shared word vectors: features derived from 64-dimensional vectors from (faruqui and #refr, including the concatenation,"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] the classifier can be trained with maximum likelihood like moses lexicalized reordering #otherefr or be trained under maximum entropy"}
{"pre": "[CLS] the semantic parser used in this paper is the conll shared task on dependency parsing #refr. [SEP] [SEP] [SEP]", "cit": "[CLS] bod #otherefra); #refr; rajman #otherefr). [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be a difficult problem #otherefr;", "cit": "[CLS] we also achieve modularity of a more basic sort: our correspondence mechanism permits contrastive transfer ules that depend on"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the input is to be", "cit": "[CLS] syntactic dependency parsers and treebanks are in fact available for a variety of languages #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the sentence in", "cit": "[CLS] today, mt evaluation is an active field of research, and modern metrics perform analysis at various levels, e.g., lexical"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal (wsj) corpus #refr. [SEP] trees and the penn", "cit": "[CLS] in addition to gold training, we also investigate an alternative method, which is to obtain training data for the"}
{"pre": "[CLS] the most popular approach to paraphrase generation has been the extraction #otherefr; #refr. [SEP] model the probability of the", "cit": "[CLS] these ?facts? can be news articles on the same event #otherefr; #refr, different translations of a source text to"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a text with a text that are semantically", "cit": "[CLS] based on the pdtb, a number of studies have provided insightful analysis of the use of discourse connectives in"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] it is interesting toconstrast this method with the \"parse-parse-match\" approaches that have been reported recently for producing parallel bracketed"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within the meaning representation of the meaning of the", "cit": "[CLS] we frame direction following as an apprenticeship learning problem and solve it with a reinforcement learning algorithm, extending previous"}
{"pre": "[CLS] the first is the annotation tool which was the annotation tool #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] previous evaluations (e.g., those by #refr and liu and ng #otherefr) have been based on markable instances, which constitute"}
{"pre": "[CLS] the task of event extraction has received a lot of attention in recent years #otherefr; #refr. [SEP] [SEP] [SEP]", "cit": "[CLS] the top system by uturku (bjo#refr attained the state-of-the-art f1 of 52.0%. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second approach is to use a small number of manually annotated corpora such as the same data set", "cit": "[CLS] the following works exemplify this approach: #refr use a pos tagger to jointly segment, pos tag, and chunk base-phrases"}
{"pre": "[CLS] the translation quality is measured by bleu #refr. [SEP] score #refr with the bleu score of the translation quality.", "cit": "[CLS] there has been a number of efforts on dialect identification #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of event extraction was first introduced by #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] the related task of cross document coreference resolution has been addressed by several researchers starting from #refr. [SEP] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the sentence planning problem that the user utterances are semantically similar to", "cit": "[CLS] predicting user satisfaction for sdss has been in the focus of research for many years, most famously the paradise"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] of a text in", "cit": "[CLS] more recent research emphasizes the integration of mwe lexical entries into existing single word lexicons and grammar systems #refr."}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be np-hard #refr. [SEP] [SEP]", "cit": "[CLS] for example, #otherefr employs mt alignment from #refr; one might employ the ibm models for mt alignment, #otherefr. [SEP]"}
{"pre": "[CLS] the semantic parser used in this paper is the conll shared task on dependency parsing #refr. [SEP] [SEP] [SEP]", "cit": "[CLS] to obtain the ssynts, we use #refr?s transition-based parser, which combines lemmatization, pos tagging, and syntactic dependency parsing?tuned and"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] at each iteration of mer training, we run the parser and decoder over the ctb dev set to generate"}
{"pre": "[CLS] the unsupervised method used by #refr is based on the statistics of the words in a dictionary of a", "cit": "[CLS] examples include summarization #otherefr, question answering #refr and machine translation #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the automatic metric of text with human annotation of text segments as shown in #refr. [SEP]", "cit": "[CLS] given a short input text and a target word in english, and given several english substitutes for the target"}
{"pre": "[CLS] topic models have been successfully applied to a variety of tasks including word segmentation and sentence compression #otherefr; #refr,", "cit": "[CLS] the effectiveness of lexical cohesion has been demonstrated by text- tiling #otherefr, mincut #refr, plda #otherefr, etc. [SEP] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the training data is to", "cit": "[CLS] joint learning over hierarchical models has been successful in nlp #refr but to our best knowledge no one has"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] #refr recently introduced the use of pcfg for authorship attribution for the first time, and demonstrated that it is"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] for even a moderately complex domain such as the atis corpus, a grammar trained on data with constituent bracketing"}
{"pre": "[CLS] the task of segmenting utterances into words, for example, johnson and colleagues have been shown to be useful in", "cit": "[CLS] ner is typically viewed as a sequential prediction problem, the typical models include hmm #otherefr, and sequential application of"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] if we approximate the search space by restricting y(xi) to a k-best list, we have the local-update #refr, where"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] joshua #otherefr using a suffix-array-based grammar extraction approach #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the context of dependency parsing, previous work has shown that it is useful to improve the performance of", "cit": "[CLS] due to similar reasons, many works are devoted to pos projection #otherefr; #refr, and they also suffer from similar"}
{"pre": "[CLS] the most popular approach to mt system combination is the one proposed by #refr, which uses a log-linear combination", "cit": "[CLS] it?s been shown that active learning can be used to reduce training data annotation burdens for a variety of"}
{"pre": "[CLS] the work of #refr is similar to ours in that it is possible to use a more complex model", "cit": "[CLS] the artful use of language is central to politics, and the language of politicians has attracted considerable interest among"}
{"pre": "[CLS] in the context of sentiment analysis and opinion mining has been used to extract opinion expressions #otherefr; #refr. [SEP]", "cit": "[CLS] purandere and pedersen is the prominent representative of feature-based models. #refr creates context vectors from local feature representations similar"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a small set of sentences of sentences of", "cit": "[CLS] while many approaches have adapted nlp systems to specific domains #otherefr; #refr; daume. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a task of identifying the degree of semantic relations between words", "cit": "[CLS] currently, the best-performing english np interpretation methods in computational linguistics focus mostly on two consecutive noun instances #otherefr, #refr,"}
{"pre": "[CLS] we use the stanford ner tagger #refr to train the english and the chinese sentences. [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] we set the weights based on a dot product of features ? and parameters ?, which we learn from"}
{"pre": "[CLS] the most common approach to paraphrase acquisition is to use a lexicon to automatically extracted from corpora #otherefr; #refr.", "cit": "[CLS] the implementation of the standard approach can be carried out by applying the following three steps #otherefr; #refr, among"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] the prague dependency treebank (pdt) presents a language resource containing a deep manual analysis of texts#refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the task of learning semantic parsers from different data #otherefr; #refr. [SEP] [SEP] [SEP] grammars (scfg) and (2) the", "cit": "[CLS] various applications of stacking in nlp have been proposed, such as collective document classification #otherefr and joint chinese word"}
{"pre": "[CLS] the model is trained using the moses decoder #refr. [SEP] score #refr with the decoder that the best alignment", "cit": "[CLS] the common practice of plugging some aspect of a learned itg into either (a) a long pipeline of training"}
{"pre": "[CLS] the system is based on the inprotk project #refr. [SEP] score for the number of the development set of", "cit": "[CLS] the differentia can be partially interpreted relative to the rqs type; in this example < rqs : o r"}
{"pre": "[CLS] the generation of referring expressions is a subject of research in natural language generation (nlg) in natural language generation", "cit": "[CLS] the system belongs to the family of wysiwym (what you see is what you mean) #refr text generation systems:"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] algorithm with the best derivation of the best derivation", "cit": "[CLS] following this discriminative approach, techniques for efficiency were investigated for estimation #otherefr and parsing #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the similarity between two sentences is computed using the wordnet #otherefr and the wordnet similarity measure proposed by #refr.", "cit": "[CLS] related methods incorporate measurements of similarity at various levels: lexical #otherefr, syntactic #refr, and semantic #otherefr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the longrange reordering problem as a sequence of words", "cit": "[CLS] non-centered cosine is used, for instance, in word sense disambiguation #otherefr, and compositional semantics #refr, to name a few."}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to translate the phrases as the", "cit": "[CLS] finally the system generates a new translation by searching the lattice based on alignment information, each system?s confidence scores"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] in addition, due to its high efficiencies, it has also been applied to a range of joint structural problems,"}
{"pre": "[CLS] we use the stanford word segmenter #refr to extract the word pairs from the target word alignments and then", "cit": "[CLS] a more principle way is to learn a bilingual topic model from bilingual corpus #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] in all of the experiments in this paper, we use a string-to-tree decoder which uses a cyk style parser"}
{"pre": "[CLS] the grammar we use is a standard phrase-based translation system for the hierarchical phrase-based translation system #refr. [SEP] model", "cit": "[CLS] #refr discuss methods for binarizing scfgs, ignoring the nonbinarizable grammars; in section 2 we discuss the generalized problem of"}
{"pre": "[CLS] the most common approach to paraphrase extraction is to use a lexicon based on bilingual parallel corpus or comparable", "cit": "[CLS] in order to induce initial translation pairs, we rely on the framework of multilingual probabilistic topic modeling #otherefr; #refr,"}
{"pre": "[CLS] the system uses the illinois named entity recognizer #refr to extract coreference resolution from the coreference resolution of coreference", "cit": "[CLS] this strategy has been successfully used before for information extraction, e.g., in the bionlp 2009 event extraction shared task"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking approaches #otherefr; #refr to learning for dependency", "cit": "[CLS] table 1 gives additional statistics for treebanks from the conll-x shared task #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the system is based on the standard approach of #refr and the standard log-linear pb-smt model: #otherefr. [SEP] evaluation", "cit": "[CLS] statistical systems often benefit from linguistic preprocessing to deal with rich morphology and long distance reordering issues #refr. [SEP]"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] english is the most studied language, using the framenet (fn) #refr and propbank #otherefr resources. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] translation model combination: in this experiment, we try to use the method of #refr to combine phrase tables or"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for each sentence in the sentence with", "cit": "[CLS] we run the unsupervised berkeleyaligner3 #refr for 4 iterations to obtain word alignments. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained using the c&c parser #refr. [SEP] parser is trained on the wall street journal (wsj)", "cit": "[CLS] xmg was used to develop a core grammar for french (frag) which was evaluated to have 75% coverage4 on"}
{"pre": "[CLS] the most common approach to word alignment is to use a word alignment model #otherefr; #refr. [SEP] model to", "cit": "[CLS] co-occurrence information between eighboring words and words in the same sentence has been used in phrase extraction #otherefr; #refr,"}
{"pre": "[CLS] the most popular approach to paraphrase generation has been the source sentences into either by using a monolingual parallel", "cit": "[CLS] examples of monolingual parallel corpora that have been used are multiple translations of classical french novels into english, and"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] recent studies #otherefr #refr have shown that more than 60% of word segmentation errors result from new words. [SEP]"}
{"pre": "[CLS] the first is the task of learning a semantic parser that is a complete parse of a dependency parser", "cit": "[CLS] for srl, high accuracy has been achieved by: #otherefr; #refr; toutanova et al, 2005; punyakanok et al, 2005a), #otherefrb)."}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for each sentence in the training corpus.", "cit": "[CLS] supervised learning approaches have advanced the state of the art on a variety of tasks in natural language processing,"}
{"pre": "[CLS] the most common approach to paraphrase generation is to use a lexicon to paraphrase generation #otherefr; #refr. [SEP] words", "cit": "[CLS] more recent approaches in steganography have used paraphrase substitution to encode information in text but focus on grammaticality, not"}
{"pre": "[CLS] the second approach is to use a small number of manually annotated corpora such as the same data set", "cit": "[CLS] when the tagger is trained in tested on the upenn treebank #otherefrb) adopted a two-stage approach to prediction, first"}
{"pre": "[CLS] #refr use a similar approach to identify the polarity of words in a sentence and their polarity in a", "cit": "[CLS] fazly et al. #otherefr; li and #refr, among others, notice that type-based approaches do not work on expressions that"}
{"pre": "[CLS] the second approach is to use a semantic parser to generate the parse tree from the target word representations", "cit": "[CLS] besides these approaches, recently there are also several works that take alternative learning approaches for semantic parsing which do"}
{"pre": "[CLS] #refr proposed a method for identifying paraphrases based on a similarity between two sentences that is used to identify", "cit": "[CLS] with these unsupervised methods, we can extract useful semantic information in a variety of tasks that depend on identifying"}
{"pre": "[CLS] we use the berkeley parser #refr to obtain the syntactic parse trees for parse trees for english and the", "cit": "[CLS] for stanford dependency trees, we parsed the source sentences with the stanford parser #refr (version 3.3.1), which was trained"}
{"pre": "[CLS] the work of #refr is similar to ours in that it is possible to use a more complex linguistic", "cit": "[CLS] and automatic citation classification #otherefr; #refr determines the function that a citation plays in the discourse. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the semantic textual similarity of a document level of a text of", "cit": "[CLS] glsa extends the ideas of lsa by defining different ways to obtain the similarities matrix and has been shown"}
{"pre": "[CLS] in the context of machine translation, itg has been explored for mt #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] in recent years, there has been an increasing interest in the task of paraphrase generation #otherefr; #refr. [SEP] [PAD]"}
{"pre": "[CLS] we use the stanford corenlp suite to lemmatize and partof-speech tag each word #otherefr; #refr, and parse the input", "cit": "[CLS] several researchers in the 1980s and 1990s addressed the problem, the most influential of which include: rhetorical structure theory"}
{"pre": "[CLS] in the last decade, there has been a lot of work on learning from corpus data #otherefr; #refr. [SEP]", "cit": "[CLS] this can be formulated as a two-step process of first segmenting words, then estimating poss #otherefr; #refr. [SEP] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] using such techniques #refr report significative improvement on the penn treebank #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the sentence in", "cit": "[CLS] so tile goal can be usefblly approximated witl~ formalisms which make some limited distincfions between informatio~a pplying to parsing"}
{"pre": "[CLS] the first is the case for the automatic evaluation of the automatic evaluation of the automatic evaluation of the", "cit": "[CLS] efficiently identifying useful features for tree kernel methods. #refr [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] ten corpora of nine different languages are used in the challenge: arabic #otherefr, english childes #refr, portuguese #otherefr. [SEP]"}
{"pre": "[CLS] #refr used a supervised sentiment classification approach to classify reviews as well as sentiment classification and classify reviews as", "cit": "[CLS] #refr also observed significant differences between the accuracy of classification of reviews in the same domain but published in"}
{"pre": "[CLS] the system is based on the inprotk project at the university of pennsylvania and the xerox project at the", "cit": "[CLS] our system can extract a number of markings, features and relations from the parsed, part-of-speech-tagged corpora of the type"}
{"pre": "[CLS] the translation model is a log-linear model #refr. [SEP] model weights ? ? ? ? ? ? ? ?", "cit": "[CLS] in #otherefr, a simple phrase-based approach is described that served as starting point for the system in this work."}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for each sentence in the training corpus.", "cit": "[CLS] hypergraph-based discriminative training for better feature engineering (li and #refrb) . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second approach is to use a maximum entropy model #otherefr, and a log-linear combination of the hidden markov", "cit": "[CLS] we propose no new solutions to handling irregular verb forms, but suggest using non-string-based techniques, such as those presented"}
{"pre": "[CLS] in this paper, we propose a method for learning a semantic parser based on the maximum entropy model of", "cit": "[CLS] there have already been some attempts, like #refr and #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] joshua ships with mert #refr and pro implementations. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] #refr, for example, mention that using functional tags of the penn treebank (temporal, location, subject, predicate, etc.) with a"}
{"pre": "[CLS] the first is the case for the semantic role labeling task of syntactic parsing, we use the penn treebank", "cit": "[CLS] 1 current statistical parsers do not use or output this richer information because performance of the parser usually decreases"}
{"pre": "[CLS] the second approach is to use a small number of seed examples for example outputs #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] #refr used atomic event as concept. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders and opinion holders in opinion holders in opinion holders in", "cit": "[CLS] to aid these tasks, recent work #otherefr; #refr has tackled the issue of identifying the orientation of subjective terms"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from the sentence pairs in the input sentence by", "cit": "[CLS] the model used to obtain the svm baseline for concept classification was trained using yamcha #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the grammar we are extracted from the treebank using the stanford parser #refr. [SEP] parser #refr with the self-trained", "cit": "[CLS] moreover, we make use of the geometric mean of the probability instead of the original probability in order to"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for each sentence in the sentence with", "cit": "[CLS] it has been shown that more sophisticated probability models #otherefr; #refr can lead to significant improvement over klein and"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] in question answering, for example, paraphrase generators can be used to paraphrase the user?s queries #otherefr; #refr. [SEP] [PAD]"}
{"pre": "[CLS] the second approach is to use a small number of seed sets of words to automatically extracted from the", "cit": "[CLS] in natural language learning, co-training was applied to statistical parsing #otherefr, part of speech tagging #refr, and others, and"}
{"pre": "[CLS] in the literature, supervised learning methods have been applied to part-of-speech tagging and unsupervised learning problems in nlp #otherefr;", "cit": "[CLS] to make use of the information about future tags, toutanova et al proposed a tagging algorithm based on bidirectional"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] neg t- inc in- e1s ch?abe vt -j sc laj prep in- a1s yol s -j sc iin pron"}
{"pre": "[CLS] topic models have been successfully applied to a wide range of nlp tasks including word sense disambiguation #otherefr, and", "cit": "[CLS] other metrics were also calculated (e.g. the v-measure #refr, and average entropy #otherefr), but these results were excluded due"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] as used in this work, arguing is a type of linguistic subjectivity, where a person is arguing for or"}
{"pre": "[CLS] the second approach is to use a word clusters as described in #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] for example, the text- tiling algorithm, introduced by #refr, assumes that the local minima of the word similarity curve"}
{"pre": "[CLS] the second approach is to use a small number of manually annotated corpora such as the penn treebank #otherefr,", "cit": "[CLS] part-of-speech #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] in the literature, supervised methods have been proposed for learning of natural language processing #otherefr; #refr. [SEP] [SEP] [SEP]", "cit": "[CLS] #refr proposed methods to improve parsing performance using bilingual parallel corpus. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the event descriptions of the bionlp?09 shared task on event extraction #refr.", "cit": "[CLS] #refr made use of this to generate automatic summaries by considering edus which are nuclei to be more salient."}
{"pre": "[CLS] the task of identifying the semantic textual similarity of a target word is a given a document #otherefr; #refr.", "cit": "[CLS] there does not exist a fully agreed evaluation metric for clustering tasks in nlp #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the similarity between two words was computed using the wordnet similarity measure proposed by #refr. [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] for example, #refr defined a similarity measure for automatic thesaurus creation from a corpus. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] the", "cit": "[CLS] a comprehensive description of the task may be found in #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr proposed a method for learning semantic parsers from a target language for a target language for a target", "cit": "[CLS] for evaluating the german dts, we replace wordnet by its german counterpart, germanet 8 #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the penn treebank #otherefr. [SEP] score", "cit": "[CLS] table 1 lists three corpora in the biomedical domain that are annotated with deep syntactic structures; craft #otherefr, and"}
{"pre": "[CLS] the work of #refr is similar to ours in that it is possible to use a more complex model", "cit": "[CLS] we have implemented the procedure described in #refr using unigrams only. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for each sentence with dependency trees for", "cit": "[CLS] to minimize information loss during treebank conversions, a treebank could be designed by considering ps and ds information simultaneously;"}
{"pre": "[CLS] the grammar is a grammar that is a grammar that is a parsing model that is a word lattice", "cit": "[CLS] this paper assumes ome familiarity with the tag formalism. #otherefr and (joshi and #refr are good introductions to the"}
{"pre": "[CLS] the most popular approach to phrase-based translation models #otherefr; #refr rely on the source side and the target side", "cit": "[CLS] these methods share a common characteristic: they learn a weight by iteratively reranking a set of complete translations represented"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] model with respect to the entire candidate set.", "cit": "[CLS] by exploiting information encoded in human-produced syntactic trees #otherefr, research on probabilistic models of syntax has driven the performance"}
{"pre": "[CLS] the first is the case for the generation of referring expressions from the linguistic phenomena of referring expressions and", "cit": "[CLS] work on incremental parsing and interpretation is typically motivated by a desire to model, or mimic, #otherefr, #refr, and"}
{"pre": "[CLS] the first is the process of annotating the sentence planning problem of the sentence pairs in the sentence planning", "cit": "[CLS] instead, we follow work done in dialog systems #refr and attempt o find metrics which on tim one hand"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of meaning that are not", "cit": "[CLS] the predominant unsupervised approach for learning inference rules between templates is via distributional similarity #otherefr; #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the similarity between two sentences is computed using the wordnet #otherefr and the wordnet similarity measure proposed by #refr.", "cit": "[CLS] to utilize this, we introduce two additional list pattern types7: [prefix] c1[infix] (ci[infix])+ (1) [infix] (ci[infix])+ cn [postfix] (2)"}
{"pre": "[CLS] the system uses a combination of natural language processing (nlp) systems to produce various types of syntactic and machine", "cit": "[CLS] while wsi has considerable appeal as a task, intrinsic cross-comparison of wsi systems is fraught with many of the"}
{"pre": "[CLS] the most popular approach to paraphrase generation has been the task into smt #otherefr; #refr. [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] in contrast, bilingual parallel data is in abundance and has been used in extracting paraphrase #otherefr; #refr. [SEP] [PAD]"}
{"pre": "[CLS] we use the stanford ner tagger #refr to train the word segmenter and pos and pos tags for each", "cit": "[CLS] research on unsupervised learning for nlp has become widespread recently, with part-of-speech induction, or syntactic class induction, being a"}
{"pre": "[CLS] #refr proposed a method for learning subjective language pairs from the target language and used to predict the target", "cit": "[CLS] for example, #refr develop a system that exploits a crf model to segment named 1http://www.twitter.com entities and then uses"}
{"pre": "[CLS] the first is the automatic evaluation of the automatic metric of the automatic evaluation of the automatic evaluation of", "cit": "[CLS] some other possible similarity measures are document edit distance, the language models from #otherefr, or generation probabilities from #refr."}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the longrange reordering model is to use a word", "cit": "[CLS] a line of work that offers similar treatment of alignment to our model is the quasi-synchronous grammar (qg) #refr."}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for parsing #refr. [SEP] grammars with", "cit": "[CLS] the idea of exploiting lexical dependency information from auto-parsed data has been explored before for dependency parsing #refr and"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] the en-de system adds a future cost component to the linear distortion model #refr.the future cost estimate allows the"}
{"pre": "[CLS] we use the berkeley parser #refr to obtain the syntactic parse trees for each sentence and the english and", "cit": "[CLS] systems are trained on a selected 1.2m chinese?english corpus. train a 5-gram language model on the xinhua portion of"}
{"pre": "[CLS] #refr use a similar approach to compute the polarity of words in a sentence. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] waltinger #otherefra) utilised automatic translations of two english resources: the sentispin lexicon, described in section 3.3 above; and the"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best results for the best", "cit": "[CLS] as regards the basic feature set for chunking, we followed #refr, which is the same feature set that provided"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] they can be roughly divided into three categories: string-to-tree models #otherefr), tree-to-string models (e.g., #refr), and tree-totree models #otherefr)."}
{"pre": "[CLS] the system is based on the moses toolkit #otherefr and the standard phrase-based smt system described in #refr. [SEP]", "cit": "[CLS] these two approaches are: improved translation accuracy via system combination #otherefr, and automatic quality-estimation techniques used as an additional"}
{"pre": "[CLS] the first is the one of the most popular approach to use crowdsourcing, which has been to use a", "cit": "[CLS] in a previous corpus-study, using transcriptions of spontaneous, task-oriented and non-task-oriented dialogue, utterances were annotated with syntactic trees, which"}
{"pre": "[CLS] the data set contains 14,619 items and is used to evaluate the gold standard data set used in the", "cit": "[CLS] al has already been applied to several nlp tasks, such as document classification #otherefr, pos tagging #refr, chunking #otherefr."}
{"pre": "[CLS] the most common approach to parsing is to use beam search #otherefr; #refr. [SEP] grammars with a probability distribution", "cit": "[CLS] by using other kernel functions, such as polynomial or radial basis function #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] in #refr, alignment model mixtures were explored as a way of performing topic-specific adaptation. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the averaged perceptron algorithm #refr to train the weight for each system. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] our approach improves upon #refrb) state-of-theart decoder, creating a 28.5% relative improvement in transliteration accuracy on a japanese katakana-to-english"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a word sense disambiguation task", "cit": "[CLS] the k?means algorithm is used for clustering the contexts, where the number of clusters is automatically discovered using the"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] we call the phrase pairs with all boundary words aligned tight phrase pairs #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr proposed a method for learning a word sense disambiguation which is a word sense disambiguation method to be", "cit": "[CLS] the overall disambiguation accuracy achieved by our method is 85.2%, which compares favorably against he accuracy (82.4%) obtained by"}
{"pre": "[CLS] the first is the discourse structure of discourse structure of discourse structure of discourse structure in the discourse structure", "cit": "[CLS] researchers have used these frameworks to evaluate the effectiveness of spoken dialog in interactive systems #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a similar to the one used in the sense of the", "cit": "[CLS] our baseline is a state-of-the-art srl system based on dependency syntactic tree #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to translate the phrases and the", "cit": "[CLS] the main differences with respect to previous year?s system #refr are as follows: #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of possible target word is", "cit": "[CLS] given the complementary nature of those two semantic models, it is not surprising that considerable research activity has been"}
{"pre": "[CLS] the second approach is to use a small number of seed examples for example outputs #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] no-edge model uses only the vertex features, while each of the sn-edge models makes use of the edge features"}
{"pre": "[CLS] #refr proposed a method for extracting phrases from a parallel corpus of sentences from the web corpus of news", "cit": "[CLS] we used the dataset by #refr, which we selected because it contains multiple realizations of an entity pair in"}
{"pre": "[CLS] in the second international chinese word segmentation bakeoff #otherefr; #refr and the same task as the best results are", "cit": "[CLS] where van zaanen #otherefr reports 42.0% on the same data, and #refr obtain 51.2% f-score on atis part-of-speech strings"}
{"pre": "[CLS] #refr use a similar approach to compute sentence importance. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] unfortunately these studies used different parts of the corpus or different labelings #refr, so our results are not directly"}
{"pre": "[CLS] the task of event extraction was first introduced by #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] the use of instances that have been incorrectly labelled as positive can lower performance #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the penn treebank #otherefr. [SEP] grammars", "cit": "[CLS] the cutting criteria employed in grammar specialization either require carefully manually tuning, or require more complicated statistical techniques #refr;"}
{"pre": "[CLS] #refr proposed a method for learning a probabilistic model that learns vector for automatically from a small set of", "cit": "[CLS] most existing models only intend to show the relevance of such properties to the acquisition of adult-like syntactic categories"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] shared", "cit": "[CLS] while several participants made use of supporting syntactic analyses provided by the organizers #otherefr, none applied the analyses for"}
{"pre": "[CLS] the task of relation extraction has received much attention in recent years #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] an lvc is a multi-word expression composed of a verb and a noun, with the noun carrying the semantic"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] much of this work has focused on up-weighting subsets of the training or language modeling data that are most"}
{"pre": "[CLS] the first is the task of identifying the polarity of a text with a text that is a text", "cit": "[CLS] #refr note acronyms in biomedical literature tend to be used much more frequently than in news media or general"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to learn a translation probabilities from", "cit": "[CLS] scfg translation models have been shown to be well suited for german-english translation, as they are able to both"}
{"pre": "[CLS] the task of identifying the overall sentence pairs in text is a text #refr. [SEP] of the same event", "cit": "[CLS] most research conducted in the nlp community focuses on extracting local relations between concept pairs #otherefr; #refr. [SEP] [PAD]"}
{"pre": "[CLS] the most common approach to paraphrase acquisition is to use a lexicon to automatically induced paraphrases from corpora #otherefr;", "cit": "[CLS] as a consequence the same entities will be denoted with the same words in different languages, allowing us to"}
{"pre": "[CLS] #refr use a similar approach to compute similarities between pairs of sentences in a source and target languages by", "cit": "[CLS] the information needed to calculate these features was extracted using the stanford tagger #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second approach is to use a small number of seed sets of words and their similarity and their", "cit": "[CLS] #refr present a detailed evaluation of topic signatures built from a variety of knowledge sources. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] words in a text", "cit": "[CLS] a great number of researchers have addressed the problem of query-focused summarization #otherefr; daume? and #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the feature weights were tuned using minimum error rate training (mert) #refr on the development set using minimum error", "cit": "[CLS] for optimization, we use a learning rate of ?=1, regularization strength of c=0.01, and a 500-best list for hope"}
{"pre": "[CLS] in the context of machine translation, itg has been explored for statistical word sense disambiguation in both supervised #otherefr;", "cit": "[CLS] to build such structures, we used the stanford pos tagger #refr, the illinois chunker #otherefr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] we used the #refr (bllip) parser1 to perform the self training experiments. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a task of identifying the degree of semantic relations between words", "cit": "[CLS] 5http://www.nactem.ac.uk/download.php?target=grec/ event annotation guidelines.pdf name abbreviation publication bionlp/nlpba 2004 shared task corpus nlpba kim et al #otherefr epigenetics and"}
{"pre": "[CLS] #refr use a similar approach to identify the polarity of words in a sentence. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] recently, the lexicon based approaches were extended to learn domaindependent lexicons #refr, but these approaches also neglect the aspect"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for each sentence with dependency trees for", "cit": "[CLS] furthermore, end-to-end systems like speech recognizers #otherefr and automatic translators #refr use increasingly sophisticated discriminative models, which generalize well"}
{"pre": "[CLS] the task of identifying the semantic textual similarity of a target word is a document #otherefr; #refr. [SEP] of", "cit": "[CLS] these include not only domain driven disambiguation algorithms #otherefr as well as algorithms that quantify the degree of association"}
{"pre": "[CLS] in the literature, slight variations of this problem are also referred to as unsupervised learning #otherefr, and semi-supervised learning", "cit": "[CLS] the third task is taken from #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] previous work #otherefr; #refr has developed a spectral method for learning of l-pcfgs; this method learns parameters of the"}
{"pre": "[CLS] #refr proposed a method for learning a word sense disambiguation which is a word sense disambiguation method to extract", "cit": "[CLS] an alternative approach proposed by #refr that handles synonyms, antonyms and associations is to use a uniform approach by"}
{"pre": "[CLS] we use the stanford ner tagger #refr to train the word segmenter #otherefr to obtain the word segmenter for", "cit": "[CLS] the wordframe model #refr uses inflection-root pairs, where unseen inflections are transformed into their corresponding root forms. [SEP] [PAD]"}
{"pre": "[CLS] the second approach is to use a semantic parser to generate the parse tree and the sentence into a", "cit": "[CLS] examples include the tree-structured semantic representations #otherefr; #refr, and dependency-based compositional semantic representations #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the translation system is evaluated by bleu score #refr on the nist #otherefr. [SEP] quality measured by the translation", "cit": "[CLS] these hybrid decoders search for the target language sentence e? that maximizes the following probability, where g(o) represents the"}
{"pre": "[CLS] the first is the task of coreference resolution #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] ? ? ? ?", "cit": "[CLS] in ace05-all, we have the full ace 2005 training set and use the standard train/test splits reported in #refr"}
{"pre": "[CLS] the model is trained with a maximum entropy model #otherefr, which is trained on the penn treebank wall street", "cit": "[CLS] we will use x? ? x to indicate that the subscript or subcategory x is a refinement of x?.1"}
{"pre": "[CLS] #refr proposed a method for learning a word clustering words from a target language model to target language to", "cit": "[CLS] we use the standard hyperparameters values ? = 1.0, ? = 0.01 and ? = 1.0 and run the"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to translate the words and the", "cit": "[CLS] for all language pairs, we used the moses decoder #otherefr, which follows the phrase-based statistical machine translation approach #refr,"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a word in a word", "cit": "[CLS] phrase patterns have been used in other nlp applications such as #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the translation quality is measured by bleu #refr. [SEP] score #refr with the bleu score of the translation quality.", "cit": "[CLS] they were also processed in order to normalize a special french form (named euphonious ?t?) as described in #refr."}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking approaches #otherefr; #refr to learning for dependency", "cit": "[CLS] research in the field of unsupervised and weakly supervised parsing ranges from various forms of em training #otherefr and"}
{"pre": "[CLS] id institution balagur yandex school of data analysis #otherefr commercial-1,2,3 anonymized commercial systems online-a,b,g anonymized commercial systems online-a,b,g anonymized", "cit": "[CLS] id institution balagur yandex school of data analysis #otherefr talp-upc talp research centre #refra) tubitak tu?bi?tak-bi?lgem #otherefr commercial-1,2,3 anonymized"}
{"pre": "[CLS] #refr proposed a method for learning a word sense disambiguation which is a word sense disambiguation method to identify", "cit": "[CLS] in the recent wssim annotation study #refr, senses sentence 1 2 3 4 5 6 7 annotator this question"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be a given by using", "cit": "[CLS] for example, we can use such a translation model to help complete target ext being drafted by a human"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? 2008. [SEP]", "cit": "[CLS] corpus-based word sense disambignation algorjthm~ such as (ng and #refr relied on supervised learning fzom annotated corpora. [SEP] [PAD]"}
{"pre": "[CLS] we use the stanford chinese word segmenter #refr to tokenize the english sentences. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] therefore, other machine learning techniques such as perceptron #refr could also be applied for this problem. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a text that is a", "cit": "[CLS] such studies have for example been described by ogren et al#otherefr, #refr, uzuner et al#otherefr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] to find such groups of morphemes and functional words, we applied a sequence of morpheme groupings by extracting frequently"}
{"pre": "[CLS] the task of paraphrase extraction is similar to the one described by #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] so far, cross-lingual textual entailment (clte) #refr has been applied to: i) available te datasets #otherefrb). [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the translation quality is measured by bleu #refr. [SEP] [SEP] = 4. [SEP] = 4. [SEP] = ? ?", "cit": "[CLS] a few exceptions are the hierarchical #otherefr; #refr and the string transduction models #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the averaged perceptron algorithm #refr to train the parameters of the averaged perceptron algorithm with the averaged", "cit": "[CLS] for example, the cubic grandparent edges in second-order dependency parsing slow down dynamic programs #otherefr, belief propagation #refr and"}
{"pre": "[CLS] the first is the task of identifying the semantic relations between entities #otherefr; #refr. [SEP] of a sentence, [SEP]", "cit": "[CLS] many ace participants have also adopted a corpus-based approach to sc determination that is investigated as part of the"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to translate the phrases as the", "cit": "[CLS] we use the offline approximation in which failure transitions are replaced with epsilons #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the system is based on the inprotk project #refr. [SEP] score for the user is not a given presentation", "cit": "[CLS] figure 3: the scene of fig. 1, after the user moved and turned in response to a referring expression."}
{"pre": "[CLS] the most common approach to machine translation is to use syntactic information to enhance the source sentences #refr. [SEP]", "cit": "[CLS] #refr propose a hierarchical model for word, phrase and sentence level combination; they use lms and interestingly find that"}
{"pre": "[CLS] #refr use a morphological analyzer and morphological analyzer and morphological analysis to train a pos tagging method for arabic", "cit": "[CLS] among this literature, #refr investigate unsupervised learning of stemming (a variant of tokenization in which only the stem is"}
{"pre": "[CLS] the most popular approach to translation models #otherefr; #refr rely on parallel data from the web corpus #otherefr. [SEP]", "cit": "[CLS] the figure illustrates a problem known as garbage collection #refr, where rare source words tend to align to many"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be a difficult problem #otherefr;", "cit": "[CLS] #refr, alshawi et at. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the berkeley parser #refr to obtain the syntactic parse trees for each sentence and the english and", "cit": "[CLS] this model can also be seen as a more generalised case of the hmm word-to-word model #otherefr; #refr, since"}
{"pre": "[CLS] the second approach is to use a semantic similarity measure and is based on the similarity of the similarity", "cit": "[CLS] in order to do better joint learning, a novel statistical relational learning framework, markov logic #otherefr was introduced to"}
{"pre": "[CLS] the translation quality is measured by bleu #refr. [SEP] bleu score #refr with the bleu score of the translation", "cit": "[CLS] in smt, the parameter space is explored by a tuning algorithm, typically mert (minimum error rate training) #refr, though"}
{"pre": "[CLS] the most popular approach to translation models #otherefr; #refr rely on parallel data #otherefr. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] we are not the first to consider this idea; #refr developed a similar approach for learning an itg using"}
{"pre": "[CLS] in the context of machine translation, itg has been explored for a variety of nlp tasks, including semantic role", "cit": "[CLS] for example, lin et al #otherefr described a parenthesis translation mining method; #refr applied graph alignment algorithm to obtain"}
{"pre": "[CLS] the first is the task of identifying the overall sentence pairs from the sentence pairs in the same text", "cit": "[CLS] dialogue acts are used to describe the function or role of an utterance in a discourse, and have been"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for learning and beam-search decoding #otherefr;", "cit": "[CLS] table 8: average accuracies over conll evaluation sets #otherefr, rf); and (iii) transfer delexicalized parsers (s?#refra, s) from resource-rich"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the training data is to", "cit": "[CLS] work on kernels for natural language parsing has been mostly focused on the definition of kernels over parse trees"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be np-hard #refr. [SEP] s", "cit": "[CLS] most of the previous approaches for acquiring syntactic transfer or reordering rules from parallel corpora use syntactic information from"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to learn a translation probabilities from", "cit": "[CLS] similarly, #refr and #otherefr show that the ordering problem can be more accurately solved by building a source-sentence word"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? the system", "cit": "[CLS] this includes the seminal paper of #otherefr, #refr and many others. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to parsing is to use a log-linear parsing model and maximise the conditional random field", "cit": "[CLS] this was shown in #refr, in which malt parser was used as a feature to mst parser. [SEP] [PAD]"}
{"pre": "[CLS] the system used in the conll shared task #refr is based on the genia corpus #otherefr. [SEP] shared task", "cit": "[CLS] evaluations such as the knowledge base population #otherefr, or different works on relation extraction of person entities #refr are"}
{"pre": "[CLS] we use the stanford core nlp suite #refr to identify the text in our work. [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] as single words are proved to be not illustrative of semantic meanings #refr and n-grams are rigid in length,"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? ? ? ?", "cit": "[CLS] our question taxonomy is derived from the uiuic dataset (li and #refr which defines 6 coarse and 50 fine"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] by contrast, explicit syntax approaches seek to directly model the relations learned from parsed data, including models between source"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] model with the average number of the average", "cit": "[CLS] vector-space models of lexical semantics have been a popular and effective approach to learning representations of word meaning #otherefr;"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the sentences and the syntactic parse trees of the sentences and", "cit": "[CLS] features 9?10 use the list of hedges from hyland #otherefr, features 19?20 use the entailments from #refr, features 21?25"}
{"pre": "[CLS] the first is the case for the identification of the penn treebank #refr, which contains the penn treebank #otherefr.", "cit": "[CLS] the algorithm operates similarly to the em algorithms used for grammar induction #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the semantic relation between nominals in the verb frame semantics and the semantic relations has been shown to be", "cit": "[CLS] motivation: open-domain information extraction methods #otherefr aim at distilling text into knowledge assertions about classes, instances and relations among"}
{"pre": "[CLS] the most common approach to machine translation is to use a log-linear framework #refr. [SEP] model to handle the", "cit": "[CLS] paraphrasing techniques can produce additional translation variants #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal, parser #refr and tested on the penn treebank", "cit": "[CLS] statistical disambiguation such as #refr for pp-attachment or #otherefr for generative parsing greatly improve disambiguation, but as they model"}
{"pre": "[CLS] #refr proposed a method for identifying the polarity of a document by a document into a document into a", "cit": "[CLS] 1998; okumura and mochizuki, 2000; #refr to verify that the method is useful for various applications. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within the source sentence into a target sentence into", "cit": "[CLS] also, most other research on evaluating grammaticality involves artificial tasks or datasets #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the generation of referring expressions is a subject of research in natural language generation (nlg) in natural language generation", "cit": "[CLS] stochastic methods for nlg may provide such automaticity, but most previous work #refr, #otherefr concentrate on the specifics of"}
{"pre": "[CLS] the task of identifying the semantic relations between nominals of relations and relations between nominals (gi#refr, the semantic relations", "cit": "[CLS] distributional thesauri have been used in a wide variety of areas including sentiment classification #otherefr, lexical substitution #refr, taxonomy"}
{"pre": "[CLS] in the context of machine translation, itg has been explored for a variety of nlp tasks, including pos tagging", "cit": "[CLS] on the other hand, al has been successfully used in a number of natural language applications such as text"}
{"pre": "[CLS] the system is a phrase-based system that uses a log-linear combination of giza++ #refr and the moses system combination", "cit": "[CLS] the main characteristic that sets our approach apart from other system combination software such as many #refr and memt"}
{"pre": "[CLS] the most common approach to paraphrase acquisition is to use a lexicon to generate paraphrases from corpora #otherefr; #refr.", "cit": "[CLS] other work in gec has defined the confu- 1for simplicity, we will collectively refer to both terms as english"}
{"pre": "[CLS] the model weights of the log-linear model are optimized with minimum error rate training (mert) #refr. [SEP] model weights", "cit": "[CLS] these schemes are overall limited by the quality of the translation hypotheses #refr, and better initial translation hypotheses lead"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the penn treebank #otherefr. [SEP] grammars", "cit": "[CLS] for 1), we don?t have any explicit information in the kyoto corpus although, in principle, each chunk has internal"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of meaning that are not", "cit": "[CLS] two evaluation metrics are used during the unsupervised evaluation in order to estimate the quality of the clustering solutions,"}
{"pre": "[CLS] the second approach is to use a word segmentation and pos tagger to tag dictionary to tag each word", "cit": "[CLS] #refr proposed a japanese unknown word model which considers pos (part of speech), word length model and orthography. [SEP]"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a word sense disambiguation of", "cit": "[CLS] a quite similar approach of using crowdsourcing has been considered by #refr for evaluating inference rules that are discovered"}
{"pre": "[CLS] the first is the case for the identification of the language model of the meaning of the meaning of", "cit": "[CLS] 1another possibility is normalization by the length of the longest alignment #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the translation quality is measured by bleu #refr. [SEP] bleu score #refr on the test sets. [SEP] [SEP] [SEP]", "cit": "[CLS] id participant cmu-uka carnegie mellon university, usa #otherefr cu charles university, czech republic #refr limsi limsi-cnrs, france #otherefr systran"}
{"pre": "[CLS] #refr use a bootstrapping method to identify the polarity of words in a sentence. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] opinion analysis is thus typically approached as a classification #otherefr; #refr task by which fragments of the input are"}
{"pre": "[CLS] the most popular approach to smt is the one proposed by #refr, who use a large set of features", "cit": "[CLS] a body of work has recently been proposed to apply al techniques to smt #otherefr; #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the entire sequence of the entire", "cit": "[CLS] as a special instance of the expectation-maximization #otherefr, based on the principle of maximum-likelihood estimation (mle), the standard io"}
{"pre": "[CLS] the parser is trained on the penn treebank corpus and tested on the penn treebank wall street journal, parser", "cit": "[CLS] tested against a 38,000-word corpus of previously unseen text, the tagger eaches a better accuracy than previous ystems (over"}
{"pre": "[CLS] the first is the task of identifying the polarity of a sentence, a sentence, and a word is then", "cit": "[CLS] mihalcea and moldovan #refr use the semantic density between words to determine the word sense. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the unsupervised pos induction task has been studied in the past decade #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] we collected part-of-speech tagged corpora for 9 languages, from conll-x and conll-2007 shared tasks on dependency parsing #otherefr; #refr."}
{"pre": "[CLS] the system is based on the inprotk toolkit for instance in the standard evaluation of #refr, which is used", "cit": "[CLS] i further use the framework of factored machine translation, where each word is represented as a vector of factors,"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] event and semantic relations", "cit": "[CLS] most previous methods of contradiction extraction require either thesauri like roget?s or wordnet #otherefr or large training data for"}
{"pre": "[CLS] the system is based on the stanford parser #refr and the stanford parser #otherefr. [SEP] system #otherefr. [SEP] system", "cit": "[CLS] recent annotation tools include the web-based brat #refr and its extension webanno #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] constituents our approach maps each argument label to one syntactic constituent, using a strategy similar to #refr. [SEP] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] previous work on coordination disambiguation has focused on the task of addressing the scope ambiguity (e.g., #refr). [SEP] [PAD]"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] recently a two-pass approach has been proposed #refr, wherein a lowerorder n-gram is used in a hypothesis-generation phase, then"}
{"pre": "[CLS] the most common approach to machine translation is to use a log-linear combination of statistical machine translation #otherefr; #refr.", "cit": "[CLS] this is not a surprise; comparisons of content-based metrics for summarization in #refr have led the authors to the"}
{"pre": "[CLS] the model is trained with a maximum entropy model #otherefr, which is trained on the penn treebank grammar and", "cit": "[CLS] the upper half of the table shows the performance using the correct poss in the penn treebank, and the"}
{"pre": "[CLS] the first is the task of identifying the event and event extraction of event descriptions of event descriptions of", "cit": "[CLS] #refr discuss some of the complexities in automatic detection of genre using a set of computationally efficient cues, such"}
{"pre": "[CLS] the system is based on the inprotk toolkit for the paradise evaluation of #refr, which uses a combination of", "cit": "[CLS] with advances in speech technology, several projects have begun to incorporate basic spoken language capabilities into their systems #otherefr;"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] algorithm with the best performing models is the average", "cit": "[CLS] the model presented in this paper has some elements borrowed from prior work, notably #otherefr; xu and #refr, while"}
{"pre": "[CLS] the system is based on the standard approach of #refr and the standard log-linear pb-smt model: #otherefr. [SEP] evaluation", "cit": "[CLS] commonly used models such as hmms, n-gram models, markov chains, probabilistic finite state transducers and pcfgs all fall in"}
{"pre": "[CLS] #refr used a similar approach to word alignment in a word alignment model. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] we use penn treebank-style part-of-speech tags as a substrate for further enrichment (for all of the experiments described here,"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a text that is a", "cit": "[CLS] 92.0) but it is the best result obtained without using lexical information 6. #refr store pos tag sequences that"}
{"pre": "[CLS] the first is the automatic evaluation of discourse structure of discourse structure of discourse structure of discourse structure in", "cit": "[CLS] this approach has been widely adopted to create discourse tree banks in several other languages such as turkish #otherefr,"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of possible antecedents for the", "cit": "[CLS] more specifically, we used svm #otherefr with a quadratic kernel implemented in tinysvm.11 the features we used are classified"}
{"pre": "[CLS] the work of #refr is similar to ours in that it is possible to use a more complex model", "cit": "[CLS] the syntactic forms of first mention?when an entity is first introduced in a text?differ from those of subsequent mentions"}
{"pre": "[CLS] the most common approach to paraphrase generation is to use a lexicon to paraphrase database into a target language", "cit": "[CLS] bilingual lexicon extraction from comparable corpora has received considerable attention since the 1990s #otherefr; #refra; de?jean et al, 2002;"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within psmt is to be a difficult problem #otherefr;", "cit": "[CLS] simultaneous translation of lectures and speeches has been addressed in #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to identify opinion holders and predict the polarity of words in a sentence and", "cit": "[CLS] following standard sentiment ranking approaches #otherefr; #refr, we employ ordinary linear regression to independently map bag-of-words representations into predicted"}
{"pre": "[CLS] the first is the one of the most popular approach to use a word alignment model that is based", "cit": "[CLS] the starting point is the final alignment generated using giza++?s implementation of ibm model 1 and the aachen hmm"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] sentence pair of a", "cit": "[CLS] many studies recently tried to extend them by incorporating specific information such as linguistic knowledge #refr, web-based resource #otherefr."}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] model with the average number of the average", "cit": "[CLS] unsupervised semantic role labeling with a generative model has also been considered #refr, however, they do not attempt to"}
{"pre": "[CLS] in the last decade, there has been a lot of work on joint inference in nlp #otherefr; #refr. [SEP]", "cit": "[CLS] in the last years, multiple spectral learning algorithms have been proposed for a wide range of models #otherefr; #refr."}
{"pre": "[CLS] in the domain of domain adaptation has been shown to be effective for various nlp tasks such as pos", "cit": "[CLS] in the tagging literature (e.g., #refr) an ambiguity class is often composed of the set of every possible tag"}
{"pre": "[CLS] in this paper, we propose a discriminative model for dependency parsing by #refr which is used to extract the", "cit": "[CLS] some research directly addresses m-to-n alignment with phrase alignment models #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the semantic role labeling task of the input text with a semantic role", "cit": "[CLS] nlp is not yet at a stage where \"covering a corpus\" can mean \"analyzing all elenmnts of meanings of"}
{"pre": "[CLS] #refr used a similar method to identify the polarity of words in a word in a sentence. [SEP] [SEP]", "cit": "[CLS] meanwhile, it also brings us lots of interesting and challenging research topics, such as subjectivity analysis #otherefr, sentiment classification"}
{"pre": "[CLS] the most popular approach to word alignment is the ibm models #otherefr; #refr. [SEP] model 4 alignments #otherefr; #refr.", "cit": "[CLS] producing many correct annotations for a complete sentence is a much more expensive undertaking: crowdsourcing can help alleviate the"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] ? ? ? ? ?", "cit": "[CLS] in this direction, #otherefr describes an inventory of preposition relations obtained using penn treebank function tags and frame elements"}
{"pre": "[CLS] the most common approach to paraphrase extraction is to use a lexicon based on bilingual parallel corpus or comparable", "cit": "[CLS] some recent research used comparable corpora to re-score name transliterations #otherefr; #refr or mine new word translations #otherefr. [SEP]"}
{"pre": "[CLS] we use the stanford named entity recognizer #refr to extract named entities from the text and extract named entities", "cit": "[CLS] for syntactic analysis, we identify words and pos tags with the japanese morphological analyser mecab2, and we use the"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal (wsj) section of the penn treebank #otherefr; #refr.", "cit": "[CLS] however, pos tags were used in parser combination in #refr for combining a set of malt parser models with"}
{"pre": "[CLS] in the second international chinese word segmentation bakeoff #otherefr; #refr and the same task as the best results are", "cit": "[CLS] unsupervised dependency parsing has seen rapid progress recently, with error reductions on english #otherefr or using cross-language adaptation #refr."}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] mira is a popular online large-margin structured learning method for nlp tasks #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a supervised sentiment classifier to detect the polarity of words and their polarity of words in a", "cit": "[CLS] thus, we extend the model with a supervised sentiment component that is capable of embracing many social and attitudinal"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] latent-variable pcfgs (l-pcfgs) are a highly successful model for natural language parsing #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second approach is to use a word lattice as a sequence of a sequence of morphological analyzer and", "cit": "[CLS] in previous work, we follow the same approach #refr, using svm-classifiers for individual morphological features and a simple combining"}
{"pre": "[CLS] the second dataset is the use of a word sense disambiguation algorithm which is based on the most similar", "cit": "[CLS] to date, elesk is the most popular such measure #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to word sense disambiguation is to use a word order features to extract translation rules", "cit": "[CLS] boldface marks scores that are significantly higher than the baseline. models, including the lexicalized reordering model and the lexical"}
{"pre": "[CLS] the most popular approach to word alignment is the hmm alignment model #refr. [SEP] model is based on the", "cit": "[CLS] more recently, submodular functions have started receiving attention in the machine learning and computer vision community #otherefr and have"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? ? ?", "cit": "[CLS] building on #refr, mccarthy et al #otherefr measure the semantic similarity between expressions (verb particles) as a whole and"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] equivalent formalizations replace parameter regularization by bayesian prior distributions on the parameters #refr or by augmentation of the feature"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] in particular, the conll shared tasks on dependency parsing have provided over twenty data sets in a standardized format"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] in order to obtain a linguistically plausible rightcorner transform representation of incomplete constituents, the switchboard corpus is subjected to"}
{"pre": "[CLS] the first is the automatic evaluation of subjectivity analysis in text classification #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] the corresponding standard words are ?together? and ?you?, respectively. of a mixture of both unintentional misspellings and intentionally-created tokens"}
{"pre": "[CLS] the first is the case for the automatic acquisition of subcategorization frames from corpora #otherefr, #refr, and many others.", "cit": "[CLS] previous standalone wsd tasks based on parallel data have obtained distinct translations for senses as listed in a dictionary"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] event", "cit": "[CLS] classical methods are either pattern-based #otherefr; #refr or classifierbased #otherefr). [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best results for the best", "cit": "[CLS] #refr and steedman et al #otherefr investigated using co-training for parsing. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] for language modeling, we use randlm #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of semantic role labeling #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] ? 2008.", "cit": "[CLS] because of the characteristics of the web, it is necessary to develop efficient algorithms able to learn from unannotated"}
{"pre": "[CLS] #refr proposed a method for learning a semantic inference based on a set of sentences paired with their meaning", "cit": "[CLS] besides vector averaging #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the polarity of a word is a word in a word in", "cit": "[CLS] also, comparisons of bayesian, informationretrieval, neural-network, and case-based methods on word-sense disambiguation have also demonstrated similar performance (leacock, towell,"}
{"pre": "[CLS] #refr proposed a method for identifying word pairs of words in a target word in a target word in", "cit": "[CLS] for example, in question answering, paraphrases from bilingual parallel corpus were used to expand the original questions #otherefr; in"}
{"pre": "[CLS] the first is the task of identifying the sense of identifying the senses of word senses in a word", "cit": "[CLS] figure 1: the senses of evidence in wordnet to exploit the natural skew of the data and focus on"}
{"pre": "[CLS] the data set contains 14,619 items and is available from the conll shared task data sets #refr. [SEP] test", "cit": "[CLS] next, we apply yarowsky is method tbr supervised decision list learning i #refr to 1vve choose tile decision list"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a text that is a", "cit": "[CLS] the techniques used to solve this problem can be roughly classified into two main categories: those relying on pre-existing"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the sentence in", "cit": "[CLS] reranking has been a popular technique applied in a variety of comparable nlp problems including parsing #otherefr; #refr, semantic"}
{"pre": "[CLS] the similarity between two words was computed using the wordnet similarity measure proposed by #refr. [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] we lemmatized and pos-tagged the german gur350 dataset #refr, a set of 350 word pairs with human similarity judgments,"}
{"pre": "[CLS] the first is the automatic evaluation of the automatic metric of the automatic evaluation of the automatic evaluation of", "cit": "[CLS] several approaches have statistically addressed the problem of prepositional phrase ambiguity, with comparable results #otherefr; #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to word alignment is the hmm alignment model #refr. [SEP] model is based on the", "cit": "[CLS] we evaluated our models using data from the bilingual word alignment workshop held at hlt-naacl 2003 #refr. [SEP] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] of a text with", "cit": "[CLS] we report 4-fold cross-validation #otherefr, where training and testing data come from different websites for each of the sides,"}
{"pre": "[CLS] the first is the polarity of a document #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] , s ?", "cit": "[CLS] knowledge of subjective language would also be useful in ame recognition #otherefr, intellectual attribution in text #refr, recognizing speaker"}
{"pre": "[CLS] the most common approach to this problem is to use a dictionary to model the entire document #otherefr; #refr.", "cit": "[CLS] many researchers have recently explored machine learning-based methods using considerable amounts of annotated data provided by, for example, the"}
{"pre": "[CLS] #refr use a supervised sentiment classifier to detect the polarity of words and their polarity of words in a", "cit": "[CLS] much previous work in natural language processing achieves better representations by learning from multiple tasks #otherefr; #refr. [SEP] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] sentence in a text", "cit": "[CLS] consequently, crowdsourcing offers a way to collect this data cheaply and quickly #otherefr; #refra). [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the feature weights were tuned using minimum error rate training (mert) #refr on the development set using minimum error", "cit": "[CLS] the adoption of discriminative learning methods for smt that scale easily to handle sparse and lexicalized features has been"}
{"pre": "[CLS] we use the berkeley parser #refr to obtain the syntactic parse trees for parse trees for each sentence in", "cit": "[CLS] most practical non-binary scfgs can be binarized using the synchronous binarization technique by #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to word alignment is the ibm models #otherefr; #refr. [SEP] model 1 and the hmm", "cit": "[CLS] the optimization problem can be solved with an efficient iterative algorithm which is parallelized in a mapreduce framework #refr."}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] model with respect to the entire candidate set.", "cit": "[CLS] recently, modeling of semantic compositionality #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the system is based on the inprotk toolkit for the best system described in #refr and the best system", "cit": "[CLS] a blackboard-like architecture is proposed by #refr in order to integrate various knowledge sources, but no evaluation is given."}
{"pre": "[CLS] the first is the task of identifying the polarity of a text that is a text of a text", "cit": "[CLS] we use a thresholding method borrowed from #refr, where our threshold is set as a proportion of the first"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to learn a translation models from a", "cit": "[CLS] one of the theoretical problems with phrase based smt models is that they can not effectively model the discontiguous"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to translate the phrases as the", "cit": "[CLS] to reduce the volume of data used, we filtered non-parallel and other unhelpful segments according to the technique described"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to learn a translation hypotheses and a", "cit": "[CLS] other applications include computing acceptable pseudo-references for discriminative training #otherefr; #refr; arun and 5the oracle decoding problem can be"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal (wsj) section of the penn treebank #otherefr; #refr.", "cit": "[CLS] efficient parsing algorithms for general lcfrs are also relevant in the context of synchronous context-free grammars (scfgs) as a"}
{"pre": "[CLS] topic models have been successfully applied to a wide range of nlp tasks including word segmentation and sentence compression", "cit": "[CLS] the use of annealing to obtain a maximum a posteriori (map) configuration from sampling-based inference is common (e.g., #refr."}
{"pre": "[CLS] the most common approach to this problem is to use a log-linear parsing model #otherefr; #refr. [SEP] model to", "cit": "[CLS] this intuition has spurred a growing interest of related research in the machine learning community, which in turn has"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] methods have been proposed, based on syntax, that take advantage of linguistic constraints and alignment of grammatical structure, such"}
{"pre": "[CLS] the first is the task of identifying the semantic textual similarity of a text of a text of a", "cit": "[CLS] a concept analogous to our notion of meta sense #otherefr; #refr, and indeed, the cam might be used for"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr.", "cit": "[CLS] markov logic networks can be represented as log-linear models, when grounded, and are therefore straightforward to extend with latent"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] from these papers followed two largely independent lines of research, respectively dubbed formally syntax-based machine translation #otherefr; #refr. [SEP]"}
{"pre": "[CLS] #refr use a similar approach to identify the topics of discourse relations in terms of discourse relations in terms", "cit": "[CLS] examples include genre classification #refr, author profiling #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the averaged perceptron algorithm #refr to train the parameters of the averaged perceptron algorithm #otherefr. [SEP] [SEP]", "cit": "[CLS] trevor et al proposed synchronous tree substitution grammar #otherefr; #refr proposed the joint compression model, which simultaneously considers the"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to learn a translation probabilities from", "cit": "[CLS] given the story that translations are generated based on the source syntax trees, the weight of the template is"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] event", "cit": "[CLS] task system team ge epi id bb bi co rel ren description uturku 1 1 1 1 1 1"}
{"pre": "[CLS] the first is the one of the most popular approaches to automatically identify the words in a text that", "cit": "[CLS] therefore, word sense disambiguation on a balanced corpus requires tackling a kind of domain (genre) adaptation problem #refr. [SEP]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the longrange reordering problem as a sequence of the", "cit": "[CLS] we presented two extensions to the beam search method presented in #refr, that reduce the search effort to decipher"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] giza++ was used for word alignment #refr and phrase translations of up to 10 words are extracted in the"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of possible target word is", "cit": "[CLS] an appealing option is the use of a nonparametric method, such as a hierarchical dirichlet process #refr. [SEP] [PAD]"}
{"pre": "[CLS] the most common approach to paraphrase generation is to use a lexicon to automatically extracted from corpora #otherefr; #refr.", "cit": "[CLS] accordingly, many researchers have recognized that automatic paraphrasing is an indispensable component of intelligent nlp systems #otherefr; #refr and"}
{"pre": "[CLS] in recent years, there has been a growing interest in the task of identifying the dialogue acts of an", "cit": "[CLS] #refr have previously shown that cue phrases automatically extracted from one corpus can be used to classify utterances from"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the longrange reordering model is to use a word", "cit": "[CLS] our work is also closely related to other recent work on learning probabilistic models involving structural latent variables #refr."}
{"pre": "[CLS] we use the minimum error rate training (mert) #refr to tune the feature weights for the feature weights of", "cit": "[CLS] semiring-weighted logic programming is a general framework to specify these algorithms #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] parsing is performed using the usual pipeline approach, first with the treetagger analyzer #otherefr and then with a state-of-the-art"}
{"pre": "[CLS] the most common approach to word alignment is to use a word alignment model that is to model that", "cit": "[CLS] there have been proposed some methods for reordering words in a japanese sentence so that the sentence becomes easier"}
{"pre": "[CLS] we use the averaged perceptron algorithm #refr to train the weights for the weights for each system. [SEP] [SEP]", "cit": "[CLS] by building the entire system on the derivation level, we side-step issues that can occur when perceptron training with"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the dependency parse trees for each sentence with dependency trees for", "cit": "[CLS] our approach builds on a variant of tree adjoining grammar #otherefr) (specifically, the formalism of #refr). [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the second group of approaches clusters instance representations #otherefr; #refr. [SEP] is a set of possible target word is", "cit": "[CLS] the task is aimed at overcoming the wellknown limitations of in vitro evaluations, such as those of previous semeval"}
{"pre": "[CLS] the first is the case for the case of the syntactic information in the case of the case of", "cit": "[CLS] hd instead uses the berkeley parser #refr, trained on negra?s successor, the larger tiger corpus #otherefr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the system is based on the inprotk project #refr. [SEP] system, which was developed at the level of the", "cit": "[CLS] for our experiments we used a corpus of 1037 calls from real users to a single dialogue system that"}
{"pre": "[CLS] the second approach is to use a maximum entropy model #otherefr; #refr. [SEP] model for the probability mass to", "cit": "[CLS] we will briefly describe the two parsing models employed #otherefr and also #refr; for a full description of the"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the sentence in", "cit": "[CLS] most existing works on sentiment summarization focus on predicting the overall rating on an entity #otherefr; #refr). [SEP] [PAD]"}
{"pre": "[CLS] the work presented in this paper connects and extends several areas of research: grounded semantics #otherefr, which aims to", "cit": "[CLS] we also briefly overview the constraint-based control architecture that enables such an approach and facilitates near linear-time processing with"}
{"pre": "[CLS] the grammar is a grammar that is a synchronous grammar (stsg) grammar (stsg) grammar (stsg) for synchronous grammar formalisms", "cit": "[CLS] #refr approached to this problem employing a nmlti-engine architecture, where outputs from transfer machine translation (mt), knowledge-based mt and"}
{"pre": "[CLS] #refr use a supervised sentiment classifier to detect the polarity of words in a target words and their polarity", "cit": "[CLS] as for aspect level rating, ranking, or summarization, benjamin#otherefr employed the good grief algorithm for multiple aspect ranking and"}
{"pre": "[CLS] the first is the task of identifying the sentence pairs from a text with a text that are semantically", "cit": "[CLS] attempts have also been made to use syntax on the source and/or the target sides to capture the syntactic"}
{"pre": "[CLS] the system is based on the inprotk project #refr. [SEP] system, which is a set of possible antecedents of", "cit": "[CLS] 3using chasen morphological-analysis software for the japanese language #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] similar to stochastic disambiguation for constraint-based parsing #refr, an exponential (a.k.a. log-linear or maximumentropy) probability model on transferred structures"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to learn a translation probabilities from", "cit": "[CLS] a third method is using multiple translation models as alternative decoding paths #otherefr, an idea which #refr first used"}
{"pre": "[CLS] in the literature, supervised learning methods have been applied to unsupervised learning of natural language processing #otherefr; #refr. [SEP]", "cit": "[CLS] while self-training is widely used and can yield good results in some applications #refr, it has no theoretical guarantees"}
{"pre": "[CLS] #refr use a similar approach to compute similarities between words in a sentence. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] finally, one common hypotheses is that the brain integrates the word with inversely proportional effort to how predictable the"}
{"pre": "[CLS] the most popular approach to translation lexicons from parallel corpora #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] web counts are frequently used to automatically re-rank candidate lists for various nlp tasks (al-onaizan and #refr. [SEP] [PAD]"}
{"pre": "[CLS] the most common approach to parsing is to use a log-linear parsing model and maximise the conditional random field", "cit": "[CLS] researchers have paid a lot of attention to datadriven dependency parsing in recent years #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a bootstrapping method to identify the polarity of words in a sentence. [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] the second dataset consists of 234 reviews for two different web-services collected from epinions.com, as described in #refr. [SEP]"}
{"pre": "[CLS] the model is trained using the moses decoder #refr. [SEP] score #refr with the decoder that the decoder is", "cit": "[CLS] these features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous"}
{"pre": "[CLS] in the context of machine translation, itg has been explored for a variety of nlp tasks such as sentiment", "cit": "[CLS] finally, our work is similar in spirit to sentiment analysis #otherefr, and metaphor understanding #refra; shutova, 2010b). [SEP] [PAD]"}
{"pre": "[CLS] the first is the case for the parsing of the penn treebank #refr, which we will use the penn", "cit": "[CLS] these language models detect repairs as they process the input; however, like past work on speech repair detection, they"}
{"pre": "[CLS] the feature weights were tuned on the wmt-10 devset using mert #refr with pro #otherefr. [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] the black lines show the expected bleu as ? in equation (5) increases from 0.1 toward?. for this honor,"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] although this kernel achieves state-of-the-art performance in nlp tasks, such as question classification #otherefr, it offers clearly possibility of"}
{"pre": "[CLS] the second approach is to use a word clusters as described in #refr. [SEP] [SEP] [SEP] [SEP] [SEP] =", "cit": "[CLS] thus much work developed in the literature has focused on designing robust projection algorithms such as graph-based projection with"}
{"pre": "[CLS] the most popular approach to translation lexicons from the source language into target language #otherefr; #refr. [SEP] grammars to", "cit": "[CLS] while several significantly improved models have been developed since then, including phrase-based #otherefr, hierarchical #refr, treelet #otherefr models, they"}
{"pre": "[CLS] #refr use a similar approach to identify the topics of discourse relations. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] implicatives and factives #refr presented an approach to the treatment of inferences involving implicatives and factives. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the translation quality is measured by bleu #refr. [SEP] score #refr on the test set. [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] in addition to the regular distance distortion model, we incorporate a maximum entropy based lexicalized phrase reordering model #refr."}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best performing models #otherefr. [SEP]", "cit": "[CLS] an alternative approach has been proposed by feldman, hana and #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford corenlp suite to lemmatize and partof-speech tag each word #otherefr; #refr, and extract the verb", "cit": "[CLS] #refr revealed that only two differences in results are significant. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to extract dependency relations for each word and parse trees from the sentence", "cit": "[CLS] previous approaches to debate stance classification have focused on three debate settings, namely congressional floor debates #otherefr; #refr, companyinternal"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal, parser #refr and tested on the penn treebank", "cit": "[CLS] following the traditional approach, we include every sentence in the test suite, along with the core phenomenon we intend"}
{"pre": "[CLS] the most popular approach to statistical machine translation is the one proposed by #refr. [SEP] model to use a", "cit": "[CLS] therefore, several approaches were proposed to limit the storage size of large lms, for instance #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the first is the annotation tool which was the annotation tool for the annotation tool #refr. [SEP] project in", "cit": "[CLS] 13) claims that in scientific research articles, ?the hedging of claims is so common that a sentence that looks"}
{"pre": "[CLS] the paraphrase collection has been shown to be effective in many nlp tasks such as paraphrase detection #otherefr, paraphrase", "cit": "[CLS] sentential paraphrase collection has been tackled from specific resources increasing the probability of sentences being paraphrases #otherefr; #refr, from"}
{"pre": "[CLS] the first is the nlg community #otherefr; #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] , which is the", "cit": "[CLS] recently, approaches have been proposed to improve system turn-taking behavior that use reinforcement learning #otherefr, decision theory (e.g., #refr,"}
{"pre": "[CLS] #refr used a supervised sentiment classification approach to classify reviews as well as sentiment classification and classify reviews as", "cit": "[CLS] however, according to kanayama?s investigation, only 60% co-occurrences in the same window in web pages reflect the same sentiment"}
{"pre": "[CLS] the second approach is to use a small number of seed examples for training data #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] besides cross-lingual dependency parsing, multilingual model learning methods have also achieved good empirical results for other multilingual nlp tasks,"}
{"pre": "[CLS] the most common approach to translation is to use a log-linear model #refr and a log-linear model to handle", "cit": "[CLS] phrase-based translation models #refr, which go beyond the original ibm translation models #otherefr 1 by modeling translations of phrases"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] the", "cit": "[CLS] for instance, ji and grishman #otherefr extracted implicit time information; #refr used broader sentential context; liao and grishman #otherefrb)"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a text that are semantically", "cit": "[CLS] unlike the earlier work that has viewed all verbs as possible light verbs #refr, we focus on a half"}
{"pre": "[CLS] the first is the task of identifying the event descriptions of a text in a text with a text", "cit": "[CLS] several lexical resources have been built manually, most notably wordnet #otherefr, framenet#refr and verbnet#otherefr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a similar to the one used in the sense of the", "cit": "[CLS] instead, postpositional case particles function as case markers. the former representation, the naist text corpora #refr, another major japanese"}
{"pre": "[CLS] the most common approach to translation is to use a log-linear combination of models #otherefr; #refr. [SEP] model to", "cit": "[CLS] the remaining free parameters, i.e. pm and the model scaling factors #refr, were adjusted on the development corpus (dev)."}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking approaches #otherefr; #refr to learning for learning", "cit": "[CLS] successes in these syntactic tasks have recently paved the way to applying novel statistical learning techniques to levels of"}
{"pre": "[CLS] the second dataset is the use of a word sense disambiguation algorithm which is based on the most popular", "cit": "[CLS] in recent years different approaches to word sense disambiguation task have been evaluated through comparative campaigns, such as the"}
{"pre": "[CLS] the model is trained with the perceptron algorithm of #refr. [SEP] score entire sequence models ? = ? ?", "cit": "[CLS] coreference resolution systems can benefit by incorporating the world knowledge including rules, constraints, and additional information from external knowledge"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] sentence pair of a", "cit": "[CLS] examples of such lexicons include the multi-perspective question answering (mpqa) subjectivity lexicon #refr and the linguistic inquiry and word"}
{"pre": "[CLS] the evaluation metric is bleu #refr, which is based on the reference evaluation. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] we used the ppdb paraphrase database #refr to get the paraphrases. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of identifying the semantic relations between nominals of noun phrases and their polarity of words was also", "cit": "[CLS] the data we have used for training our system were made available in the framework of the semeval (task"}
{"pre": "[CLS] the first is the automatic evaluation of discourse structure of discourse structure of discourse structure of discourse structure of", "cit": "[CLS] #refr and vogel and jurafsky #otherefr work in the domain of computer technical support instructions, mapping language to actions"}
{"pre": "[CLS] the grammar is a grammar that is a grammar that is a given a grammar that is a grammar", "cit": "[CLS] several general-purpose off-the-shelf (ots) parsers have become widely available #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the bionlp shared task 2011 (bionlp st?11) #refra), the follow-up event to the bionlp?09 shared task #otherefr. [SEP] the", "cit": "[CLS] we also intend to investigate the possibility of discovering and utilising shared information between the two domains #refr. [SEP]"}
{"pre": "[CLS] the second approach is to use a word lattice as a word segmentation and its context #otherefr; #refr. [SEP]", "cit": "[CLS] for english, a number of methods have been proposed to cope with real-word errors in spelling correction #otherefr; #refr."}
{"pre": "[CLS] #refr use a bootstrapping method to identify opinion holders and predict the polarity of opinion expressions in opinion holders", "cit": "[CLS] a wide spectrum of tasks have been studied under review mining, ranging from coarse-grained document-level polarity classification #refr to"}
{"pre": "[CLS] the first is the case for the semantic textual similarity of the words in the context of a text", "cit": "[CLS] this problem was illustrated using a german lfg grammar #otherefr constructed as part of the pargram project #refr. [SEP]"}
{"pre": "[CLS] the second approach is to use a word clusters as described in #refr. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] that local collocation knowledge provides important clues to wsd has also been pointed out previously by #refr. [SEP] [PAD]"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] 3 of #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most popular approach to word alignment is the ibm models #otherefr; #refr. [SEP] model 1 and the hmm", "cit": "[CLS] therefore, researchers usually resort to an approximate k-nn algorithms such as locality-sensitive hashing (?; #refr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the unsupervised pos induction task has been extensively studied in the nlp community, and has been studied in the", "cit": "[CLS] these models are either hidden markov models #otherefr; #refr or clustering models #otherefr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the parser is trained on the penn treebank wall street journal, parser #refr and tested on the penn treebank", "cit": "[CLS] the parsing strategy is left-to-right, data driven, with parallel treatment of alterna- 2for arguments in favor of interactive systems"}
{"pre": "[CLS] the first is the task of identifying the polarity of a text that is a text of a text", "cit": "[CLS] verb classes have proved useful in various #otherefr, word sense disambiguation #refr, document classification #otherefr. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best derivation of the best", "cit": "[CLS] when ime fails to correct a typo and generate the expected sentence, the user will have to takemuch extra"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] most of them have been proposed in order to make translation systems perform better for resource-scarce domains when most"}
{"pre": "[CLS] the first is the case for the parsing of the penn treebank #refr, which we use the berkeley parser", "cit": "[CLS] the only rule-based approach to german lfg-parsing we are aware of is the hand-crafted german grammar in the pargram"}
{"pre": "[CLS] the first is the case for the parsing of the english lexical substitution task #refr. [SEP] test set of", "cit": "[CLS] tree edit distance has previously been used in the tedeval software #otherefr; #refr for parser evaluation agnostic to both"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a similar to the one used in the same textual entailment", "cit": "[CLS] the fact that voorbij is unaccentable in these positions cannot be explained by claiming the word itself is unaccentable,"}
{"pre": "[CLS] the task of identifying the semantic relations between nominals of noun phrases and their semantic relations has been shown", "cit": "[CLS] various attempts have been made to learn the taxonomic organization of concepts #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the case of the syntactic structure of the english lexical substitution task #refr,", "cit": "[CLS] the knowledge-rich approach needs mechanisms to take into account errors within a rigid system of rules, and thus different"}
{"pre": "[CLS] in the graph-based dependency parsing literature, the highest-scoring projective dependency tree is the projective dependency tree of #refr. [SEP]", "cit": "[CLS] hypergraph search the structured perceptron algorithm #refr is a general learning algorithm. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] however, certain properties of the bleu metric can be exploited to speed up search, as described in detail by"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] hierarchical translation was combined with target side linguistic annotation in #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training and", "cit": "[CLS] therefore, researchers have proposed alternative approaches to learning synchronous grammars directly from sentence pairs without word alignments, via generative"}
{"pre": "[CLS] the most common approach to paraphrase generation is to use a lexicon to paraphrase database into a target language", "cit": "[CLS] several solutions to this problem have been proposed including query expansion #otherefr; #refr and semantic information retrieval #otherefr. [SEP]"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within the meaning representation of the meaning of the", "cit": "[CLS] the projected lexical features that we propose in this work are based on lexicalized versions of features found in"}
{"pre": "[CLS] the first is the case for the syntactic analysis of the english lexical substitution task of the penn treebank", "cit": "[CLS] icicle (interactive computer identification and correction of language errors) is an intelligent tutoring system currently under development #refr. [SEP]"}
{"pre": "[CLS] the work of #refr is similar to ours in that the generation of referring expressions is in that the", "cit": "[CLS] although we address these user states in the tutoring domain, spoken dialogue researchers across domains and applications have investigated"}
{"pre": "[CLS] the bionlp 2011 shared task #refr focuses on event extraction and event extraction and event extraction and event extraction", "cit": "[CLS] #refr also explore temporal linking with events, but do not assume that events and time stamps have been provided"}
{"pre": "[CLS] the first is the task of identifying the event descriptions of the bionlp?09 shared task on event extraction #refr.", "cit": "[CLS] while our approach is not specific to a particular framework for deep syntactic analysis, we have chosen to build"}
{"pre": "[CLS] the first is the task of sentence compression which is a sentence is a sentence based on the idea", "cit": "[CLS] an appealing approach to such textual inferences is to explicitly transform t into h, using a sequence of transformations"}
{"pre": "[CLS] the second approach is to use a small number of seed examples for example outputs #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] for purposes of pruning, and only for purposes of pruning, the prior probability of each constituent category is multiplied"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] specifically, the table shows the best overall correlation results obtained by the three emotion recognition systems in the semeval"}
{"pre": "[CLS] the task of identifying the semantic textual similarity of a text is a text of a text #refr. [SEP]", "cit": "[CLS] in addition, an alignment-based approach has the advantage of generality: almost all existing rte models align the linguistic material"}
{"pre": "[CLS] the conll shared tasks on dependency parsing in 2006 and 2007 #refr have been shown to be a wide", "cit": "[CLS] the best projective parse tree is obtained using the eisner algorithm #refr with the scores, and the best non-projective"}
{"pre": "[CLS] the first is the annotation tool which was the annotation tool for the annotation tool #refr. [SEP] project in", "cit": "[CLS] there has already been evidence of models trained on wsj doing poorly on non-wsj data on parses #refr, semantic"}
{"pre": "[CLS] the system used in this paper is based on the senseval-3 english lexical sample task #refr. [SEP] test set", "cit": "[CLS] while the conventional view on prepositions from the computational linguistics community has been that they are semantically transient at"}
{"pre": "[CLS] the first is the task of converting natural language processing #otherefr; #refr. [SEP] [SEP] to be a given a", "cit": "[CLS] these features specify whether a discourse marker that introduces expectations #refr (such as although) was used in the sentence"}
{"pre": "[CLS] the translation quality is evaluated by case-insensitive bleu-4 #refr. [SEP] score #refr with the bleu score for the translation", "cit": "[CLS] the differences in bleu points are 0.14 and 0.16, which are not statistically significant according to the paired bootstrap"}
{"pre": "[CLS] the wsd system is based on the standard approach of #refr and the dso corpus, which is used for", "cit": "[CLS] a variety of unsupervised wsd methods, which use a machinereadable dictionary or thesaurus in addition to a corpus, have"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] it is generally formulated as a semantic role labeling #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the second approach is to use a word lattice to improve the performance of a word alignment model that", "cit": "[CLS] other approaches to improving alignment have combined alignment models, e.g., using a log-linear combination #refr or mutually independent association"}
{"pre": "[CLS] the first is the automatic evaluation of discourse relations #refr. [SEP] of the fact that the accuracy of the", "cit": "[CLS] patterndriven search engine queries allow to access such information and gather the required data very efficiently #refr. [SEP] [PAD]"}
{"pre": "[CLS] the first is the task of identifying the noun phrases in a text with a text that is a", "cit": "[CLS] comic started off as a re-implementation of the content assessment module (cam) of #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the one of the most popular approaches to automatically identify the words in a text that", "cit": "[CLS] #refra)), and little consideration is given to the applicability of that method to more general linguistic properties. [SEP] [PAD]"}
{"pre": "[CLS] the most popular approach to smt is the one proposed by #refr, who use a large set of features", "cit": "[CLS] as for the smt system, we use a standard log-linear pb-smt model #otherefr, minimum-error-rate training #refr, a 5-gram language"}
{"pre": "[CLS] the second approach is to use a small number of seed examples for example outputs #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] coreference is determined using an information extraction engine, serif #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the weights of the log-linear model are optimized using minimum error rate training (mert) #refr on the development set", "cit": "[CLS] #refr also tuned the parameters with small batches of sentences and optimized a hinge loss not explicitly related to"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] score #refr with the best results for the best", "cit": "[CLS] word segmentation and pos tagging in a joint process have received much attention in recent research and have shown"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking approaches #otherefr; #refr to learning for dependency", "cit": "[CLS] attach-right 38.4 31.7 em #otherefrc) 69.3 50.4 adaptor grammar #refr 50.2 - tsg-dmv #otherefr 84.5 68.8 [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the generation of referring expressions is a difficult problem that is central to a more difficult problem in the", "cit": "[CLS] verb~;iobil also contains a component hat automatically generates dialogue scripts and result summaries of the dialogues in all target"}
{"pre": "[CLS] we used the stanford parser #refr to extract the syntactic parse trees for the sentence in the training and", "cit": "[CLS] they train a phrase-based statistical mt (pbsmt) system #refr translating from lh to lt , and use the translation"}
{"pre": "[CLS] #refr use a similar approach to identify the topics of discourse relations in terms of discourse relations in a", "cit": "[CLS] we used mechanical turk for our experiments since it has been shown to be useful for evaluating nlp systems"}
{"pre": "[CLS] the translation quality is measured by bleu #refr. [SEP] score #refr with the bleu score of the translation quality.", "cit": "[CLS] mert #otherefr and mira #refr are popular solution to compute an optimal weight vector by minimizing the error on"}
{"pre": "[CLS] the grammar we use is a standard grammar for the grammar development and test and test and test data", "cit": "[CLS] #refr argue that the effect is simply an instance of a pervasive syntactic priming mechanism in human parsing. [SEP]"}
{"pre": "[CLS] the second approach is to use a word segmentation and pos tagger to tag dictionary to tag each word", "cit": "[CLS] segmentation for japanese is a successful field of research, achieving the f-score of nearly 99% #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the semantic textual similarity (sts) task #refr is a task of identifying the degree of semantic relations between words", "cit": "[CLS] they are needed for textual inference, in which one has to infer certain relations between eventualities #otherefr, for information"}
{"pre": "[CLS] the most common approach to handling the longrange reordering problem within the source sentence into a target sentence into", "cit": "[CLS] in addition, by selecting character n-grams instead of word n-grams, one avoids to segment words in chinese which has"}
{"pre": "[CLS] the most common approach to machine translation is to use a log-linear combination of statistical machine translation #otherefr; #refr.", "cit": "[CLS] this latter problem is viewed by some as the key translation problem, #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] figure 1: smatch scores of annotators (a-d) against the consensus annotation (e) over time. ment between an input sentence"}
{"pre": "[CLS] the model is trained using the giza++ implementation of ibm model 4 alignments #refr. [SEP] model 4 alignments #otherefr", "cit": "[CLS] the model is similar to previously proposed phrase alignment models based on inversion transduction grammars #otherefr; #refr, with one"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to learn a translation rules for", "cit": "[CLS] bleu has shown to be a strong baseline metric to use as an objective function #refr, and so the"}
{"pre": "[CLS] we use the stanford parser #refr to obtain the syntactic parse trees for each sentence in the training corpus.", "cit": "[CLS] 3btgs cannot reproduce all possible reorderings, but can handle most reorderings occurring in natural translated text #refr. [SEP] [PAD]"}
{"pre": "[CLS] the most popular approach to mt system combination is the one proposed by #refr, which uses a log-linear combination", "cit": "[CLS] some of these are the wer #otherefr with its newer version terp #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] we use the berkeley parser #refr to obtain the syntactic parse trees for each sentence and the english and", "cit": "[CLS] weak synchronization is closely related to the quasi-synchronous models of #refr and the bilingual parse reranking model of burkett"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the input is to be", "cit": "[CLS] figure 2 shows some of the realizations of alternative sentence plans generated by our spg for utterance sys- 3the"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the unbounded nature of the", "cit": "[CLS] probabilistic parsers trained over labeled data have high accuracy on indomain data: lexicalized parsers get an f-score of up"}
{"pre": "[CLS] the task of identifying the semantic relations between nominals of noun phrases and their semantic relations has been shown", "cit": "[CLS] the significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules #otherefr;"}
{"pre": "[CLS] the second approach to the problem of word segmentation is to use a word lattice #refr, which is based", "cit": "[CLS] it is unclear, however, what kind of metric would be most suitable for the evaluation of string realisations, so"}
{"pre": "[CLS] the parser is trained using the averaged perceptron algorithm #refr. [SEP] algorithm with the collins parser is a variant", "cit": "[CLS] most of the participants took language-independent approaches toward leveraging this complexity into better performance: generating machine learning features based"}
{"pre": "[CLS] the first is the task of identifying the polarity of a document #otherefr; #refr. [SEP] of a text with", "cit": "[CLS] in contrast to many previous approaches to automated essay grading #refr, our goal is not to assign a letter"}
{"pre": "[CLS] the most common approach to paraphrase generation is to use a lexicon to paraphrase database into a target language", "cit": "[CLS] bleu #refr), it considers not only exact matches, but also word stems, synonyms, and paraphrases #otherefr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the first is the annotation tool which was the annotation tool for the annotation tool #refr. [SEP] project in", "cit": "[CLS] the hilda discourse parser of hernault and his colleagues #refr; hernault et al, 2010b) is the first fully-implemented featurebased"}
{"pre": "[CLS] the most common approach to sentiment analysis is to use a labelled lexicon to score sentences #otherefr; #refr. [SEP]", "cit": "[CLS] in #refr a study was conducted on a reduced corpus of tweets labelled manually. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to this problem is to use a dictionary to model the decision to generate coherent", "cit": "[CLS] recently, chang and lai #otherefr, #refr, li and yarowsky #otherefr have investigated this task. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the most common approach to phrase-based translation is to use a log-linear model #refr and a log-linear model to", "cit": "[CLS] we are currently in the process of implementing and testing other parameter tuning methods #otherefr and batch mira #refr."}
{"pre": "[CLS] in the last decade, there has been a lot of work on sentiment analysis in english #otherefr; #refr. [SEP]", "cit": "[CLS] we rank the candidates based on the following two criteria: #otherefr, it?s more humorous #refr. #otherefr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] in this paper, we propose a discriminative model for learning from random field (crf) and perceptron learning #refr. [SEP]", "cit": "[CLS] on one hand, researchers have been developing modified learning algorithms that allow inexact search #otherefr; #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] in the future, we plan to investigate the use of discriminative reranking techniques for parsing #refr. [SEP] grammars with", "cit": "[CLS] #refr presented a parsing model that allows for the introduction of non-projective edges into dependency trees through learned edge"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses a log-linear model to learn a translation hypotheses and a", "cit": "[CLS] but fluency and adequacy are subjective and not easy to tease apart #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the process of annotating the word in a language #refr. [SEP] project in a number of", "cit": "[CLS] for literature, topical segmentations have been produced for a short story #otherefr and a novel #refr. [SEP] [PAD] [PAD]"}
{"pre": "[CLS] the model is trained using the structured perceptron algorithm #refr. [SEP] model with the average number of the average", "cit": "[CLS] recently, a number of researchers have tried to reconcile the framework of distributional semantics with the principle of compositionality"}
{"pre": "[CLS] the most common approach to handling the unbounded nature of the unbounded nature of the words is to be", "cit": "[CLS] thus, some research has been focused on deriving different word-sense groupings to overcome the fine?grained distinctions of wn (hearst"}
{"pre": "[CLS] id participant cmu carnegie mellon university #otherefr cu-bojar charles university - bojar #refr cu-depfix charles university - depfix #otherefr", "cit": "[CLS] edinburgh, uk #otherefr ucb university of california at berkeley, usa #refr uedin university of edinburgh, uk #otherefr [SEP] [PAD]"}
{"pre": "[CLS] the first is the case for the syntactic analysis of the english language model used in the penn treebank", "cit": "[CLS] we use our own parser which trains a pcfg-la using the above procedure and parses using the max- 1estimation"}
{"pre": "[CLS] the second approach is to use a word clusters as described in #refr. [SEP] model to predict the probability", "cit": "[CLS] a large body of related work can be found in #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] #refr use a similar approach to compute sentence importance. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]", "cit": "[CLS] an alternative abstractive or ?bottom-up? approach involves identifying high-interest words and phrases in the source text, and combining them"}
{"pre": "[CLS] the model is trained using the perceptron algorithm #refr. [SEP] algorithm with the average perceptron algorithm with the average", "cit": "[CLS] recent work #refr has introduced a spectral algorithm for parameter estimation of l-pcfgs, which?unlike the em algorithm?is guaranteed to"}
{"pre": "[CLS] the first is the case for the generation of referring expressions from the air travel information system #refr, a", "cit": "[CLS] recent corpus-based generation approaches faced the problem that existing standard treebank representations for parsing or other analysis tasks do"}
{"pre": "[CLS] the first is the task of identifying the event descriptions of the bionlp?09 shared task on event extraction #refr.", "cit": "[CLS] bridging anaphora #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the task of semantic relation extraction has received a lot of attention in recent years #otherefr; #refr. [SEP] [SEP]", "cit": "[CLS] for example, #refr improve textrunner precision by using deep parsing information via semantic role labelling. [SEP] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the semantic role labeling is the process of annotating the predicate-argument structure in text with se- ?this research was", "cit": "[CLS] with iob2 representation #refr, the problem of chinese chunking can be regarded as a sequence labeling task. [SEP] [PAD]"}
{"pre": "[CLS] the translation model is a phrase-based decoder that uses the log-linear model #refr to learn a translation probabilities from", "cit": "[CLS] syntax-based statistical machine translation #otherefr; #refr. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]"}
{"pre": "[CLS] the first is the case for the identification of the language model of the meaning of the discourse structure", "cit": "[CLS] #refr introduced a twostage process, first transforming from sentence to syntactic tree, then from syntactic tree to new sentence;"}
{"pre": "[CLS] the dependency parser is trained on the penn treebank #refr and tested on the penn treebank #otherefr. [SEP] score", "cit": "[CLS] one significant advantage of transition-based dependency parsing #otherefr; #refr is that they can utilize rich feature representations. [SEP] [PAD]"}
